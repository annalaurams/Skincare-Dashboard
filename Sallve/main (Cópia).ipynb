{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d3a4b5",
   "metadata": {},
   "source": [
    "# Sallve - Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c035c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, json, time, unicodedata\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, urlencode\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ========= GLOBAIS FALTANTES (definidos aqui p/ evitar NameError) =========\n",
    "\n",
    "\n",
    "# ========= IMPORTS DOS MODELS =========\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "from category import CATEGORY_HINTS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575f060",
   "metadata": {},
   "source": [
    "## Configurações Iniciais -> variáveis e requisição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL       = os.environ.get(\"SALLVE_BASE_URL\", \"https://www.sallve.com.br\")\n",
    "COLLECTION_URL = os.environ.get(\"SALLVE_COLLECTION_URL\", f\"{BASE_URL}/collections/loja\")\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.8\",\n",
    "}\n",
    "IMAGES_DIR = Path(\"images\")\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68ec5a",
   "metadata": {},
   "source": [
    "## Funções Utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _strip_accents_lower(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _safe_join_url(url: str) -> str:\n",
    "    return \"https:\" + url if isinstance(url, str) and url.startswith(\"//\") else url\n",
    "\n",
    "def _tokenize_ingredientes(texto: str):\n",
    "    if not texto: return []\n",
    "    texto = texto.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    partes = re.split(r\"[;,·•|\\u2022]|(?:\\s{2,})|,|\\.\", texto)\n",
    "    return [p.strip() for p in partes if p and p.strip()]\n",
    "\n",
    "def _padroniza_por_lista(tokens, lista_validos):\n",
    "    valid_norm_map = {_strip_accents_lower(v): v for v in lista_validos}\n",
    "    padronizados, vistos = [], set()\n",
    "    for tok in tokens:\n",
    "        n = _strip_accents_lower(tok)\n",
    "        # normalizações especiais\n",
    "        if \"hialuronato\" in n and \"sodio\" in n and \"acido hialuronico\" in valid_norm_map:\n",
    "            key = \"acido hialuronico\"\n",
    "        elif \"matrixyl\" in n:\n",
    "            key = _strip_accents_lower(\"peptídeos matrixyl\")\n",
    "        else:\n",
    "            key = None\n",
    "            for kn in valid_norm_map.keys():\n",
    "                if n == kn or n.startswith(kn) or kn in n:\n",
    "                    key = kn\n",
    "                    break\n",
    "        if key and key not in vistos:\n",
    "            padronizados.append(valid_norm_map[key]); vistos.add(key)\n",
    "    return padronizados\n",
    "\n",
    "def _add_or_replace_page_param(url: str, page: int) -> str:\n",
    "    parsed = urlparse(url)\n",
    "    q = parse_qs(parsed.query, keep_blank_values=True)\n",
    "    q[\"page\"] = [str(page)]\n",
    "    new_query = urlencode({k: v[0] if isinstance(v, list) and len(v) == 1 else v for k, v in q.items()}, doseq=True)\n",
    "    return parsed._replace(query=new_query).geturl()\n",
    "\n",
    "def _sanitize_filename(name: str) -> str:\n",
    "    base = _strip_accents_lower(name)\n",
    "    base = re.sub(r\"[^a-z0-9]+\", \"-\", base).strip(\"-\")\n",
    "    base = re.sub(r\"-{2,}\", \"-\", base)\n",
    "    return base or \"produto\"\n",
    "\n",
    "def _normalize_price_text(txt: str) -> str | None:\n",
    "    if not txt: return None\n",
    "    t = re.sub(r\"\\s+\", \" \", txt).strip().replace(\"\\xa0\", \" \")\n",
    "    m = re.search(r\"R\\$\\s*\\d{1,3}(?:\\.\\d{3})*,\\d{2}\", t)\n",
    "    if m:\n",
    "        p = m.group(0)\n",
    "        p = re.sub(r\"\\s+\", \" \", p).replace(\"R$ \", \"R$ \").strip()\n",
    "        return p\n",
    "    m2 = re.search(r\"\\d{1,3}(?:\\.\\d{3})*,\\d{2}\", t)\n",
    "    if m2:\n",
    "        return \"R$ \" + m2.group(0)\n",
    "    return None\n",
    "\n",
    "def should_exclude_product(product_name):\n",
    "    if not product_name: return True\n",
    "    product_name_lower = product_name.lower()\n",
    "    return any(k in product_name_lower for k in EXCLUDE_KEYWORDS)\n",
    "\n",
    "\n",
    "_CATEGORY_ORDER_MAP = {name: i for i, name in enumerate(CATEGORY_CANONICAL_ORDER)}\n",
    "\n",
    "def classify_category_from_name(name: str, description: str | None = None) -> str | None:\n",
    "    txt = _strip_accents_lower(f\"{name or ''} {description or ''}\")\n",
    "    hits = []\n",
    "    for cat, needles in _CATEGORY_HINTS.items():\n",
    "        if any(_strip_accents_lower(n) in txt for n in needles):\n",
    "            hits.append(cat)\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits.sort(key=lambda c: _CATEGORY_ORDER_MAP.get(c, 10_000))\n",
    "    return hits[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144073e4",
   "metadata": {},
   "source": [
    "# Podutos Nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04673398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_links_from_collection(max_pages: int = 8):\n",
    "    links, vistos = [], set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{COLLECTION_URL}?page={page}\"\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                if \"/products/\" in href:\n",
    "                    full = urljoin(BASE_URL, href)\n",
    "                    if full not in vistos:\n",
    "                        vistos.add(full); links.append(full)\n",
    "            time.sleep(0.7)\n",
    "        except Exception as e:\n",
    "            print(f\"[get_product_links_from_collection] Erro na página {page}: {e}\")\n",
    "            break\n",
    "    return links\n",
    "\n",
    "\n",
    "def extract_ingredients(soup: BeautifulSoup):\n",
    "    ingredientes_brutos = []\n",
    "    area_ing = soup.find(\"div\", class_=re.compile(r\"\\btabcontent\\b.*\\bingredients\\b\", re.I))\n",
    "    if area_ing:\n",
    "        for h2 in area_ing.find_all(\"h2\"):\n",
    "            txt = h2.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                ingredientes_brutos.append(txt)\n",
    "        resume = area_ing.find(\"div\", class_=re.compile(r\"\\bingredients_resume\\b\", re.I))\n",
    "        if resume:\n",
    "            raw = resume.get_text(\"\\n\", strip=True)\n",
    "            raw_norm = _strip_accents_lower(raw)\n",
    "            i1 = raw_norm.find(\"ingredientes:\")\n",
    "            if i1 != -1:\n",
    "                bloco_raw = raw[i1 + len(\"ingredientes:\") :]\n",
    "                for stopper in [\"ingredientes em portugues:\", \"ingredientes em português:\"]:\n",
    "                    cut_idx = _strip_accents_lower(bloco_raw).find(stopper)\n",
    "                    if cut_idx != -1:\n",
    "                        bloco_raw = bloco_raw[:cut_idx]\n",
    "                        break\n",
    "                ingredientes_brutos.extend(_tokenize_ingredientes(bloco_raw))\n",
    "            for key in [\"ingredientes em portugues:\", \"ingredientes em português:\"]:\n",
    "                j = raw_norm.find(key)\n",
    "                if j != -1:\n",
    "                    bloco_pt = raw[j + len(key) :]\n",
    "                    ingredientes_brutos.extend(_tokenize_ingredientes(bloco_pt))\n",
    "                    break\n",
    "\n",
    "    tokens = []\n",
    "    for item in ingredientes_brutos:\n",
    "        if not item: continue\n",
    "        t = item.strip().strip(\":\").strip()\n",
    "        if not t: continue\n",
    "        if len(t.split()) > 8 and \",\" not in t and \";\" not in t:\n",
    "            continue\n",
    "        tokens.append(t)\n",
    "\n",
    "    padronizados = _padroniza_por_lista(tokens, INGREDIENTES_VALIDOS)\n",
    "    return \"; \".join(padronizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d058b72",
   "metadata": {},
   "source": [
    "## Tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27631161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_size(soup: BeautifulSoup):\n",
    "    size_element = soup.find(\"span\", class_=re.compile(r\"\\bProductWeight\\b\", re.I))\n",
    "    if size_element:\n",
    "        return size_element.get_text(strip=True)\n",
    "    for alt in [\n",
    "        soup.find(class_=re.compile(r\"weight\", re.I)),\n",
    "        soup.find(class_=re.compile(r\"size\", re.I)),\n",
    "        soup.find(class_=re.compile(r\"volume\", re.I)),\n",
    "        soup.find(class_=re.compile(r\"quantity\", re.I)),\n",
    "    ]:\n",
    "        if alt and alt.get_text(strip=True):\n",
    "            text = alt.get_text(strip=True)\n",
    "            m = re.search(r\"\\d+\\,?\\d*\\s*(?:ml|g|mg|kg|l|oz)\", text, re.I)\n",
    "            if m: return m.group()\n",
    "    return None\n",
    "\n",
    "def extract_name(soup: BeautifulSoup):\n",
    "    name_element = soup.find(\"span\", id=\"ProductNameTitle\")\n",
    "    if name_element:\n",
    "        return name_element.get_text(strip=True)\n",
    "    for alt in [soup.find(\"h1\"), soup.find(\"h2\"), soup.find(\"title\"),\n",
    "                soup.find(class_=re.compile(r\"product.*title\", re.I)),\n",
    "                soup.find(class_=re.compile(r\"\\bname\\b\", re.I))]:\n",
    "        if alt and alt.get_text(strip=True):\n",
    "            return alt.get_text(strip=True)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570e9a0",
   "metadata": {},
   "source": [
    "## Ingredientes\n",
    "### Normaliza e padroniza usando a lista INGREDIENTES_VALIDOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b757b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _collect_collection_products_with_pagination(collection_url: str, max_pages: int = 20):\n",
    "    encontrados = set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        page_url = _add_or_replace_page_param(collection_url, page)\n",
    "        try:\n",
    "            r = requests.get(page_url, headers=HEADERS, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            anchors = soup.find_all(\"a\", href=True)\n",
    "            page_found = 0\n",
    "            for a in anchors:\n",
    "                href = a[\"href\"]\n",
    "                if \"/products/\" in href:\n",
    "                    text = a.get_text(\" \", strip=True) or \"\"\n",
    "                    if not text:\n",
    "                        img = a.find(\"img\", alt=True)\n",
    "                        if img and img.get(\"alt\"):\n",
    "                            text = img[\"alt\"]\n",
    "                    if text:\n",
    "                        page_found += 1\n",
    "                        encontrados.add(_strip_accents_lower(text))\n",
    "            if page_found == 0:\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"[skin-type-collect] Erro na página: {page_url} -> {e}\")\n",
    "            break\n",
    "    return encontrados\n",
    "\n",
    "def enrich_with_skin_types(products: list):\n",
    "    if not products:\n",
    "        return products\n",
    "\n",
    "    # 1) Sementes por texto (sinônimos)\n",
    "    for p in products:\n",
    "        if not (p.get(\"tipo_pele\") or \"\").strip():\n",
    "            nome = p.get(\"nome\", \"\")\n",
    "            subt = p.get(\"subtitulo\", \"\")\n",
    "            bene = p.get(\"beneficios\", \"\")\n",
    "            initial = classify_skin_types_from_texts(nome, subt, bene)\n",
    "            if initial:\n",
    "                p[\"tipo_pele\"] = \"; \".join(initial)\n",
    "\n",
    "    # 2) Enriquecimento por filtros (se SKIN_FILTER_URLS estiver preenchido)\n",
    "    if SKIN_FILTER_URLS:\n",
    "        print(\"\\nColetando tipos de pele via filtros da Sallve...\")\n",
    "        name_to_idx = {}\n",
    "        for idx, p in enumerate(products):\n",
    "            n = _strip_accents_lower(p.get(\"nome\", \"\"))\n",
    "            if n:\n",
    "                name_to_idx.setdefault(n, set()).add(idx)\n",
    "\n",
    "        for canonical, url in SKIN_FILTER_URLS.items():\n",
    "            print(f\" - Filtro '{canonical}': {url}\")\n",
    "            norm_names = _collect_collection_products_with_pagination(url, max_pages=20)\n",
    "            print(f\"   · {len(norm_names)} nome(s) coletado(s)\")\n",
    "            for nn in norm_names:\n",
    "                if nn in name_to_idx:\n",
    "                    for idx in name_to_idx[nn]:\n",
    "                        current = products[idx].get(\"tipo_pele\", \"\") or \"\"\n",
    "                        tipos = [t.strip() for t in current.split(\";\") if t.strip()]\n",
    "                        if canonical not in tipos:\n",
    "                            tipos.append(canonical)\n",
    "                        if SKIN_TYPE_CANONICAL_ORDER:\n",
    "                            order = {v: i for i, v in enumerate(SKIN_TYPE_CANONICAL_ORDER)}\n",
    "                            tipos = sorted(tipos, key=lambda x: order.get(x, 999))\n",
    "                        products[idx][\"tipo_pele\"] = \"; \".join(tipos)\n",
    "            time.sleep(0.6)\n",
    "\n",
    "    # 3) default se ainda vazio\n",
    "    for p in products:\n",
    "        if not (p.get(\"tipo_pele\") or \"\").strip():\n",
    "            p[\"tipo_pele\"] = \"todos os tipos\"\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50085d41",
   "metadata": {},
   "source": [
    "## Benefícios \n",
    "### Recebe lista, normaliza, filtra, padroniza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def padroniza_beneficios(textos_beneficios):\n",
    "    if not textos_beneficios: return []\n",
    "    encontrados = set()\n",
    "    norm_syn = {\n",
    "        canonico: [_strip_accents_lower(s) for s in patt_list if s]\n",
    "        for canonico, patt_list in BENEFIT_SYNONYMS_PT.items()\n",
    "    }\n",
    "    for txt in textos_beneficios:\n",
    "        n = _strip_accents_lower(txt)\n",
    "        for canonico, padds in norm_syn.items():\n",
    "            if any(patt in n for patt in padds):\n",
    "                encontrados.add(canonico)\n",
    "\n",
    "    if BENEFIT_CANONICAL_ORDER:\n",
    "        order_map = {name: i for i, name in enumerate(BENEFIT_CANONICAL_ORDER)}\n",
    "        return sorted(list(encontrados), key=lambda x: order_map.get(x, 999))\n",
    "    return sorted(list(encontrados))\n",
    "\n",
    "def extract_beneficios(soup: BeautifulSoup):\n",
    "    candidatos = []\n",
    "    for det in soup.find_all(\"details\", class_=re.compile(r\"\\bDifferentials\\b\", re.I)):\n",
    "        for li in det.find_all(\"li\"):\n",
    "            txt = li.get_text(\" \", strip=True)\n",
    "            if not txt: continue\n",
    "            tnorm = _strip_accents_lower(txt)\n",
    "            if re.fullmatch(r\"\\d+(?:[.,]\\d+)?\", tnorm):  # evita pontuações/numéricos\n",
    "                continue\n",
    "            if any(w in tnorm for w in [\"ponto\",\"pontos\",\"minha sallve\"]):\n",
    "                continue\n",
    "            if len(txt) <= 200:\n",
    "                candidatos.append(txt)\n",
    "\n",
    "    if not candidatos:\n",
    "        main = soup.find(\"article\", class_=re.compile(r\"\\bRegularMain__content\\b\"))\n",
    "        cont = main or soup\n",
    "        for li in cont.find_all(\"li\"):\n",
    "            txt = li.get_text(\" \", strip=True)\n",
    "            if txt and len(txt) <= 160:\n",
    "                candidatos.append(txt)\n",
    "    cats = padroniza_beneficios(candidatos)\n",
    "    return \"; \".join(cats)\n",
    "\n",
    "def _pick_best_from_srcset(srcset: str) -> str:\n",
    "    best_url, best_w = None, -1\n",
    "    for part in srcset.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part: continue\n",
    "        m = re.match(r\"(.+?)\\s+(\\d+)w\", part)\n",
    "        if m:\n",
    "            url, w = m.group(1).strip(), int(m.group(2))\n",
    "            if w > best_w:\n",
    "                best_url, best_w = url, w\n",
    "        else:\n",
    "            if best_url is None:\n",
    "                best_url = part\n",
    "    return best_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b25eb",
   "metadata": {},
   "source": [
    "## Imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f1dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_image_url(soup: BeautifulSoup) -> str | None:\n",
    "    img = soup.find(\"img\", class_=re.compile(r\"\\bview__image\\b\", re.I))\n",
    "    candidates = []\n",
    "    if img:\n",
    "        if img.get(\"srcset\"): candidates.append(_pick_best_from_srcset(img[\"srcset\"]))\n",
    "        if img.get(\"src\"):    candidates.append(img[\"src\"])\n",
    "    if not candidates:\n",
    "        for im in soup.find_all(\"img\"):\n",
    "            classes = \" \".join(im.get(\"class\", []))\n",
    "            if re.search(r\"(product|image|gallery|media|hero|view)\", classes, re.I):\n",
    "                if im.get(\"srcset\"):\n",
    "                    candidates.append(_pick_best_from_srcset(im[\"srcset\"]))\n",
    "                if im.get(\"src\"):\n",
    "                    candidates.append(im.get(\"src\"))\n",
    "    if not candidates:\n",
    "        meta = soup.find(\"meta\", property=\"og:image\")\n",
    "        if meta and meta.get(\"content\"):\n",
    "            candidates.append(meta[\"content\"])\n",
    "    for url in candidates:\n",
    "        if not url: continue\n",
    "        url = _safe_join_url(url.strip())\n",
    "        return urljoin(BASE_URL, url)\n",
    "    return None\n",
    "\n",
    "def _infer_ext_from_url(url: str) -> str:\n",
    "    path = urlparse(url).path\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".jpg\",\".jpeg\",\".png\",\".webp\",\".gif\"]:\n",
    "        return \".jpg\" if ext == \".jpeg\" else ext\n",
    "    return \".jpg\"\n",
    "\n",
    "def download_image(image_url: str, product_name: str) -> str | None:\n",
    "    if not image_url: return None\n",
    "    try:\n",
    "        r = requests.get(image_url, headers=HEADERS, timeout=40)\n",
    "        r.raise_for_status()\n",
    "        ext = _infer_ext_from_url(image_url)\n",
    "        base = _sanitize_filename(product_name)\n",
    "        filename = f\"{base}{ext}\"\n",
    "        dest = IMAGES_DIR / filename\n",
    "        counter = 1\n",
    "        while dest.exists():\n",
    "            filename = f\"{base}-{counter}{ext}\"\n",
    "            dest = IMAGES_DIR / filename\n",
    "            counter += 1\n",
    "        with open(dest, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"[download_image] Falha ao baixar {image_url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8085210",
   "metadata": {},
   "source": [
    "## Preço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b74b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(soup: BeautifulSoup) -> str | None:\n",
    "    for cls_pat in [r\"\\bTotalPrice\\b\", r\"\\bTotalPrice__CTA\\b\"]:\n",
    "        el = soup.find([\"strong\",\"span\"], class_=re.compile(cls_pat, re.I))\n",
    "        if el and el.get_text(strip=True):\n",
    "            p = _normalize_price_text(el.get_text(\" \", strip=True))\n",
    "            if p: return p\n",
    "\n",
    "    box = soup.find(class_=re.compile(r\"\\bProductPrice\\b\", re.I))\n",
    "    if box:\n",
    "        p = _normalize_price_text(box.get_text(\" \", strip=True))\n",
    "        if p: return p\n",
    "        strong = box.find(\"strong\")\n",
    "        if strong:\n",
    "            p = _normalize_price_text(strong.get_text(\" \", strip=True))\n",
    "            if p: return p\n",
    "\n",
    "    generic = soup.find(class_=re.compile(r\"price\", re.I))\n",
    "    if generic:\n",
    "        p = _normalize_price_text(generic.get_text(\" \", strip=True))\n",
    "        if p: return p\n",
    "\n",
    "    return _normalize_price_text(soup.get_text(\" \", strip=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40687f3e",
   "metadata": {},
   "source": [
    "## Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_data_from_soup(soup: BeautifulSoup, product_url: str, nome: str):\n",
    "    categoria = classify_category_from_name(nome)\n",
    "\n",
    "    beneficios_txt = extract_beneficios(soup) or \"\"\n",
    "    subtitulo_txt  = None  # ajuste se tiver seletor específico\n",
    "    tipos_detectados = classify_skin_types_from_texts(nome, subtitulo_txt or \"\", beneficios_txt)\n",
    "\n",
    "    data = {\n",
    "        \"marca\": \"sallve\",\n",
    "        \"nome\": nome,\n",
    "        \"categoria\": categoria,\n",
    "        \"subtitulo\": subtitulo_txt,\n",
    "        \"tamanho_quantidade\": extract_size(soup),\n",
    "        \"preco\": extract_price(soup),\n",
    "        \"ingredientes\": extract_ingredients(soup),\n",
    "        \"beneficios\": beneficios_txt,\n",
    "        \"tipo_pele\": \"; \".join(tipos_detectados) if tipos_detectados else \"\",\n",
    "        \"imagem\": \"\",\n",
    "        \"url\": product_url,\n",
    "    }\n",
    "\n",
    "    img_url = extract_image_url(soup)\n",
    "    if img_url:\n",
    "        img_filename = download_image(img_url, nome)\n",
    "        if img_filename:\n",
    "            data[\"imagem\"] = img_filename\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959df42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_soup(url: str) -> BeautifulSoup | None:\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        return BeautifulSoup(r.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"[fetch_soup] Erro em {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_sallve_products():\n",
    "    print(\"Iniciando webscraping da Sallve...\")\n",
    "    print(f\"Coletando produtos em {COLLECTION_URL} com paginação...\")\n",
    "    product_links = get_product_links_from_collection(max_pages=8)\n",
    "    if not product_links:\n",
    "        print(\"Nenhum produto encontrado na coleção. (Checar HTML/seletores)\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Encontrados {len(product_links)} links de produto (antes de filtro).\")\n",
    "    products = []\n",
    "\n",
    "    for i, url in enumerate(product_links, 1):\n",
    "        print(f\" [{i}/{len(product_links)}] {url}\")\n",
    "        soup = fetch_soup(url)\n",
    "        if not soup:\n",
    "            print(\"   ⚠️  Falha ao abrir página.\")\n",
    "            continue\n",
    "\n",
    "        nome = extract_name(soup)\n",
    "        if not nome:\n",
    "            print(\"   ⚠️  Nome não encontrado.\")\n",
    "            continue\n",
    "\n",
    "        if should_exclude_product(nome):\n",
    "            print(f\"   ❌ Excluído por keyword (models): {nome}\")\n",
    "            continue\n",
    "\n",
    "        prod = extract_product_data_from_soup(soup, url, nome)\n",
    "        products.append(prod)\n",
    "        print(f\"   ✅ OK: {nome}\")\n",
    "        time.sleep(0.7)\n",
    "\n",
    "    print(f\"Total pós-filtro: {len(products)}\")\n",
    "    return products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dd417",
   "metadata": {},
   "source": [
    "## Nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bdae99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57deac86",
   "metadata": {},
   "source": [
    "## Requisição -> html do produto-> extrai dados -> busca pelos links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b5a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10343f35",
   "metadata": {},
   "source": [
    "## Tipos de pele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SKIN_SYNONYMS_NORM = {\n",
    "    canon: [_strip_accents_lower(x) for x in lst if x]\n",
    "    for canon, lst in SKIN_TYPE_SYNONYMS_PT.items()\n",
    "}\n",
    "_SKIN_ORDER_MAP = {name: i for i, name in enumerate(SKIN_TYPE_CANONICAL_ORDER or [])}\n",
    "\n",
    "def classify_skin_types_from_texts(*strings: str) -> list[str]:\n",
    "    big = _strip_accents_lower(\" \".join(s for s in strings if s))\n",
    "    found = set()\n",
    "    for canon, pats in _SKIN_SYNONYMS_NORM.items():\n",
    "        if any(p and p in big for p in pats):\n",
    "            found.add(canon)\n",
    "    return sorted(found, key=lambda x: _SKIN_ORDER_MAP.get(x, 10_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64bb21",
   "metadata": {},
   "source": [
    "## Arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(products_data):\n",
    "    if not products_data:\n",
    "        print(\"Nenhum dado para salvar.\")\n",
    "        return\n",
    "\n",
    "    clean = []\n",
    "    for p in products_data:\n",
    "        clean.append({\n",
    "            \"marca\": p.get(\"marca\"),\n",
    "            \"nome\": p.get(\"nome\"),\n",
    "            \"categoria\": p.get(\"categoria\"),\n",
    "            \"subtitulo\": p.get(\"subtitulo\"),\n",
    "            \"tamanho_quantidade\": p.get(\"tamanho_quantidade\"),\n",
    "            \"preco\": p.get(\"preco\"),\n",
    "            \"ingredientes\": p.get(\"ingredientes\"),\n",
    "            \"beneficios\": p.get(\"beneficios\"),\n",
    "            \"tipo_pele\": p.get(\"tipo_pele\"),\n",
    "            \"imagem\": p.get(\"imagem\"),\n",
    "            \"url\": p.get(\"url\"),\n",
    "        })\n",
    "\n",
    "    with open(\"sallve_products.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clean, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    df = pd.DataFrame(clean)\n",
    "    df.to_csv(\"sallve_products.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\nDados salvos:\")\n",
    "    print(f\"  - JSON: sallve_products.json ({len(clean)} produtos)\")\n",
    "    print(f\"  - CSV:  sallve_products.csv  ({len(clean)} produtos)\")\n",
    "\n",
    "# ========= MAIN =========\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data = scrape_sallve_products()\n",
    "        data = enrich_with_skin_types(data)  # usa sinônimos + (opcional) filtros\n",
    "        save_data(data)\n",
    "        print(f\"\\nConcluído! Produtos extraídos: {len(data)}\")\n",
    "        print(f\"Imagens salvas em: {IMAGES_DIR.resolve()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERRO: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530eeb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando webscraping da Sallve...\n",
      "Coletando produtos em /collections/loja com paginação...\n",
      "Encontrados 138 links de produto (antes de filtro).\n",
      " [1/138] https://www.sallve.com.br/products/bastao-antioxidante-para-olhos\n",
      "\n",
      "ERRO: 'list' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
