{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a353db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Farmax] URLs a processar: 31\n",
      "[1/31] ok - https://www.farmax.com.br/agua-micelar-hialuronico-hidraderm-200ml/p\n",
      "[2/31] ok - https://www.farmax.com.br/agua-micelar-matte-hidraderm-200ml/p\n",
      "[3/31] ok - https://www.farmax.com.br/agua-micelar-vitamina-c-hidraderm-100ml/p\n",
      "[4/31] ok - https://www.farmax.com.br/agua-termal-hidraderm-ciclos-100ml/p\n",
      "[5/31] ok - https://www.farmax.com.br/contorno-de-olhos-hidraderm-ciclos-15g/p\n",
      "[6/31] ok - https://www.farmax.com.br/creme-hidratante-antissinais-hidraderm-ciclos-60g/p\n",
      "[7/31] ok - https://www.farmax.com.br/esfoliante-labial-hidraderm-ciclos-10g/p\n",
      "[8/31] ok - https://www.farmax.com.br/gel-de-limpeza-facial-vitamina-c-hidraderm-120g/p\n",
      "[9/31] ok - https://www.farmax.com.br/gel-de-limpeza-facial-vitamina-c-hidraderm-60g/p\n",
      "[10/31] ok - https://www.farmax.com.br/gel-hidratante-facial-antioleosidade-hidraderm-100g/p\n",
      "[11/31] ok - https://www.farmax.com.br/mascara-facial-argila-hidraderm-ciclos-60g/p\n",
      "[12/31] ok - https://www.farmax.com.br/mascara-facial-detox-hidraderm-ciclos-60g/p\n",
      "[13/31] ok - https://www.farmax.com.br/po-compacto-c-filtro-solar-fps-50-bege-medio-sunless-10g/p\n",
      "[14/31] ok - https://www.farmax.com.br/protetor-facial-fps50-gel-translucido-sunless-35g/p\n",
      "[15/31] ok - https://www.farmax.com.br/protetor-solar-stick-com-base-bronze-sunless-12g/p\n",
      "[16/31] ok - https://www.farmax.com.br/protetor-solar-stick-com-base-clara-sunless-12g/p\n",
      "[17/31] ok - https://www.farmax.com.br/protetor-solar-stick-com-base-extra-clara-sunless-12g/p\n",
      "[18/31] ok - https://www.farmax.com.br/protetor-solar-stick-com-base-media-sunless-12g/p\n",
      "[19/31] ok - https://www.farmax.com.br/sabonete--liquido-esfoliante--pitaya-hidraderm-180ml/p\n",
      "[20/31] ok - https://www.farmax.com.br/sabonete-liquido-esfoliante-camomila-hidraderm-180ml/p\n",
      "[21/31] ok - https://www.farmax.com.br/sabonete-liquido-esfoliante-coco-hidraderm-180ml/p\n",
      "[22/31] ok - https://www.farmax.com.br/sabonete-liquido-esfoliante-lavanda-hidraderm-180ml/p\n",
      "[23/31] ok - https://www.farmax.com.br/sabonete-liquido-esfoliante-maracuja-hidraderm-180ml/p\n",
      "[24/31] ok - https://www.farmax.com.br/sabonete-liquido-esfoliante-morango-hidraderm-180ml/p\n",
      "[25/31] ok - https://www.farmax.com.br/serum-acido-hialuronico-hidraderm-ciclos-30ml/p\n",
      "[26/31] ok - https://www.farmax.com.br/serum-facial-antissinais-beveg-30ml/p\n",
      "[27/31] ok - https://www.farmax.com.br/serum-multicorretivo-hidraderm-30ml/p\n",
      "[28/31] ok - https://www.farmax.com.br/serum-niacinamida-hidraderm-ciclos-30ml/p\n",
      "[29/31] ok - https://www.farmax.com.br/serum-retinol-hidraderm-ciclos-30ml/p\n",
      "[30/31] ok - https://www.farmax.com.br/serum-salicilico-hidraderm-ciclos-30ml/p\n",
      "[31/31] ok - https://www.farmax.com.br/tonico-glicolico-renovador-hidraderm-120ml/p\n",
      "Salvo JSON: farmax_products.json\n",
      "Salvo CSV : farmax_products.csv\n",
      "Itens válidos: 31\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys, os, re, json, time, unicodedata, io\n",
    "from gzip import BadGzipFile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, urlunparse\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ===== seus dicionários / regras =====\n",
    "sys.path.append(os.path.abspath(\"/home/usuario/Área de trabalho/Dados/models\"))\n",
    "from skin import SKIN_TYPE_CANONICAL_ORDER, SKIN_TYPE_SYNONYMS_PT\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "from category import CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS\n",
    "\n",
    "# ================== Config ==================\n",
    "BASE_URL = \"https://www.farmax.com.br\"\n",
    "JSON_PATH = Path(\"farmax_products.json\")\n",
    "CSV_PATH  = Path(\"farmax_products.csv\")\n",
    "\n",
    "IMG_DIR = Path(\"./images/farmax\")\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.1\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "})\n",
    "\n",
    "NBSP = \"\\xa0\"\n",
    "CURRENCY_RE = re.compile(r\"R\\$\\s*([\\d\\.]+,\\d{2})\")\n",
    "QTY_RE      = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*(ml|g|l|kg|un|unid|unidades)\\b\", re.I)\n",
    "\n",
    "CATEGORY_URLS: List[str] = [\n",
    "    \"https://www.farmax.com.br/skin-care/limpeza\",\n",
    "    \"https://www.farmax.com.br/skin-care/hidratantes\",\n",
    "    \"https://www.farmax.com.br/skin-care/seruns-de-tratamento\",\n",
    "    \"https://www.farmax.com.br/skin-care/protecao-solar\",\n",
    "    \"https://www.farmax.com.br/skin-care/mascaras-faciais\",\n",
    "    \"https://www.farmax.com.br/skin-care/esfoliantes\",\n",
    "    \"https://www.farmax.com.br/skin-care/agua-termal\",\n",
    "    \"https://www.farmax.com.br/skin-care/labiais\",\n",
    "]\n",
    "\n",
    "PDP_URLS: List[str] = [\n",
    "    # ex.: \"https://www.farmax.com.br/protetor-solar-toque-seco-sunless-fps60-120g/p\"\n",
    "]\n",
    "\n",
    "# ================== Helpers ==================\n",
    "def _strip(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = (s or \"\").replace(NBSP, \" \")\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = re.sub(r\"[\\s_/,-]+\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    s = _norm(s)\n",
    "    s = re.sub(r\"[^a-z0-9\\._-]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s or \"img\"\n",
    "\n",
    "def br_money_to_float_str(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    m = CURRENCY_RE.search(text)\n",
    "    if not m:\n",
    "        return None\n",
    "    raw = m.group(1).replace(\".\", \"\").replace(\",\", \".\")\n",
    "    try:\n",
    "        return f\"{float(raw):.2f}\"\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def extract_quantity(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    m = QTY_RE.search(text)\n",
    "    if not m:\n",
    "        return None\n",
    "    val = m.group(1).replace(\",\", \".\")\n",
    "    unit = m.group(2).lower()\n",
    "    unit = {\"unid\": \"un\", \"unidades\": \"un\"}.get(unit, unit)\n",
    "    return f\"{val}{unit}\" if unit in {\"ml\",\"g\",\"l\",\"kg\"} else f\"{val} {unit}\"\n",
    "\n",
    "def should_exclude(name: str) -> bool:\n",
    "    n = _norm(name)\n",
    "    for kw in list(EXCLUDE_KEYWORDS) + [\"kit\", \"refil\", \"refill\", \"combo\", \"duo\", \"trio\", \"necessaire\", \"presente\", \"gift\"]:\n",
    "        if _norm(kw) in n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_soup(url: str) -> Optional[BeautifulSoup]:\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                return BeautifulSoup(r.text, \"lxml\")\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(1 + i)\n",
    "    return None\n",
    "\n",
    "def collect_product_links_from_category(url: str) -> List[str]:\n",
    "    soup = get_soup(url)\n",
    "    if not soup:\n",
    "        return []\n",
    "    links = set()\n",
    "    for a in soup.select(\"a.vtex-product-summary-2-x-clearLink, a\"):\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        if not href:\n",
    "            continue\n",
    "        if href.startswith(\"//\"):\n",
    "            href = \"https:\" + href\n",
    "        elif href.startswith(\"/\"):\n",
    "            href = urljoin(BASE_URL, href)\n",
    "        if re.search(r\"/p(\\?|$)\", href):\n",
    "            links.add(href)\n",
    "    return sorted(links)\n",
    "\n",
    "# ================== PDP fields ==================\n",
    "def extract_name_subtitle_qty_farmax(soup: BeautifulSoup) -> Tuple[str, Optional[str], Optional[str]]:\n",
    "    name = \"\"\n",
    "    span = soup.select_one(\"span.vtex-store-components-3-x-productBrand, span[class*='vtex-store-components-3-x-productBrand']\")\n",
    "    if span:\n",
    "        name = _strip(span.get_text())\n",
    "    subtitle = None\n",
    "    qty = extract_quantity(name)\n",
    "    return name, subtitle, qty\n",
    "\n",
    "def extract_price_str_farmax(soup: BeautifulSoup) -> Optional[str]:\n",
    "    prices = set()\n",
    "    for el in soup.select(\"span.locateoffer__price, span, div, p\"):\n",
    "        val = br_money_to_float_str(el.get_text(\" \", strip=True))\n",
    "        if val:\n",
    "            prices.add(val)\n",
    "    return f\"{min(prices, key=lambda x: float(x))}\" if prices else None\n",
    "\n",
    "def extract_skin_text_farmax(soup: BeautifulSoup) -> str:\n",
    "    blocks = soup.select(\"div.vtex-store-components-3-x-specificationsTab\")\n",
    "    longest = \"\"\n",
    "    for b in blocks:\n",
    "        t = _strip(b.get_text(\" \", strip=True))\n",
    "        if len(t) > len(longest):\n",
    "            longest = t\n",
    "    return longest\n",
    "\n",
    "def map_skin_types_from_text(text: str) -> List[str]:\n",
    "    t = _norm(text)\n",
    "    if \"todos os tipos de pele\" in t:\n",
    "        return [\"todos os tipos\"]\n",
    "    out = set()\n",
    "    for canonical, syns in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for s in syns + [canonical]:\n",
    "            if _norm(s) in t:\n",
    "                out.add(canonical)\n",
    "                break\n",
    "    ordered = [s for s in SKIN_TYPE_CANONICAL_ORDER if s in out]\n",
    "    return ordered or [\"todos os tipos\"]\n",
    "\n",
    "def extract_benefits_farmax(soup: BeautifulSoup) -> List[str]:\n",
    "    candidates = []\n",
    "    for el in soup.select(\"li, p\"):\n",
    "        txt = _strip(el.get_text(\" \", strip=True))\n",
    "        if re.match(r\"^[•\\-\\u2022]\\s*\", txt):\n",
    "            txt = re.sub(r\"^[•\\-\\u2022]\\s*\", \"\", txt).strip()\n",
    "            if 3 <= len(txt) <= 240:\n",
    "                candidates.append(txt)\n",
    "    spec_text = extract_skin_text_farmax(soup)\n",
    "    if spec_text:\n",
    "        for sent in re.split(r\"[.;]\\s+\", spec_text):\n",
    "            s = _strip(sent)\n",
    "            if any(k in _norm(s) for k in [\"hidrata\", \"toque seco\", \"prote\", \"macia\", \"revitaliz\", \"antissinais\", \"uniformiza\", \"oleos\", \"matte\", \"uva\", \"uvb\"]):\n",
    "                candidates.append(s)\n",
    "    found = set()\n",
    "    joined = \" \" + \" ; \".join(_norm(x) for x in candidates) + \" \"\n",
    "    for canonical in BENEFIT_CANONICAL_ORDER:\n",
    "        syns = BENEFIT_SYNONYMS_PT.get(canonical, [])\n",
    "        if any(re.search(rf\"\\b{re.escape(_norm(s))}\\b\", joined) for s in syns + [canonical]):\n",
    "            found.add(canonical)\n",
    "    return [b for b in BENEFIT_CANONICAL_ORDER if b in found]\n",
    "\n",
    "def extract_ingredients_text_farmax(soup: BeautifulSoup) -> str:\n",
    "    for tag in soup.find_all([\"button\", \"div\", \"span\", \"p\", \"h2\", \"h3\", \"h4\"]):\n",
    "        label = _strip(tag.get_text())\n",
    "        if _norm(label) in {\"ingredientes\", \"composicao\", \"composição\"}:\n",
    "            sib = tag.find_next_sibling()\n",
    "            steps = 0\n",
    "            chunks = []\n",
    "            while sib and steps < 12:\n",
    "                for s in sib.select(\"span, p, div\"):\n",
    "                    t = _strip(s.get_text(\" \", strip=True))\n",
    "                    if len(t) > 20:\n",
    "                        chunks.append(t)\n",
    "                sib = sib.find_next_sibling()\n",
    "                steps += 1\n",
    "            if chunks:\n",
    "                chunks.sort(key=len, reverse=True)\n",
    "                return chunks[0]\n",
    "    candidate = \"\"\n",
    "    for el in soup.select(\"span, p, div\"):\n",
    "        tt = _strip(el.get_text(\" \", strip=True))\n",
    "        if (tt.count(\",\") >= 6 or \"/\" in tt) and len(tt) > len(candidate):\n",
    "            candidate = tt\n",
    "    return candidate\n",
    "\n",
    "def tokenize_and_filter_ingredients(raw_text: str) -> List[str]:\n",
    "    if not raw_text:\n",
    "        return []\n",
    "    parts: List[str] = []\n",
    "    for chunk in re.split(r\",\", raw_text):\n",
    "        parts.extend(re.split(r\"/\", chunk))\n",
    "    valid_norm_map = {_norm(v): v for v in INGREDIENTES_VALIDOS}\n",
    "    out, seen = [], set()\n",
    "    for p in parts:\n",
    "        tok = _strip(p).strip().strip(\".\")\n",
    "        if not tok:\n",
    "            continue\n",
    "        key = _norm(tok)\n",
    "        matched = None\n",
    "        if key in valid_norm_map:\n",
    "            matched = valid_norm_map[key]\n",
    "        else:\n",
    "            for k_valid, v_canon in valid_norm_map.items():\n",
    "                if k_valid and (k_valid in key or key in k_valid) and len(k_valid) >= 4:\n",
    "                    matched = v_canon\n",
    "                    break\n",
    "        if matched:\n",
    "            k2 = _norm(matched)\n",
    "            if k2 not in seen:\n",
    "                seen.add(k2)\n",
    "                out.append(matched)\n",
    "    return out\n",
    "\n",
    "def classify_category(name: str, subtitle: Optional[str]) -> Optional[str]:\n",
    "    base = f\"{name} {subtitle or ''}\"\n",
    "    text = _norm(base)\n",
    "    hits = []\n",
    "    for cat, hints in CATEGORY_HINTS.items():\n",
    "        if any(_norm(h) in text for h in hints):\n",
    "            hits.append(cat)\n",
    "    if not hits:\n",
    "        return None\n",
    "    for cat in CATEGORY_CANONICAL_ORDER:\n",
    "        if cat in hits:\n",
    "            return cat\n",
    "    return hits[0]\n",
    "\n",
    "# ================== Imagens (100%) ==================\n",
    "SRCSET_RE = re.compile(r\"\\s*(\\S+)\\s+(\\d+)w\\s*\")\n",
    "\n",
    "def _parse_srcset(srcset: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Retorna [(url, width_int), ...] a partir de um srcset.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    if not srcset:\n",
    "        return out\n",
    "    for part in srcset.split(\",\"):\n",
    "        part = part.strip()\n",
    "        m = SRCSET_RE.match(part)\n",
    "        if m:\n",
    "            url, w = m.group(1), int(m.group(2))\n",
    "            out.append((url, w))\n",
    "        else:\n",
    "            # pode vir sem 'w' (raro). Tenta pegar só a URL.\n",
    "            url = part.split()[0]\n",
    "            if url:\n",
    "                out.append((url, 0))\n",
    "    return out\n",
    "\n",
    "def _canonicalize_url(u: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza a URL para deduplicar (remove fragmento, mantém query porque define tamanho).\n",
    "    \"\"\"\n",
    "    pr = urlparse(u)\n",
    "    return urlunparse((pr.scheme or \"https\", pr.netloc, pr.path, \"\", pr.query, \"\"))\n",
    "\n",
    "def extract_all_product_image_urls_farmax(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"\n",
    "    Coleta TODAS as imagens de produto do PDP (src + todos do srcset),\n",
    "    prioriza vtexassets/arquivos/files/assets, e ordena por largura desc.\n",
    "    \"\"\"\n",
    "    blacklist = (\"logo\", \"icone\", \"icon\", \"sprite\", \"favicon\", \"/icons/\")\n",
    "    def _bad(u: str) -> bool:\n",
    "        lu = (u or \"\").lower()\n",
    "        return any(b in lu for b in blacklist)\n",
    "\n",
    "    candidates: Dict[str, int] = {}  # url -> width score\n",
    "\n",
    "    # 1) picture/source[srcset]\n",
    "    for src in soup.select(\"picture source[srcset]\"):\n",
    "        for url, w in _parse_srcset(src.get(\"srcset\", \"\")):\n",
    "            if any(k in url for k in (\"vtexassets\", \"/arquivos/\", \"/files/\", \"/assets/\")) and not _bad(url):\n",
    "                candidates[_canonicalize_url(url)] = max(candidates.get(url, 0), w)\n",
    "\n",
    "    # 2) img (classe principal e demais)\n",
    "    for img in soup.select(\"img.vtex-store-components-3-x-productImageTag, img\"):\n",
    "        # src direto\n",
    "        s = img.get(\"src\")\n",
    "        if s and any(k in s for k in (\"vtexassets\", \"/arquivos/\", \"/files/\", \"/assets/\")) and not _bad(s):\n",
    "            candidates[_canonicalize_url(s)] = max(candidates.get(s, 0), 0)\n",
    "        # srcset do <img>\n",
    "        for url, w in _parse_srcset(img.get(\"srcset\", \"\")):\n",
    "            if any(k in url for k in (\"vtexassets\", \"/arquivos/\", \"/files/\", \"/assets/\")) and not _bad(url):\n",
    "                candidates[_canonicalize_url(url)] = max(candidates.get(url, 0), w)\n",
    "\n",
    "    # ordena por largura desc (w), depois por comprimento do path\n",
    "    def score(u: str) -> Tuple[int, int]:\n",
    "        pr = urlparse(u)\n",
    "        return (candidates.get(u, 0), len(pr.path))\n",
    "\n",
    "    ordered = sorted(set(candidates.keys()), key=score, reverse=True)\n",
    "    return ordered\n",
    "\n",
    "def _ext_from_url(u: str) -> str:\n",
    "    p = urlparse(u).path.lower()\n",
    "    for ext in (\".jpg\", \".jpeg\", \".png\", \".webp\"):\n",
    "        if p.endswith(ext):\n",
    "            return ext\n",
    "    return \".jpg\"\n",
    "\n",
    "def download_images(img_urls: List[str], product_name: str, referer: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Baixa TODAS as imagens (ou as que conseguirmos) e retorna a lista de filenames salvos.\n",
    "    \"\"\"\n",
    "    saved = []\n",
    "    base = sanitize_filename(product_name)\n",
    "    for idx, url in enumerate(img_urls, 1):\n",
    "        if not url:\n",
    "            continue\n",
    "        # absolutiza\n",
    "        if url.startswith(\"//\"):\n",
    "            url = \"https:\" + url\n",
    "        elif url.startswith(\"/\"):\n",
    "            url = urljoin(BASE_URL, url)\n",
    "\n",
    "        ext = _ext_from_url(url)\n",
    "        fname = f\"{base}-{idx}{ext}\"\n",
    "        fpath = IMG_DIR / fname\n",
    "\n",
    "        try:\n",
    "            # set Referer para este download\n",
    "            headers = SESSION.headers.copy()\n",
    "            headers[\"Referer\"] = referer\n",
    "\n",
    "            r = SESSION.get(url, timeout=40, headers=headers)\n",
    "            if r.status_code == 200:\n",
    "                ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "                if not ctype.startswith(\"image\"):\n",
    "                    continue\n",
    "                if len(r.content) < 6000:  # ~6 KB: evita placeholders muito pequenos\n",
    "                    continue\n",
    "                with open(fpath, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                saved.append(fname)\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return saved\n",
    "\n",
    "# ================== Parse completo ==================\n",
    "def parse_farmax_product(url: str) -> Optional[Dict]:\n",
    "    soup = get_soup(url)\n",
    "    if not soup:\n",
    "        return None\n",
    "\n",
    "    name, subtitle, qty = extract_name_subtitle_qty_farmax(soup)\n",
    "    if not name or should_exclude(name):\n",
    "        return None\n",
    "\n",
    "    price = extract_price_str_farmax(soup)\n",
    "    skin_text = extract_skin_text_farmax(soup)\n",
    "    skin_types = map_skin_types_from_text(skin_text)\n",
    "    benefits = extract_benefits_farmax(soup)\n",
    "\n",
    "    ing_text = extract_ingredients_text_farmax(soup)\n",
    "    ingredients = tokenize_and_filter_ingredients(ing_text)\n",
    "\n",
    "    category = classify_category(name, subtitle)\n",
    "\n",
    "    # === IMAGENS: pega todas e baixa todas; no JSON/CSV mantém só a 1ª em 'imagem' ===\n",
    "    all_img_urls = extract_all_product_image_urls_farmax(soup)\n",
    "    saved_files = download_images(all_img_urls, name, referer=url)\n",
    "    main_image = saved_files[0] if saved_files else None\n",
    "\n",
    "    return {\n",
    "        \"marca\": \"farmax\",\n",
    "        \"nome\": name,\n",
    "        \"subtitulo\": subtitle if subtitle else None,\n",
    "        \"categoria\": category,\n",
    "        \"quantidade\": qty,\n",
    "        \"preco\": price,\n",
    "        \"beneficios\": \"; \".join(benefits) if benefits else None,\n",
    "        \"ingredientes\": \"; \".join(ingredients) if ingredients else None,\n",
    "        \"tipo_pele\": \"; \".join(skin_types) if skin_types else None,\n",
    "        \"imagem\": main_image,          # primeira salva\n",
    "        \"url\": url,                    # útil para auditoria\n",
    "        # As demais imagens ficam salvas no disco como {slug}-2.jpg, {slug}-3.jpg, ...\n",
    "    }\n",
    "\n",
    "# ================== Main ==================\n",
    "def main():\n",
    "    product_urls: List[str] = []\n",
    "    for cat in CATEGORY_URLS:\n",
    "        product_urls.extend(collect_product_links_from_category(cat))\n",
    "    product_urls.extend(PDP_URLS)\n",
    "\n",
    "    # de-dup por URL\n",
    "    product_urls = sorted(set(product_urls))\n",
    "    print(f\"[Farmax] URLs a processar: {len(product_urls)}\")\n",
    "\n",
    "    results = []\n",
    "    seen_urls = set()\n",
    "    for i, url in enumerate(product_urls, 1):\n",
    "        if url in seen_urls:\n",
    "            print(f\"[{i}/{len(product_urls)}] pulado (URL repetida) - {url}\")\n",
    "            continue\n",
    "        seen_urls.add(url)\n",
    "\n",
    "        try:\n",
    "            it = parse_farmax_product(url)\n",
    "            status = \"ok\" if it else \"descartado\"\n",
    "            if it:\n",
    "                results.append(it)\n",
    "        except Exception as e:\n",
    "            status = f\"erro:{e.__class__.__name__}\"\n",
    "        print(f\"[{i}/{len(product_urls)}] {status} - {url}\")\n",
    "        time.sleep(0.15)\n",
    "\n",
    "    with open(JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    pd.DataFrame(results).to_csv(CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Salvo JSON: {JSON_PATH}\")\n",
    "    print(f\"Salvo CSV : {CSV_PATH}\")\n",
    "    print(f\"Itens válidos: {len(results)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
