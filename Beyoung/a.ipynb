{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c4d6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs de produtos encontrados na coleção: 25\n",
      "Registros válidos coletados: 23\n",
      "CSV salvo em:  beyoung_skincare.csv\n",
      "JSON salvo em: beyoung_skincare.json\n",
      "Imagens salvas em: /home/usuario/Área de trabalho/Dados/Beyoung/images\n",
      "{'site': 'beyoung', 'nome': 'Sérum facial com Retinol + Niacinamida', 'subtitulo': 'Aging Care', 'preco': '129.90', 'quantidade': '30ml', 'beneficios': 'hidratação, limpeza, antissinais, uniformiza o tom, antioxidante, esfoliação, acalma, luminosidade, fortalece a barreira, proteção solar, fortalece os fios, compatível com maquiagem, resultados rápidos', 'ingredientes': 'ácido cítrico, ácido glicólico, ácido hialurônico, ácido linoleico, ácido salicílico, ácido tranexâmico, água de maçã, alpha arbutin, alpha glucan, cafeína, esqualano, fenoxietanol, hipoalergênico, lecitina, lha, niacinamida, pantenol, propanodiol, retinol, vitamina c, vitamina e', 'tipo_pele': 'oleosa, seca, normal, sensível, madura, com flacidez, com manchas, radiante, todos os tipos', 'categoria': 'sérum', 'imagem': 'serum-facial-com-retinol-niacinamida.jpg', 'url': 'https://www.beyoung.com.br/products/aging-care'}\n",
      "{'site': 'beyoung', 'nome': 'Água Micelar Hidratante', 'subtitulo': 'Micellar Water', 'preco': '44.86', 'quantidade': '200ml', 'beneficios': 'hidratação, limpeza, controle da oleosidade, antissinais, antioxidante, esfoliação, acalma, proteção solar, crescimento de cílios/sobrancelhas, compatível com maquiagem', 'ingredientes': 'ácido glicólico, ácido hialurônico, ácido salicílico, ácido tranexâmico, água de maçã, alpha arbutin, alpha glucan, cafeína, esqualano, lha, niacinamida, pantenol, retinol, vitamina c, vitamina e', 'tipo_pele': 'oleosa, seca, mista, normal, sensível, madura, com manchas, radiante, todos os tipos', 'categoria': 'água micelar', 'imagem': 'agua-micelar-hidratante.jpg', 'url': 'https://www.beyoung.com.br/products/agua-micelar-hidratante'}\n",
      "{'site': 'beyoung', 'nome': 'Água Micelar Hidratante', 'subtitulo': 'Micellar Water', 'preco': '44.86', 'quantidade': '200ml', 'beneficios': 'hidratação, limpeza, controle da oleosidade, antissinais, antioxidante, esfoliação, acalma, proteção solar, crescimento de cílios/sobrancelhas, compatível com maquiagem', 'ingredientes': 'ácido glicólico, ácido hialurônico, ácido salicílico, ácido tranexâmico, água de maçã, alpha arbutin, alpha glucan, cafeína, esqualano, lha, niacinamida, pantenol, retinol, vitamina c, vitamina e', 'tipo_pele': 'oleosa, seca, mista, normal, sensível, madura, com manchas, radiante, todos os tipos', 'categoria': 'água micelar', 'imagem': 'agua-micelar-hidratante-1.jpg', 'url': 'https://www.beyoung.com.br/products/agua-micelar-hidratante#judgeme_product_reviews'}\n",
      "{'site': 'beyoung', 'nome': 'Hidratante facial em gel com ácido hialurônico (pele refrescante)', 'subtitulo': 'Hydra Gel', 'preco': '52.72', 'quantidade': '30ml', 'beneficios': 'hidratação, limpeza, controle da oleosidade, antissinais, uniformiza o tom, antioxidante, luminosidade, fortalece a barreira, fortalece os fios, compatível com maquiagem', 'ingredientes': 'ácido glicólico, ácido hialurônico, ácido salicílico, ácido tranexâmico, água de maçã, alpha arbutin, alpha glucan, cafeína, disodium edta, esqualano, glycerin, lha, niacinamida, pantenol, phenoxyethanol, retinol, vitamina c, vitamina e', 'tipo_pele': 'oleosa, seca, normal, sensível, madura, com flacidez, com manchas, radiante, todos os tipos', 'categoria': 'hidratante', 'imagem': 'hidratante-facial-em-gel-com-acido-hialuronico-pele-refrescante.jpg', 'url': 'https://www.beyoung.com.br/products/beyoung-hydra-gel'}\n",
      "{'site': 'beyoung', 'nome': 'Sérum Facial Booster Multifuncional 3 em 1', 'subtitulo': 'Sérum Multifuncional Antioxidante', 'preco': '69.90', 'quantidade': '30ml', 'beneficios': 'hidratação, limpeza, controle da oleosidade, antissinais, uniformiza o tom, antioxidante, acalma, luminosidade, fortalece a barreira, proteção solar, fortalece os fios, compatível com maquiagem', 'ingredientes': 'ácido glicólico, ácido hialurônico, ácido salicílico, ácido tranexâmico, água de maçã, alpha arbutin, alpha glucan, benzoato de sódio, cafeína, esqualano, fenoxietanol, lha, niacinamida, pantenol, peptídeo de cobre, retinol, vitamina c, vitamina e', 'tipo_pele': 'oleosa', 'categoria': 'sérum', 'imagem': 'serum-facial-booster-multifuncional-3-em-1.jpg', 'url': 'https://www.beyoung.com.br/products/booster'}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Extração Beyoung (collections/skincare) — célula única para Jupyter\n",
    "# Requisitos: requests, selectolax\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import time, sys, os, unicodedata, json\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "# ===== Imports de modelos =====\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "\n",
    "# Tenta importar hints/ordem de categorias\n",
    "try:\n",
    "    from category import CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS\n",
    "except Exception:\n",
    "    CATEGORY_CANONICAL_ORDER = [\n",
    "        \"óleo de limpeza\",\"balm demaquilante\",\"água micelar\",\"gel de limpeza\",\n",
    "        \"espuma/mousse de limpeza\",\"sabonete facial\",\"tônico/essência/loção\",\n",
    "        \"tônico esfoliante (ácido)\",\"sérum\",\"ampola/ampoule\",\n",
    "        \"hidratante\",\"gel-creme\",\"creme\",\"loção\",\"óleo facial\",\"máscara facial\",\n",
    "        \"barreira/repair/recuperador\",\n",
    "        \"tratamento para área dos olhos\",\"hidratante labial\",\"máscara labial\",\n",
    "        \"tratamento antiacne (spot/gel)\",\"clareador/anti-manchas\",\"antissinais/firmador\",\n",
    "        \"controle de oleosidade (matificante)\",\"calmante/antirresíduo/sensibilidade\",\n",
    "        \"protetor solar\",\n",
    "        \"hidratante corporal\",\"óleo corporal\",\"esfoliante corporal\",\"sabonete corporal\",\n",
    "        \"protetor solar corporal\",\"creme para mãos/pés\",\"desodorante\",\n",
    "        \"primer skincare\",\"bruma facial\",\"spray/névoa hidratante\",\n",
    "    ]\n",
    "    CATEGORY_HINTS = {}\n",
    "\n",
    "# ==== Configs básicas ====\n",
    "BASE = \"https://www.beyoung.com.br\"\n",
    "COLLECTION_URL = \"https://www.beyoung.com.br/collections/skincare\"\n",
    "SITE_LABEL = \"beyoung\"\n",
    "OUT_CSV = \"beyoung_skincare.csv\"\n",
    "OUT_JSON = \"beyoung_skincare.json\"\n",
    "IMAGES_DIR = \"images\"   # <- pasta de imagens\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.8\"\n",
    "}\n",
    "\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# ==== Utils ====\n",
    "def get_html(url: str) -> Optional[HTMLParser]:\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        if resp.ok:\n",
    "            return HTMLParser(resp.text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def text(node) -> str:\n",
    "    return (node.text().strip() if node else \"\").strip()\n",
    "\n",
    "def norm_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def norm_for_match(s: str) -> str:\n",
    "    s = norm_spaces(s).lower()\n",
    "    s = strip_accents(s)\n",
    "    return s\n",
    "\n",
    "def price_to_float(s: str) -> Optional[float]:\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"(\\d{1,3}(?:\\.\\d{3})*|\\d+)(?:,(\\d{2}))?\", s.replace(\"\\xa0\",\" \").replace(\"\\n\",\" \"))\n",
    "    if not m:\n",
    "        return None\n",
    "    inteiro = m.group(1).replace(\".\", \"\")\n",
    "    centavos = m.group(2) or \"00\"\n",
    "    try:\n",
    "        return float(f\"{inteiro}.{centavos}\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def has_excluded_keyword(name: str) -> bool:\n",
    "    n = norm_for_match(name)\n",
    "    for kw in EXCLUDE_KEYWORDS:\n",
    "        if kw and norm_for_match(kw) in n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def guess_quantity(name: str, fallback: str = \"\") -> str:\n",
    "    patterns = [\n",
    "        r\"(\\d+)\\s*(ml|g|kg|l|L)\\b\",\n",
    "        r\"(\\d+,\\d+)\\s*(ml|g|kg|l|L)\\b\",\n",
    "        r\"\\b(\\d{2,4})\\s*(ml)\\b\",\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, name, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            return \"\".join(m.groups()).replace(\" \", \"\")\n",
    "    return fallback.strip()\n",
    "\n",
    "# ==== Imagens ====\n",
    "def pick_best_from_srcset(srcset: str) -> Optional[str]:\n",
    "    best_url, best_w = None, -1\n",
    "    for part in srcset.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        m = re.match(r\"(.+?)\\s+(\\d+)w\", part)\n",
    "        if m:\n",
    "            url, w = m.group(1).strip(), int(m.group(2))\n",
    "            if w > best_w:\n",
    "                best_url, best_w = url, w\n",
    "        else:\n",
    "            if best_url is None:\n",
    "                best_url = part\n",
    "    return best_url\n",
    "\n",
    "def abs_url(href: str) -> str:\n",
    "    if not href:\n",
    "        return \"\"\n",
    "    if href.startswith(\"//\"):\n",
    "        return \"https:\" + href\n",
    "    if href.startswith(\"http\"):\n",
    "        return href\n",
    "    return urljoin(BASE, href)\n",
    "\n",
    "def infer_ext_from_url(u: str) -> str:\n",
    "    path = urlparse(u).path\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".jpg\", \".jpeg\", \".png\", \".webp\", \".gif\"):\n",
    "        return \".jpg\" if ext == \".jpeg\" else ext\n",
    "    return \".jpg\"\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    base = norm_for_match(name)\n",
    "    base = re.sub(r\"[^a-z0-9]+\", \"-\", base).strip(\"-\")\n",
    "    base = re.sub(r\"-{2,}\", \"-\", base)\n",
    "    return base or \"produto\"\n",
    "\n",
    "def download_image(image_url: str, product_name: str) -> Optional[str]:\n",
    "    if not image_url:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(image_url, headers=HEADERS, timeout=40)\n",
    "        r.raise_for_status()\n",
    "        ext = infer_ext_from_url(image_url)\n",
    "        base = sanitize_filename(product_name)\n",
    "        filename = f\"{base}{ext}\"\n",
    "        dest = os.path.join(IMAGES_DIR, filename)\n",
    "        counter = 1\n",
    "        while os.path.exists(dest):\n",
    "            filename = f\"{base}-{counter}{ext}\"\n",
    "            dest = os.path.join(IMAGES_DIR, filename)\n",
    "            counter += 1\n",
    "        with open(dest, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return filename\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_main_image_url(doc: HTMLParser) -> Optional[str]:\n",
    "    # og:image primeiro\n",
    "    og = doc.css_first('meta[property=\"og:image\"]')\n",
    "    if og:\n",
    "        content = og.attributes.get(\"content\", \"\")\n",
    "        if content:\n",
    "            return abs_url(content)\n",
    "\n",
    "    # imagens do produto (srcset/src)\n",
    "    candidates = []\n",
    "    for sel in [\n",
    "        \"img.product__media\", \"img.product__image\", \"img\",  # genérico por último\n",
    "    ]:\n",
    "        for im in doc.css(sel):\n",
    "            srcset = im.attributes.get(\"srcset\", \"\")\n",
    "            src = im.attributes.get(\"src\", \"\")\n",
    "            if srcset:\n",
    "                best = pick_best_from_srcset(srcset)\n",
    "                if best:\n",
    "                    candidates.append(best)\n",
    "            if src:\n",
    "                candidates.append(src)\n",
    "    for u in candidates:\n",
    "        u = abs_url(u)\n",
    "        if u:\n",
    "            return u\n",
    "    return None\n",
    "\n",
    "# ==== Coleta de URLs da coleção ====\n",
    "def extract_product_urls_from_collection(doc: HTMLParser) -> List[str]:\n",
    "    urls = set()\n",
    "    for a in doc.css(\"a.full-unstyled-link\"):\n",
    "        href = a.attributes.get(\"href\", \"\")\n",
    "        if href and \"/products/\" in href:\n",
    "            urls.add(urljoin(BASE, href))\n",
    "    for a in doc.css(\"a.card__heading, a.product-grid-item, a.product-item\"):\n",
    "        href = a.attributes.get(\"href\", \"\")\n",
    "        if href and \"/products/\" in href:\n",
    "            urls.add(urljoin(BASE, href))\n",
    "    for a in doc.css(\"a\"):\n",
    "        href = a.attributes.get(\"href\", \"\")\n",
    "        if href and \"/products/\" in href:\n",
    "            urls.add(urljoin(BASE, href))\n",
    "    return sorted(urls)\n",
    "\n",
    "# ==== BENEFÍCIOS (models) ====\n",
    "def collect_benefits_text(doc: HTMLParser) -> str:\n",
    "    parts = []\n",
    "    for sel in [\n",
    "        \".product__description\", \".product__description.rte\", \".rte\",\n",
    "        \".product__accordion\", \".accordion__content\", \".product__text\",\n",
    "        \"section, article\"\n",
    "    ]:\n",
    "        for n in doc.css(sel):\n",
    "            t = norm_spaces(n.text())\n",
    "            if t and len(t) > 40:\n",
    "                parts.append(t)\n",
    "    joined = \" \".join(parts)\n",
    "    if not joined:\n",
    "        joined = norm_spaces(doc.body.text() if doc.body else \"\")\n",
    "    return joined\n",
    "\n",
    "def classify_benefits(doc: HTMLParser) -> str:\n",
    "    txt = norm_for_match(collect_benefits_text(doc))\n",
    "    found = set()\n",
    "    for canonical, synonyms in BENEFIT_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            if syn and norm_for_match(syn) in txt:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "    if not found:\n",
    "        return \"\"\n",
    "    ordered = [b for b in BENEFIT_CANONICAL_ORDER if b in found]\n",
    "    return \", \".join(ordered)\n",
    "\n",
    "# ==== INGREDIENTES (whitelist do models) ====\n",
    "def collect_ingredients_text(doc: HTMLParser) -> str:\n",
    "    parts = []\n",
    "    anchors = (\"COMPOSIÇÃO\", \"COMPOSICAO\", \"INGREDIENTES\", \"ATIVOS\", \"PRINCIPAIS ATIVOS\")\n",
    "    for node in doc.css(\"strong, b, h1, h2, h3\"):\n",
    "        title = norm_spaces(node.text()).upper()\n",
    "        if any(a in title for a in anchors):\n",
    "            hops, cur = 0, node\n",
    "            while cur and hops < 12:\n",
    "                cur = cur.next\n",
    "                if not cur: break\n",
    "                try:\n",
    "                    t = getattr(cur, \"text\", None)\n",
    "                    if t:\n",
    "                        val = norm_spaces(cur.text())\n",
    "                        if val:\n",
    "                            parts.append(val)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                hops += 1\n",
    "    if not parts:\n",
    "        parts = [norm_spaces(doc.body.text() if doc.body else \"\")]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def filter_ingredients(doc: HTMLParser) -> str:\n",
    "    txt = norm_for_match(collect_ingredients_text(doc))\n",
    "    hits = set()\n",
    "    for ing in INGREDIENTES_VALIDOS:\n",
    "        if norm_for_match(ing) in txt:\n",
    "            hits.add(ing)\n",
    "    if not hits:\n",
    "        return \"\"\n",
    "    return \", \".join(sorted(hits, key=lambda s: strip_accents(s).lower()))\n",
    "\n",
    "# ==== TIPO DE PELE (models) ====\n",
    "def collect_skin_text(doc: HTMLParser) -> str:\n",
    "    parts = []\n",
    "    anchors = (\n",
    "        \"PARA QUAIS TIPOS DE PELE\",\"TIPO DE PELE\",\"TIPOS DE PELE\",\n",
    "        \"PELE OLEOSA\",\"PELE MISTA\",\"PELE SECA\",\"PELE SENSÍVEL\",\"PELE SENSIVEL\",\n",
    "        \"TODOS OS TIPOS\"\n",
    "    )\n",
    "    for node in doc.css(\"strong, b, h1, h2, h3\"):\n",
    "        title = norm_spaces(node.text()).upper()\n",
    "        if any(a in title for a in anchors):\n",
    "            hops, cur = 0, node\n",
    "            while cur and hops < 10:\n",
    "                cur = cur.next\n",
    "                if not cur: break\n",
    "                try:\n",
    "                    t = getattr(cur, \"text\", None)\n",
    "                    if t:\n",
    "                        val = norm_spaces(cur.text())\n",
    "                        if val:\n",
    "                            parts.append(val)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                hops += 1\n",
    "    if not parts:\n",
    "        parts = [norm_spaces(doc.body.text() if doc.body else \"\")]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def classify_skin_types(doc: HTMLParser) -> str:\n",
    "    txt = norm_for_match(collect_skin_text(doc))\n",
    "    found = set()\n",
    "    for canonical, synonyms in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            if syn and norm_for_match(syn) in txt:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "    if not found:\n",
    "        return \"\"\n",
    "    ordered = [k for k in SKIN_TYPE_CANONICAL_ORDER if k in found]\n",
    "    return \", \".join(ordered)\n",
    "\n",
    "# ==== CATEGORIA (category hints/order) ====\n",
    "_CATEGORY_ORDER_MAP = {name: i for i, name in enumerate(CATEGORY_CANONICAL_ORDER)}\n",
    "\n",
    "def classify_category_from_name(name: str, subtitle: str | None = None) -> str:\n",
    "    if not CATEGORY_HINTS:\n",
    "        return \"\"\n",
    "    txt = norm_for_match(f\"{name or ''} {subtitle or ''}\")\n",
    "    hits = []\n",
    "    for cat, needles in CATEGORY_HINTS.items():\n",
    "        for n in needles:\n",
    "            if n and norm_for_match(n) in txt:\n",
    "                hits.append(cat)\n",
    "                break\n",
    "    if not hits:\n",
    "        return \"\"\n",
    "    hits.sort(key=lambda c: _CATEGORY_ORDER_MAP.get(c, 10_000))\n",
    "    return hits[0]\n",
    "\n",
    "# ==== Extração de um produto ====\n",
    "def extract_product_data(url: str) -> Optional[Dict]:\n",
    "    doc = get_html(url)\n",
    "    if not doc:\n",
    "        return None\n",
    "\n",
    "    # Nome\n",
    "    name_node = doc.css_first(\"h1.product__title\") or doc.css_first(\"h1.product__title.hd3\")\n",
    "    name = text(name_node)\n",
    "\n",
    "    # Subtítulo\n",
    "    subtitle_node = doc.css_first(\"p.product__text.inline-richtext\") or doc.css_first(\".product__subtitle, .product__text\")\n",
    "    subtitle = text(subtitle_node)\n",
    "\n",
    "    # Preço\n",
    "    price_selectors = [\n",
    "        \"span.f-price-item.f-price-item--sale\",\n",
    "        \"span.price-item.price-item--sale\",\n",
    "        \"span.price-item.price-item--regular\",\n",
    "        \"span.money\",\n",
    "        \"[data-product-price] .price-item--sale\",\n",
    "        \".price__container .price-item--sale\",\n",
    "        \".price__regular .price-item--regular\",\n",
    "    ]\n",
    "    raw_price = \"\"\n",
    "    for sel in price_selectors:\n",
    "        node = doc.css_first(sel)\n",
    "        if node and node.text().strip():\n",
    "            raw_price = node.text().strip()\n",
    "            break\n",
    "    price = price_to_float(raw_price)\n",
    "\n",
    "    # Quantidade\n",
    "    qty_node = doc.css_first('[data-selected-swatch-value=\"Tamanho\"]') or doc.css_first(\"label[for*='template'][for*='main'][for*='-0']\")\n",
    "    quantity = text(qty_node) or guess_quantity(name, \"\")\n",
    "\n",
    "    if not name or has_excluded_keyword(name):\n",
    "        return None\n",
    "\n",
    "    # Benefícios / Ingredientes / Tipo de pele\n",
    "    beneficios = classify_benefits(doc)\n",
    "    ingredientes = filter_ingredients(doc)\n",
    "    tipo_pele = classify_skin_types(doc)\n",
    "\n",
    "    # Categoria\n",
    "    categoria = classify_category_from_name(name, subtitle)\n",
    "\n",
    "    # Imagem\n",
    "    image_url = extract_main_image_url(doc)\n",
    "    image_filename = download_image(image_url, name) if image_url else None\n",
    "\n",
    "    return {\n",
    "        \"site\": SITE_LABEL,\n",
    "        \"nome\": name,\n",
    "        \"subtitulo\": subtitle,\n",
    "        \"preco\": f\"{price:.2f}\" if price is not None else \"\",\n",
    "        \"quantidade\": quantity,\n",
    "        \"beneficios\": beneficios,\n",
    "        \"ingredientes\": ingredientes,\n",
    "        \"tipo_pele\": tipo_pele,\n",
    "        \"categoria\": categoria,\n",
    "        \"imagem\": image_filename or \"\",   # <- nome do arquivo salvo (se baixado)\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "# ==== Paginação da coleção ====\n",
    "def paginate_collection(base_url: str, sleep_s: float = 0.8, max_pages: int = 50) -> List[str]:\n",
    "    all_urls = set()\n",
    "    page = 1\n",
    "    while page <= max_pages:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        doc = get_html(url)\n",
    "        if not doc:\n",
    "            break\n",
    "        urls = extract_product_urls_from_collection(doc)\n",
    "        if not urls:\n",
    "            if page == 1 and base_url != url:\n",
    "                doc0 = get_html(base_url)\n",
    "                if doc0:\n",
    "                    urls0 = extract_product_urls_from_collection(doc0)\n",
    "                    for u in urls0:\n",
    "                        all_urls.add(u)\n",
    "            break\n",
    "        for u in urls:\n",
    "            all_urls.add(u)\n",
    "        page += 1\n",
    "        time.sleep(sleep_s)\n",
    "    return sorted(all_urls)\n",
    "\n",
    "# ==== Escrita CSV/JSON ====\n",
    "def write_csv(rows: List[Dict], path: str):\n",
    "    fieldnames = [\n",
    "        \"site\",\"nome\",\"subtitulo\",\"preco\",\"quantidade\",\n",
    "        \"beneficios\",\"ingredientes\",\"tipo_pele\",\"categoria\",\"imagem\",\"url\"\n",
    "    ]\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "\n",
    "def write_json(rows: List[Dict], path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ==== Execução ====\n",
    "all_product_urls = paginate_collection(COLLECTION_URL)\n",
    "print(f\"URLs de produtos encontrados na coleção: {len(all_product_urls)}\")\n",
    "\n",
    "rows = []\n",
    "for i, purl in enumerate(all_product_urls, 1):\n",
    "    data = extract_product_data(purl)\n",
    "    if data:\n",
    "        rows.append(data)\n",
    "    time.sleep(0.6 if i % 3 else 1.0)\n",
    "\n",
    "write_csv(rows, OUT_CSV)\n",
    "write_json(rows, OUT_JSON)\n",
    "\n",
    "print(f\"Registros válidos coletados: {len(rows)}\")\n",
    "print(f\"CSV salvo em:  {OUT_CSV}\")\n",
    "print(f\"JSON salvo em: {OUT_JSON}\")\n",
    "print(f\"Imagens salvas em: {os.path.abspath(IMAGES_DIR)}\")\n",
    "\n",
    "# Visualização de amostra\n",
    "for r in rows[:5]:\n",
    "    print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
