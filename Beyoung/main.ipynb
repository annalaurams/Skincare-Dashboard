{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badd592f",
   "metadata": {},
   "source": [
    "# Beyoung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1ede23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, time, unicodedata, sys\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import List, Dict, Optional\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/usuario/Área de trabalho/Dados/models\"))\n",
    "\n",
    "from skin import (SKIN_TYPE_CANONICAL_ORDER, SKIN_TYPE_SYNONYMS_PT)\n",
    "from exclude import (EXCLUDE_KEYWORDS,)\n",
    "from ingredient import (INGREDIENTES_VALIDOS,)\n",
    "from benefits import (BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER)\n",
    "from category import (CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c12ba",
   "metadata": {},
   "source": [
    "## Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f19084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_URL = \"https://www.beyoung.com.br\"\n",
    "BASE_SKINCARE_URL = \"https://www.beyoung.com.br/collections/skincare\"\n",
    "\n",
    "CATEGORY_URLS = {\n",
    "    \"Limpeza\":        \"https://www.beyoung.com.br/collections/skincare-limpeza\",\n",
    "    \"Tratamento\":     \"https://www.beyoung.com.br/collections/skincare-tratamento\",\n",
    "    \"Hidratação\":     \"https://www.beyoung.com.br/collections/skincare-hidratacao\",\n",
    "    \"Proteção Solar\": \"https://www.beyoung.com.br/collections/protecao-solar\",\n",
    "}\n",
    "\n",
    "OUTPUT_JSON_PATH = \"/home/usuario/Área de trabalho/Dados/Beyoung/beyoung_scraper.json\"\n",
    "IMAGES_DIR = Path(\"images\")\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "}\n",
    "\n",
    "def build_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update(HEADERS)\n",
    "    retry = Retry(\n",
    "        total=5, backoff_factor=0.7,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"HEAD\", \"OPTIONS\"]\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
    "    return s\n",
    "\n",
    "SESSION = build_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500ddee",
   "metadata": {},
   "source": [
    "## Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5afcfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_accents_lower(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def sentence_case(s: str) -> str:\n",
    "    \"\"\"Primeira letra maiúscula, restante minúsculo (normaliza caixa alta).\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    s_low = s.lower()\n",
    "    return s_low[0].upper() + s_low[1:] if len(s_low) > 1 else s_low.upper()\n",
    "\n",
    "def slugify(name: str) -> str:\n",
    "    base = _strip_accents_lower(name)\n",
    "    base = re.sub(r\"[^a-z0-9]+\", \"-\", base).strip(\"-\")\n",
    "    base = re.sub(r\"-{2,}\", \"-\", base)\n",
    "    return base or \"produto\"\n",
    "\n",
    "def http_get(url: str, timeout: int = 30) -> Optional[bytes]:\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=timeout)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        low = r.text.lower()\n",
    "        if any(x in low for x in [\"please enable cookies\", \"attention required\", \"access denied\"]):\n",
    "            return None\n",
    "        return r.content\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def soup_from_url(url: str) -> Optional[BeautifulSoup]:\n",
    "    content = http_get(url)\n",
    "    if not content:\n",
    "        return None\n",
    "    return BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "def should_exclude(name: str) -> bool:\n",
    "    n = _strip_accents_lower(name)\n",
    "    for kw in EXCLUDE_KEYWORDS:\n",
    "        if not kw:\n",
    "            continue\n",
    "        if _strip_accents_lower(kw) in n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def normalize_price_text(txt: str) -> Optional[str]:\n",
    "    if not txt: return None\n",
    "    t = re.sub(r\"\\s+\", \" \", txt).replace(\"\\xa0\", \" \").strip()\n",
    "    m = re.search(r\"(?:R\\$)?\\s*(\\d{1,3}(?:\\.\\d{3})*,\\d{2})\", t)\n",
    "    if not m:\n",
    "        m = re.search(r\"(?:R\\$)?\\s*(\\d{1,3}(?:\\.\\d{3})*)\\b(?!,)\", t)\n",
    "        if m:\n",
    "            num = m.group(1).replace(\".\", \"\")\n",
    "            return f\"{float(num):.2f}\"\n",
    "    if m:\n",
    "        num = m.group(1).replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try:\n",
    "            return f\"{float(num):.2f}\"\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def safe_join_url(url: str) -> str:\n",
    "    return \"https:\" + url if url and url.startswith(\"//\") else url\n",
    "\n",
    "def infer_img_ext(url: str) -> str:\n",
    "    if not url:\n",
    "        return \".jpg\"\n",
    "    path = urlparse(url).path\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".jpg\", \".jpeg\", \".png\", \".webp\"]:\n",
    "        return \".jpg\" if ext == \".jpeg\" else ext\n",
    "    return \".jpg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e37710",
   "metadata": {},
   "source": [
    "## Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c6ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(img_url: str, product_name: str) -> Optional[str]:\n",
    "    if not img_url:\n",
    "        return None\n",
    "    try:\n",
    "        r = SESSION.get(\n",
    "            img_url,\n",
    "            headers={\n",
    "                **HEADERS,\n",
    "                \"Accept\": \"image/avif,image/webp,image/apng,image/*,*/*;q=0.8\",\n",
    "                \"Referer\": BASE_URL\n",
    "            },\n",
    "            timeout=40\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        ext = infer_img_ext(img_url)\n",
    "        base = slugify(product_name)\n",
    "        dest = IMAGES_DIR / f\"{base}{ext}\"\n",
    "        i = 1\n",
    "        while dest.exists():\n",
    "            dest = IMAGES_DIR / f\"{base}-{i}{ext}\"\n",
    "            i += 1\n",
    "        dest.write_bytes(r.content)\n",
    "        return dest.name\n",
    "    except Exception as e:\n",
    "        print(f\"[download_image] Falha: {img_url} -> {e}\")\n",
    "        return None\n",
    "\n",
    "#  Benefícios\n",
    "_norm_benefit_map = {\n",
    "    canonico: [_strip_accents_lower(x) for x in syns if x]\n",
    "    for canonico, syns in (BENEFIT_SYNONYMS_PT or {}).items()\n",
    "}\n",
    "_benefit_order = {b: i for i, b in enumerate(BENEFIT_CANONICAL_ORDER or [])}\n",
    "\n",
    "def standardize_benefits(raw_benefits: List[str]) -> str:\n",
    "    if not raw_benefits:\n",
    "        return \"\"\n",
    "    found = set()\n",
    "    for txt in raw_benefits:\n",
    "        n = _strip_accents_lower(txt)\n",
    "        for canon, pats in _norm_benefit_map.items():\n",
    "            if any(p and p in n for p in pats):\n",
    "                found.add(canon)\n",
    "    ordered = sorted(found, key=lambda x: _benefit_order.get(x, 999))\n",
    "    return \"; \".join(ordered)\n",
    "\n",
    "#  Ingredientes \n",
    "_valid_norm_map = {_strip_accents_lower(v): v for v in INGREDIENTES_VALIDOS}\n",
    "_valid_norm_list = list(_valid_norm_map.keys())\n",
    "_valid_order = {v: i for i, v in enumerate(INGREDIENTES_VALIDOS)}  # ordena como na sua lista\n",
    "\n",
    "def _collect_ativos_block_text(soup: BeautifulSoup) -> str:\n",
    "\n",
    "    for h in soup.find_all(re.compile(r\"h\\d\", re.I)):\n",
    "        txt = h.get_text(\" \", strip=True)\n",
    "        if txt and _strip_accents_lower(txt) == \"principais ativos\":\n",
    "\n",
    "            parts = []\n",
    "            sib = h.find_next_sibling()\n",
    "            # percorre até próximo heading\n",
    "            while sib and not re.match(r\"h\\d\", sib.name or \"\", re.I):\n",
    "                t = sib.get_text(\" \", strip=True)\n",
    "                if t:\n",
    "                    parts.append(t)\n",
    "                sib = sib.find_next_sibling()\n",
    "            if parts:\n",
    "                return \" \\n \".join(parts)\n",
    "\n",
    "    blocks = [el.get_text(\" \\n \", strip=True) for el in soup.select(\".metafield-multi_line_text_field\")]\n",
    "    if blocks:\n",
    "        return \" \\n \".join(blocks)\n",
    "\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "def _ingredients_from_text(block_text: str) -> List[str]:\n",
    "\n",
    "    if not block_text:\n",
    "        return []\n",
    "    nbig = _strip_accents_lower(block_text)\n",
    "    found_norm = set()\n",
    "    nbig = re.sub(r\"\\b\\d+[.,]?\\d*\\s*%\\b\", \" \", nbig)\n",
    "\n",
    "    for vnorm in _valid_norm_list:\n",
    "        if vnorm and vnorm in nbig:\n",
    "            found_norm.add(vnorm)\n",
    "\n",
    "    ordered = sorted(found_norm, key=lambda x: _valid_order.get(_valid_norm_map[x], 9999))\n",
    "    return [_valid_norm_map[vn] for vn in ordered]\n",
    "\n",
    "def extract_active_ingredients(soup: BeautifulSoup) -> str:\n",
    "    block = _collect_ativos_block_text(soup)\n",
    "    itens = _ingredients_from_text(block)\n",
    "    return \"; \".join(itens)\n",
    "\n",
    "# Categorias\n",
    "def extract_category_product_links(cat_url: str, categoria: str) -> List[Dict]:\n",
    "\n",
    "    soup = soup_from_url(cat_url)\n",
    "    if not soup:\n",
    "        return []\n",
    "\n",
    "    products = []\n",
    "    for a in soup.select('a[href*=\"/products/\"]'):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if \"/products/\" not in href:\n",
    "            continue\n",
    "        full = urljoin(BASE_URL, href)\n",
    "        name = a.get_text(\" \", strip=True) or \"\"\n",
    "        if not name:\n",
    "            img = a.find(\"img\", alt=True)\n",
    "            if img and img.get(\"alt\"):\n",
    "                name = img[\"alt\"].strip()\n",
    "        if not name:\n",
    "            name = href.rstrip(\"/\").split(\"/\")[-1].replace(\"-\", \" \").title()\n",
    "\n",
    "        if should_exclude(name):\n",
    "            continue\n",
    "\n",
    "        products.append({\"url\": full, \"categoria\": categoria, \"nome_card\": name})\n",
    "\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for p in products:\n",
    "        if p[\"url\"] not in seen:\n",
    "            seen.add(p[\"url\"])\n",
    "            dedup.append(p)\n",
    "    return dedup\n",
    "\n",
    "# Extrações na página de produto\n",
    "def extract_name(soup: BeautifulSoup) -> Optional[str]:\n",
    "    for sel in [\"h1.product__title\", \"h1\", \"meta[property='og:title']\", \"title\"]:\n",
    "        node = soup.select_one(sel)\n",
    "        if node:\n",
    "            txt = node.get(\"content\", \"\") if node.name == \"meta\" else node.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                return txt.strip()\n",
    "    return None\n",
    "\n",
    "def extract_price(soup: BeautifulSoup) -> Optional[str]:\n",
    "    candidates = [\n",
    "        \".price__container .price__current, .price__regular .price-item--regular\",\n",
    "        \".price__container .price-item--regular\",\n",
    "        \".product__price .price\",\n",
    "        \".f-price-item--current\",\n",
    "        \".price, .Price\",\n",
    "    ]\n",
    "    for sel in candidates:\n",
    "        node = soup.select_one(sel)\n",
    "        if node:\n",
    "            p = normalize_price_text(node.get_text(\" \", strip=True))\n",
    "            if p:\n",
    "                return p\n",
    "    p = normalize_price_text(soup.get_text(\" \", strip=True))\n",
    "    return p\n",
    "\n",
    "def extract_quantity(soup: BeautifulSoup) -> Optional[str]:\n",
    "    node = soup.select_one('[data-selected-swatch-value=\"Tamanho\"]')\n",
    "    if node and node.get_text(strip=True):\n",
    "        return node.get_text(strip=True)\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    m = re.search(r\"\\b\\d+\\,?\\d*\\s*(?:ml|g|mg|kg|l|oz)\\b\", text, re.I)\n",
    "    if m:\n",
    "        return m.group(0)\n",
    "    return None\n",
    "\n",
    "def extract_benefits(soup: BeautifulSoup) -> List[str]:\n",
    "    out = []\n",
    "    for el in soup.select(\".section__content .feature-list__items .feature-item__text\"):\n",
    "        txt = el.get_text(\" \", strip=True)\n",
    "        if txt and len(txt) <= 200:\n",
    "            out.append(txt)\n",
    "    if not out:\n",
    "        for li in soup.select(\"li\"):\n",
    "            t = li.get_text(\" \", strip=True)\n",
    "            if t and 3 <= len(t) <= 160:\n",
    "                out.append(t)\n",
    "    return out\n",
    "\n",
    "def extract_image_url(soup: BeautifulSoup) -> Optional[str]:\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        src = img.get(\"src\") or \"\"\n",
    "        if \"cdn/shop/files\" in src or \"cdn.shopify.com\" in src:\n",
    "            return safe_join_url(src.strip())\n",
    "    meta = soup.find(\"meta\", property=\"og:image\")\n",
    "    if meta and meta.get(\"content\"):\n",
    "        return safe_join_url(meta[\"content\"].strip())\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67006b4a",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_beyoung() -> List[Dict]:\n",
    "    print(\"Coletando links por categoria…\")\n",
    "    links = []\n",
    "    for categoria, url in CATEGORY_URLS.items():\n",
    "        items = extract_category_product_links(url, categoria)\n",
    "        print(f\" - {categoria}: {len(items)} links após filtro\")\n",
    "        links.extend(items)\n",
    "        time.sleep(0.6)\n",
    "\n",
    "    seen = set()\n",
    "    final_links = []\n",
    "    for it in links:\n",
    "        if it[\"url\"] not in seen:\n",
    "            seen.add(it[\"url\"])\n",
    "            final_links.append(it)\n",
    "\n",
    "    print(f\"Total de URLs únicas: {len(final_links)}\")\n",
    "    products = []\n",
    "\n",
    "    for i, item in enumerate(final_links, 1):\n",
    "        url = item[\"url\"]; categoria = item[\"categoria\"]\n",
    "        print(f\"[{i}/{len(final_links)}] {url}\")\n",
    "        soup = soup_from_url(url)\n",
    "        if not soup:\n",
    "            print(\"  ! Falha ao carregar página.\")\n",
    "            continue\n",
    "\n",
    "        nome_raw = extract_name(soup) or item.get(\"nome_card\") or \"\"\n",
    "        if not nome_raw:\n",
    "            print(\"  ! Nome não encontrado.\")\n",
    "            continue\n",
    "\n",
    "        nome = sentence_case(nome_raw)\n",
    "\n",
    "        if should_exclude(nome):\n",
    "            print(\"  - Excluído por keywords.\")\n",
    "            continue\n",
    "\n",
    "        preco = extract_price(soup) or \"\"\n",
    "        quantidade = extract_quantity(soup) or \"\"\n",
    "        raw_benefits = extract_benefits(soup)\n",
    "        beneficios = standardize_benefits(raw_benefits)\n",
    "\n",
    "        ingredientes = extract_active_ingredients(soup)  \n",
    "\n",
    "        img_url = extract_image_url(soup)\n",
    "        img_file = download_image(img_url, nome) if img_url else None\n",
    "\n",
    "        prod = {\n",
    "            \"marca\": \"beyoung\",\n",
    "            \"nome\": nome,\n",
    "            \"subtitulo\": None,\n",
    "            \"categoria\": categoria,\n",
    "            \"quantidade\": quantidade or \"\",\n",
    "            \"preco\": preco or \"\",\n",
    "            \"beneficios\": beneficios or \"\",\n",
    "            \"ingredientes\": ingredientes or \"\",  \n",
    "            \"tipo_pele\": \"todos os tipos\",        \n",
    "            \"imagem\": img_file or \"\",\n",
    "        }\n",
    "        products.append(prod)\n",
    "        print(f\"\\n Concluído: {nome}\")\n",
    "        time.sleep(0.6)\n",
    "\n",
    "    return products\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578e4a7",
   "metadata": {},
   "source": [
    "## Saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd563c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coletando links por categoria…\n",
      " - Limpeza: 13 links após filtro\n",
      " - Tratamento: 16 links após filtro\n",
      " - Hidratação: 12 links após filtro\n",
      " - Proteção Solar: 15 links após filtro\n",
      "Total de URLs únicas: 27\n",
      "[1/27] https://www.beyoung.com.br/products/agua-micelar-hidratante\n",
      "\n",
      " Concluído: Água micelar hidratante\n",
      "[2/27] https://www.beyoung.com.br/products/gentle-cleanser\n",
      "\n",
      " Concluído: Gel de limpeza facial suave\n",
      "[3/27] https://www.beyoung.com.br/products/exfoliant-cleanser\n",
      "\n",
      " Concluído: Esfoliante facial smart peeling\n",
      "[4/27] https://www.beyoung.com.br/products/gel-de-limpeza-facial-com-acido-glicolico-controle-de-oleosidade\n",
      "\n",
      " Concluído: Gel de limpeza facial com ácido glicólico (controle de oleosidade)\n",
      "[5/27] https://www.beyoung.com.br/products/stick-multifuncional-com-cor-fps-80#judgeme_product_reviews\n",
      "\n",
      " Concluído: Stick bastão multifuncional com cor fps 80\n",
      "[6/27] https://www.beyoung.com.br/products/booster#judgeme_product_reviews\n",
      "\n",
      " Concluído: Sérum facial booster multifuncional 3 em 1\n",
      "[7/27] https://www.beyoung.com.br/products/lip-tint-duo-color#judgeme_product_reviews\n",
      "\n",
      " Concluído: Lip tint com ácido hialurônico (duas cores)\n",
      "[8/27] https://www.beyoung.com.br/products/agua-micelar-hidratante#judgeme_product_reviews\n",
      "\n",
      " Concluído: Água micelar hidratante\n",
      "[9/27] https://www.beyoung.com.br/products/conceal-repair#judgeme_product_reviews\n",
      "\n",
      " Concluído: Corretivo líquido matte com ácido hialurônico\n",
      "[10/27] https://www.beyoung.com.br/products/gentle-cleanser#judgeme_product_reviews\n",
      "\n",
      " Concluído: Gel de limpeza facial suave\n",
      "[11/27] https://www.beyoung.com.br/products/essential-oil-control-fps-30#judgeme_product_reviews\n",
      "\n",
      " Concluído: Protetor solar matte fps 30 com niacinamida (toque seco)\n",
      "[12/27] https://www.beyoung.com.br/products/primer#judgeme_product_reviews\n",
      "\n",
      " Concluído: Primer facial com niacinamida e efeito lifting\n",
      "[13/27] https://www.beyoung.com.br/products/acido-glicolico-acido-salicilico-acne-control\n",
      "\n",
      " Concluído: Sérum facial antiacne e oleosidade 10% ácido glicólico + 0,5% ácido salicílico\n",
      "[14/27] https://www.beyoung.com.br/products/booster\n",
      "\n",
      " Concluído: Sérum facial booster multifuncional 3 em 1\n",
      "[15/27] https://www.beyoung.com.br/products/vita-c-18\n",
      "\n",
      " Concluído: Sérum facial antioxidante 18% nano vitamina c + 3% esqualano\n",
      "[16/27] https://www.beyoung.com.br/products/aging-care\n",
      "\n",
      " Concluído: Sérum facial com retinol + niacinamida\n",
      "[17/27] https://www.beyoung.com.br/products/serum-facial-antiacne-e-oleosidade\n",
      "\n",
      " Concluído: Sérum facial antiacne e oleosidade 10% ácido glicólico + 0,5% ácido salicílico.\n",
      "[18/27] https://www.beyoung.com.br/products/serum-facial-antimanchas-com-10-acido-mandelico-3-acido-tranexamico\n",
      "\n",
      " Concluído: Sérum facial antimanchas com 10% ácido mandélico + 3% ácido tranexâmico\n",
      "[19/27] https://www.beyoung.com.br/products/exfoliant-cleanser#judgeme_product_reviews\n",
      "\n",
      " Concluído: Esfoliante facial smart peeling\n",
      "[20/27] https://www.beyoung.com.br/products/serum-facial-antiacne-e-oleosidade#judgeme_product_reviews\n",
      "\n",
      " Concluído: Sérum facial antiacne e oleosidade 10% ácido glicólico + 0,5% ácido salicílico.\n",
      "[21/27] https://www.beyoung.com.br/products/vita-c-18#judgeme_product_reviews\n",
      "\n",
      " Concluído: Sérum facial antioxidante 18% nano vitamina c + 3% esqualano\n",
      "[22/27] https://www.beyoung.com.br/products/aging-care#judgeme_product_reviews\n",
      "\n",
      " Concluído: Sérum facial com retinol + niacinamida\n",
      "[23/27] https://www.beyoung.com.br/products/beyoung-hydra-gel\n",
      "\n",
      " Concluído: Hidratante facial em gel com ácido hialurônico (pele refrescante)\n",
      "[24/27] https://www.beyoung.com.br/products/stick-multifuncional-com-cor-fps-80\n",
      "\n",
      " Concluído: Stick bastão multifuncional com cor fps 80\n",
      "[25/27] https://www.beyoung.com.br/products/essential-oil-control-fps-30\n",
      "\n",
      " Concluído: Protetor solar matte fps 30 com niacinamida (toque seco)\n",
      "[26/27] https://www.beyoung.com.br/products/protetor_solar_trasnparente_fps_50\n",
      "\n",
      " Concluído: Protetor solar transparente fps 50\n",
      "[27/27] https://www.beyoung.com.br/products/stick-multifuncional-incolor-fps-80\n",
      "\n",
      " Concluído: Stick bastão multifuncional incolor fps 80\n",
      "\n",
      "JSON salvo em: /home/usuario/Área de trabalho/Dados/Beyoung/beyoung_scraper.json  (27 produtos)\n"
     ]
    }
   ],
   "source": [
    "def save_json(data: List[Dict], path: str):\n",
    "    Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nJSON salvo em: {path}  ({len(data)} produtos)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_beyoung()\n",
    "    save_json(data, OUTPUT_JSON_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
