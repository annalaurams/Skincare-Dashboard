{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12065ce1",
   "metadata": {},
   "source": [
    "# Beyoung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1ede23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, unicodedata, sys, csv, json\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import List, Dict, Optional\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sys.path.append(os.path.abspath(\"./../models\"))\n",
    "\n",
    "from skin import (SKIN_TYPE_CANONICAL_ORDER, SKIN_TYPE_SYNONYMS_PT)\n",
    "from exclude import (EXCLUDE_KEYWORDS,)\n",
    "from ingredient import (INGREDIENTES_VALIDOS,)\n",
    "from benefits import (BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER)\n",
    "from category import (CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c12ba",
   "metadata": {},
   "source": [
    "### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f19084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_URL = \"https://www.beyoung.com.br\"\n",
    "BASE_SKINCARE_URL = \"https://www.beyoung.com.br/collections/skincare\"\n",
    "\n",
    "CATEGORY_URLS = {\n",
    "    \"Limpeza\":        \"https://www.beyoung.com.br/collections/skincare-limpeza\",\n",
    "    \"Tratamento\":     \"https://www.beyoung.com.br/collections/skincare-tratamento\",\n",
    "    \"Hidratação\":     \"https://www.beyoung.com.br/collections/skincare-hidratacao\",\n",
    "    \"Proteção Solar\": \"https://www.beyoung.com.br/collections/protecao-solar\",\n",
    "}\n",
    "\n",
    "OUTPUT_JSON_PATH = \"/home/usuario/Área de trabalho/Dados/Beyoung/beyoung_scraper.json\"\n",
    "IMAGES_DIR = Path(\"images\")\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "}\n",
    "\n",
    "def build_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update(HEADERS)\n",
    "    retry = Retry(\n",
    "        total=5, backoff_factor=0.7,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"HEAD\", \"OPTIONS\"]\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
    "    return s\n",
    "\n",
    "SESSION = build_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500ddee",
   "metadata": {},
   "source": [
    "## Utilitários\n",
    "\n",
    "### Funções auxiliares para normalização de texto, remoção de acentos, tokenização dos ingredientes, nomes de arquivos e formatação de preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_accents_lower(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def sentence_case(s: str) -> str:\n",
    "  \n",
    "    if not s:\n",
    "        return s\n",
    "    s_low = s.lower()\n",
    "    return s_low[0].upper() + s_low[1:] if len(s_low) > 1 else s_low.upper()\n",
    "\n",
    "def slugify(name: str) -> str:\n",
    "    base = _strip_accents_lower(name)\n",
    "    base = re.sub(r\"[^a-z0-9]+\", \"-\", base).strip(\"-\")\n",
    "    base = re.sub(r\"-{2,}\", \"-\", base)\n",
    "    return base or \"produto\"\n",
    "\n",
    "def http_get(url: str, timeout: int = 30) -> Optional[bytes]:\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=timeout)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        low = r.text.lower()\n",
    "        if any(x in low for x in [\"please enable cookies\", \"attention required\", \"access denied\"]):\n",
    "            return None\n",
    "        return r.content\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def soup_from_url(url: str) -> Optional[BeautifulSoup]:\n",
    "    content = http_get(url)\n",
    "    if not content:\n",
    "        return None\n",
    "    return BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "def should_exclude(name: str) -> bool:\n",
    "    n = _strip_accents_lower(name)\n",
    "    for kw in EXCLUDE_KEYWORDS:\n",
    "        if not kw:\n",
    "            continue\n",
    "        if _strip_accents_lower(kw) in n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def normalize_price_text(txt: str) -> Optional[str]:\n",
    "    if not txt: return None\n",
    "    t = re.sub(r\"\\s+\", \" \", txt).replace(\"\\xa0\", \" \").strip()\n",
    "    m = re.search(r\"(?:R\\$)?\\s*(\\d{1,3}(?:\\.\\d{3})*,\\d{2})\", t)\n",
    "    if not m:\n",
    "        m = re.search(r\"(?:R\\$)?\\s*(\\d{1,3}(?:\\.\\d{3})*)\\b(?!,)\", t)\n",
    "        if m:\n",
    "            num = m.group(1).replace(\".\", \"\")\n",
    "            return f\"{float(num):.2f}\"\n",
    "    if m:\n",
    "        num = m.group(1).replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try:\n",
    "            return f\"{float(num):.2f}\"\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def safe_join_url(url: str) -> str:\n",
    "    return \"https:\" + url if url and url.startswith(\"//\") else url\n",
    "\n",
    "def infer_img_ext(url: str) -> str:\n",
    "    if not url:\n",
    "        return \".jpg\"\n",
    "    path = urlparse(url).path\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".jpg\", \".jpeg\", \".png\", \".webp\"]:\n",
    "        return \".jpg\" if ext == \".jpeg\" else ext\n",
    "    return \".jpg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e37710",
   "metadata": {},
   "source": [
    "## Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c6ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_product_image(image_url: str, product_name: str) -> Optional[str]:\n",
    "    if not image_url:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        image_request_headers = {\n",
    "            **HEADERS,\n",
    "            \"Accept\": \"image/avif,image/webp,image/apng,image/*,*/*;q=0.8\",\n",
    "            \"Referer\": BASE_URL\n",
    "        }\n",
    "        \n",
    "        response = SESSION.get(image_url, headers=image_request_headers, timeout=40)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        file_extension = infer_img_ext(image_url)\n",
    "        filename_base = slugify(product_name)\n",
    "        destination_path = IMAGES_DIR / f\"{filename_base}{file_extension}\"\n",
    "        \n",
    "        counter = 1\n",
    "        while destination_path.exists():\n",
    "            destination_path = IMAGES_DIR / f\"{filename_base}-{counter}{file_extension}\"\n",
    "            counter += 1\n",
    "        \n",
    "        destination_path.write_bytes(response.content)\n",
    "        return destination_path.name\n",
    "    except Exception as download_error:\n",
    "        print(f\"[download_product_image] Falha: {image_url} -> {download_error}\")\n",
    "        return None\n",
    "\n",
    "normalized_benefit_synonyms = {\n",
    "    canonical_benefit: [_strip_accents_lower(synonym) for synonym in synonym_list if synonym]\n",
    "    for canonical_benefit, synonym_list in (BENEFIT_SYNONYMS_PT or {}).items()\n",
    "}\n",
    "benefit_canonical_order = {benefit: index for index, benefit in enumerate(BENEFIT_CANONICAL_ORDER or [])}\n",
    "\n",
    "def standardize_product_benefits(raw_benefit_list: List[str]) -> str:\n",
    "    if not raw_benefit_list:\n",
    "        return \"\"\n",
    "    \n",
    "    identified_benefits = set()\n",
    "    for benefit_text in raw_benefit_list:\n",
    "        normalized_text = _strip_accents_lower(benefit_text)\n",
    "        for canonical_benefit, pattern_list in normalized_benefit_synonyms.items():\n",
    "            if any(pattern and pattern in normalized_text for pattern in pattern_list):\n",
    "                identified_benefits.add(canonical_benefit)\n",
    "    \n",
    "    ordered_benefits = sorted(identified_benefits, key=lambda benefit: benefit_canonical_order.get(benefit, 999))\n",
    "    return \"; \".join(ordered_benefits)\n",
    "\n",
    "normalized_ingredient_mapping = {_strip_accents_lower(ingredient): ingredient for ingredient in INGREDIENTES_VALIDOS}\n",
    "normalized_ingredient_list = list(normalized_ingredient_mapping.keys())\n",
    "ingredient_canonical_order = {ingredient: index for index, ingredient in enumerate(INGREDIENTES_VALIDOS)}\n",
    "\n",
    "def collect_active_ingredients_section_text(html_soup: BeautifulSoup) -> str:\n",
    "    heading_elements = html_soup.find_all(re.compile(r\"h\\d\", re.I))\n",
    "    \n",
    "    for heading_element in heading_elements:\n",
    "        heading_text = heading_element.get_text(\" \", strip=True)\n",
    "        if heading_text and _strip_accents_lower(heading_text) == \"principais ativos\":\n",
    "            \n",
    "            content_parts = []\n",
    "            next_sibling = heading_element.find_next_sibling()\n",
    "            \n",
    "            while next_sibling and not re.match(r\"h\\d\", next_sibling.name or \"\", re.I):\n",
    "                sibling_text = next_sibling.get_text(\" \", strip=True)\n",
    "                if sibling_text:\n",
    "                    content_parts.append(sibling_text)\n",
    "                next_sibling = next_sibling.find_next_sibling()\n",
    "            \n",
    "            if content_parts:\n",
    "                return \" \\n \".join(content_parts)\n",
    "\n",
    "    metafield_blocks = [element.get_text(\" \\n \", strip=True) for element in html_soup.select(\".metafield-multi_line_text_field\")]\n",
    "    if metafield_blocks:\n",
    "        return \" \\n \".join(metafield_blocks)\n",
    "\n",
    "    return html_soup.get_text(\" \", strip=True)\n",
    "\n",
    "def extract_ingredients_from_text(ingredient_text_block: str) -> List[str]:\n",
    "    if not ingredient_text_block:\n",
    "        return []\n",
    "    \n",
    "    normalized_block_text = _strip_accents_lower(ingredient_text_block)\n",
    "    identified_ingredients = set()\n",
    "    \n",
    "    percentage_cleaned_text = re.sub(r\"\\b\\d+[.,]?\\d*\\s*%\\b\", \" \", normalized_block_text)\n",
    "\n",
    "    for normalized_ingredient in normalized_ingredient_list:\n",
    "        if normalized_ingredient and normalized_ingredient in percentage_cleaned_text:\n",
    "            identified_ingredients.add(normalized_ingredient)\n",
    "\n",
    "    ordered_ingredients = sorted(\n",
    "        identified_ingredients, \n",
    "        key=lambda ingredient: ingredient_canonical_order.get(normalized_ingredient_mapping[ingredient], 9999)\n",
    "    )\n",
    "    return [normalized_ingredient_mapping[normalized_ingredient] for normalized_ingredient in ordered_ingredients]\n",
    "\n",
    "def extract_active_ingredients(html_soup: BeautifulSoup) -> str:\n",
    "    ingredients_text_block = collect_active_ingredients_section_text(html_soup)\n",
    "    ingredient_items = extract_ingredients_from_text(ingredients_text_block)\n",
    "    return \"; \".join(ingredient_items)\n",
    "\n",
    "def extract_category_product_links(category_url: str, category_name: str) -> List[Dict]:\n",
    "    category_soup = soup_from_url(category_url)\n",
    "    if not category_soup:\n",
    "        return []\n",
    "\n",
    "    discovered_products = []\n",
    "    product_anchor_elements = category_soup.select('a[href*=\"/products/\"]')\n",
    "    \n",
    "    for anchor_element in product_anchor_elements:\n",
    "        href_attribute = anchor_element.get(\"href\", \"\")\n",
    "        if \"/products/\" not in href_attribute:\n",
    "            continue\n",
    "        \n",
    "        absolute_product_url = urljoin(BASE_URL, href_attribute)\n",
    "        product_name = anchor_element.get_text(\" \", strip=True) or \"\"\n",
    "        \n",
    "        if not product_name:\n",
    "            image_element = anchor_element.find(\"img\", alt=True)\n",
    "            if image_element and image_element.get(\"alt\"):\n",
    "                product_name = image_element[\"alt\"].strip()\n",
    "        \n",
    "        if not product_name:\n",
    "            url_slug = href_attribute.rstrip(\"/\").split(\"/\")[-1].replace(\"-\", \" \").title()\n",
    "            product_name = url_slug\n",
    "\n",
    "        if should_exclude(product_name):\n",
    "            continue\n",
    "\n",
    "        discovered_products.append({\n",
    "            \"url\": absolute_product_url, \n",
    "            \"categoria\": category_name, \n",
    "            \"nome_card\": product_name\n",
    "        })\n",
    "\n",
    "    unique_products = []\n",
    "    processed_urls = set()\n",
    "    for product in discovered_products:\n",
    "        if product[\"url\"] not in processed_urls:\n",
    "            processed_urls.add(product[\"url\"])\n",
    "            unique_products.append(product)\n",
    "    \n",
    "    return unique_products\n",
    "\n",
    "def extract_product_name(html_soup: BeautifulSoup) -> Optional[str]:\n",
    "    name_selectors = [\"h1.product__title\", \"h1\", \"meta[property='og:title']\", \"title\"]\n",
    "    \n",
    "    for selector in name_selectors:\n",
    "        element = html_soup.select_one(selector)\n",
    "        if element:\n",
    "            if element.name == \"meta\":\n",
    "                extracted_text = element.get(\"content\", \"\")\n",
    "            else:\n",
    "                extracted_text = element.get_text(\" \", strip=True)\n",
    "            \n",
    "            if extracted_text:\n",
    "                return extracted_text.strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_product_price(html_soup: BeautifulSoup) -> Optional[str]:\n",
    "    price_selector_candidates = [\n",
    "        \".price__container .price__current, .price__regular .price-item--regular\",\n",
    "        \".price__container .price-item--regular\",\n",
    "        \".product__price .price\",\n",
    "        \".f-price-item--current\",\n",
    "        \".price, .Price\",\n",
    "    ]\n",
    "    \n",
    "    for selector in price_selector_candidates:\n",
    "        price_element = html_soup.select_one(selector)\n",
    "        if price_element:\n",
    "            formatted_price = normalize_price_text(price_element.get_text(\" \", strip=True))\n",
    "            if formatted_price:\n",
    "                return formatted_price\n",
    "    \n",
    "    fallback_price = normalize_price_text(html_soup.get_text(\" \", strip=True))\n",
    "    return fallback_price\n",
    "\n",
    "def extract_product_quantity(html_soup: BeautifulSoup) -> Optional[str]:\n",
    "    size_element = html_soup.select_one('[data-selected-swatch-value=\"Tamanho\"]')\n",
    "    if size_element and size_element.get_text(strip=True):\n",
    "        return size_element.get_text(strip=True)\n",
    "    \n",
    "    page_text = html_soup.get_text(\" \", strip=True)\n",
    "    quantity_match = re.search(r\"\\b\\d+\\,?\\d*\\s*(?:ml|g|mg|kg|l|oz)\\b\", page_text, re.I)\n",
    "    if quantity_match:\n",
    "        return quantity_match.group(0)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_product_benefits(html_soup: BeautifulSoup) -> List[str]:\n",
    "    extracted_benefits = []\n",
    "    \n",
    "    feature_elements = html_soup.select(\".section__content .feature-list__items .feature-item__text\")\n",
    "    for feature_element in feature_elements:\n",
    "        benefit_text = feature_element.get_text(\" \", strip=True)\n",
    "        if benefit_text and len(benefit_text) <= 200:\n",
    "            extracted_benefits.append(benefit_text)\n",
    "    \n",
    "    if not extracted_benefits:\n",
    "        list_item_elements = html_soup.select(\"li\")\n",
    "        for list_item in list_item_elements:\n",
    "            item_text = list_item.get_text(\" \", strip=True)\n",
    "            if item_text and 3 <= len(item_text) <= 160:\n",
    "                extracted_benefits.append(item_text)\n",
    "    \n",
    "    return extracted_benefits\n",
    "\n",
    "def extract_product_image_url(html_soup: BeautifulSoup) -> Optional[str]:\n",
    "    image_elements = html_soup.find_all(\"img\")\n",
    "    for image_element in image_elements:\n",
    "        source_url = image_element.get(\"src\") or \"\"\n",
    "        if \"cdn/shop/files\" in source_url or \"cdn.shopify.com\" in source_url:\n",
    "            return safe_join_url(source_url.strip())\n",
    "    \n",
    "    open_graph_meta = html_soup.find(\"meta\", property=\"og:image\")\n",
    "    if open_graph_meta and open_graph_meta.get(\"content\"):\n",
    "        return safe_join_url(open_graph_meta[\"content\"].strip())\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67006b4a",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "253c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute_beyoung_scraper() -> List[Dict]:\n",
    "    print(\"____________________________________________________________________________________________________________\")\n",
    "    print(\"WEB SCRAPING Beyoung...\")\n",
    "    \n",
    "    discovered_product_links = []\n",
    "    \n",
    "    for category_name, category_url in CATEGORY_URLS.items():\n",
    "        category_product_results = extract_category_product_links(category_url, category_name)\n",
    "        print(f\"Categoria '{category_name}': {len(category_product_results)} produtos encontrados\")\n",
    "        discovered_product_links.extend(category_product_results)\n",
    "        time.sleep(0.6)\n",
    "\n",
    "    unique_product_links = []\n",
    "    processed_product_urls = set()\n",
    "    for product_item in discovered_product_links:\n",
    "        if product_item[\"url\"] not in processed_product_urls:\n",
    "            processed_product_urls.add(product_item[\"url\"])\n",
    "            unique_product_links.append(product_item)\n",
    "\n",
    "    print(f\"Total de produtos únicos para processar: {len(unique_product_links)}\")\n",
    "    successfully_extracted_products = []\n",
    "\n",
    "    for current_product_index, product_item in enumerate(unique_product_links, 1):\n",
    "        product_url = product_item[\"url\"]\n",
    "        product_category = product_item[\"categoria\"]\n",
    "        \n",
    "        print(f\" [{current_product_index}/{len(unique_product_links)}] {product_url}\")\n",
    "        product_page_soup = soup_from_url(product_url)\n",
    "        \n",
    "        if not product_page_soup:\n",
    "            print(\"\\n Falha ao abrir página.\")\n",
    "            continue\n",
    "\n",
    "        raw_product_name = extract_product_name(product_page_soup) or product_item.get(\"nome_card\") or \"\"\n",
    "        if not raw_product_name:\n",
    "            print(\"\\n Nome não encontrado.\")\n",
    "            continue\n",
    "\n",
    "        formatted_product_name = sentence_case(raw_product_name)\n",
    "\n",
    "        if should_exclude(formatted_product_name):\n",
    "            print(f\"\\nExcluído por keyword (models): {formatted_product_name}\")\n",
    "            continue\n",
    "\n",
    "        extracted_product_price = extract_product_price(product_page_soup) or \"\"\n",
    "        extracted_product_quantity = extract_product_quantity(product_page_soup) or \"\"\n",
    "        raw_benefits_list = extract_product_benefits(product_page_soup)\n",
    "        standardized_product_benefits = standardize_product_benefits(raw_benefits_list)\n",
    "\n",
    "        extracted_active_ingredients = extract_active_ingredients(product_page_soup)  \n",
    "\n",
    "        product_image_url = extract_product_image_url(product_page_soup)\n",
    "        downloaded_image_filename = download_product_image(product_image_url, formatted_product_name) if product_image_url else None\n",
    "\n",
    "        structured_product_data = {\n",
    "            \"marca\": \"beyoung\",\n",
    "            \"nome\": formatted_product_name,\n",
    "            \"subtitulo\": None,\n",
    "            \"categoria\": product_category,\n",
    "            \"quantidade\": extracted_product_quantity or \"\",\n",
    "            \"preco\": extracted_product_price or \"\",\n",
    "            \"beneficios\": standardized_product_benefits or \"\",\n",
    "            \"ingredientes\": extracted_active_ingredients or \"\",  \n",
    "            \"tipo_pele\": \"todos os tipos\",        \n",
    "            \"imagem\": downloaded_image_filename or \"\",\n",
    "        }\n",
    "        \n",
    "        successfully_extracted_products.append(structured_product_data)\n",
    "        print(f\" \\nOK: {formatted_product_name}\")\n",
    "        time.sleep(0.6)\n",
    "\n",
    "    print(f\"Total pós-filtro: {len(successfully_extracted_products)}\")\n",
    "    return successfully_extracted_products\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578e4a7",
   "metadata": {},
   "source": [
    "## Saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd563c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________________\n",
      "WEB SCRAPING Beyoung...\n",
      "Categoria 'Limpeza': 15 produtos encontrados\n",
      "Categoria 'Tratamento': 15 produtos encontrados\n",
      "Categoria 'Hidratação': 11 produtos encontrados\n",
      "Categoria 'Proteção Solar': 14 produtos encontrados\n",
      "Total de produtos únicos para processar: 25\n",
      " [1/25] https://www.beyoung.com.br/products/agua-micelar-hidratante\n",
      " \n",
      "OK: Água micelar hidratante\n",
      " [2/25] https://www.beyoung.com.br/products/gentle-cleanser\n",
      " \n",
      "OK: Gel de limpeza facial suave\n",
      " [3/25] https://www.beyoung.com.br/products/exfoliant-cleanser\n",
      " \n",
      "OK: Esfoliante facial smart peeling\n",
      " [4/25] https://www.beyoung.com.br/products/gel-de-limpeza-facial-com-acido-glicolico-controle-de-oleosidade\n",
      " \n",
      "OK: Gel de limpeza facial com ácido glicólico (controle de oleosidade)\n",
      " [5/25] https://www.beyoung.com.br/products/micellar-cleanser-wipes\n",
      " \n",
      "OK: Lenço umedecido para limpeza facial\n",
      " [6/25] https://www.beyoung.com.br/products/conceal-repair#judgeme_product_reviews\n",
      " \n",
      "OK: Corretivo líquido matte com ácido hialurônico\n",
      " [7/25] https://www.beyoung.com.br/products/aging-care#judgeme_product_reviews\n",
      " \n",
      "OK: Sérum facial com retinol + niacinamida\n",
      " [8/25] https://www.beyoung.com.br/products/stick-multifuncional-com-cor-fps-80#judgeme_product_reviews\n",
      " \n",
      "OK: Stick bastão multifuncional com cor fps 80\n",
      " [9/25] https://www.beyoung.com.br/products/essential-oil-control-fps-30#judgeme_product_reviews\n",
      " \n",
      "OK: Protetor solar matte fps 30 com niacinamida (toque seco)\n",
      " [10/25] https://www.beyoung.com.br/products/primer#judgeme_product_reviews\n",
      " \n",
      "OK: Primer facial com niacinamida e efeito lifting\n",
      " [11/25] https://www.beyoung.com.br/products/vita-c-18#judgeme_product_reviews\n",
      " \n",
      "OK: Sérum facial antioxidante 18% nano vitamina c + 3% esqualano\n",
      " [12/25] https://www.beyoung.com.br/products/agua-micelar-hidratante#judgeme_product_reviews\n",
      " \n",
      "OK: Água micelar hidratante\n",
      " [13/25] https://www.beyoung.com.br/products/booster#judgeme_product_reviews\n",
      " \n",
      "OK: Sérum facial booster multifuncional 3 em 1\n",
      " [14/25] https://www.beyoung.com.br/products/lip-tint-duo-color#judgeme_product_reviews\n",
      " \n",
      "OK: Lip tint com ácido hialurônico (duas cores)\n",
      " [15/25] https://www.beyoung.com.br/products/acido-glicolico-acido-salicilico-acne-control\n",
      " \n",
      "OK: Sérum facial antiacne e oleosidade 10% ácido glicólico + 0,5% ácido salicílico\n",
      " [16/25] https://www.beyoung.com.br/products/booster\n",
      " \n",
      "OK: Sérum facial booster multifuncional 3 em 1\n",
      " [17/25] https://www.beyoung.com.br/products/vita-c-18\n",
      " \n",
      "OK: Sérum facial antioxidante 18% nano vitamina c + 3% esqualano\n",
      " [18/25] https://www.beyoung.com.br/products/aging-care\n",
      " \n",
      "OK: Sérum facial com retinol + niacinamida\n",
      " [19/25] https://www.beyoung.com.br/products/serum-facial-antimanchas-com-10-acido-mandelico-3-acido-tranexamico\n",
      " \n",
      "OK: Sérum facial antimanchas com 10% ácido mandélico + 3% ácido tranexâmico\n",
      " [20/25] https://www.beyoung.com.br/products/serum-facial-antioxidante-18-nano-vitamina-c-3-esqualano\n",
      " \n",
      "OK: Sérum facial antioxidante 18% nano vitamina c + 3% esqualano.\n",
      " [21/25] https://www.beyoung.com.br/products/beyoung-hydra-gel\n",
      " \n",
      "OK: Hidratante facial em gel com ácido hialurônico (pele refrescante)\n",
      " [22/25] https://www.beyoung.com.br/products/stick-multifuncional-com-cor-fps-80\n",
      " \n",
      "OK: Stick bastão multifuncional com cor fps 80\n",
      " [23/25] https://www.beyoung.com.br/products/essential-oil-control-fps-30\n",
      " \n",
      "OK: Protetor solar matte fps 30 com niacinamida (toque seco)\n",
      " [24/25] https://www.beyoung.com.br/products/protetor_solar_trasnparente_fps_50\n",
      " \n",
      "OK: Protetor solar transparente fps 50\n",
      " [25/25] https://www.beyoung.com.br/products/stick-multifuncional-incolor-fps-80\n",
      " \n",
      "OK: Stick bastão multifuncional incolor fps 80\n",
      "Total pós-filtro: 25\n",
      "JSON: beyoung_products.json (25 produtos)\n",
      "____________________________________________________________________________________________________________\n",
      "\n",
      "Fim da execução! Produtos extraídos: 25\n",
      "Imagens salvas em: /home/usuario/Área de trabalho/CEFET/Dados/Beyoung/images\n",
      "____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def save_data(products_data: List[Dict]):\n",
    "    if not products_data:\n",
    "        print(\"\\nNenhum dado para salvar.\")\n",
    "        return\n",
    "\n",
    "    cleaned_products = []\n",
    "    for product in products_data:\n",
    "        cleaned_products.append({\n",
    "            \"marca\": product.get(\"marca\"),\n",
    "            \"nome\": product.get(\"nome\"),\n",
    "            \"subtitulo\": product.get(\"subtitulo\"),\n",
    "            \"categoria\": product.get(\"categoria\"),\n",
    "            \"quantidade\": product.get(\"quantidade\"),\n",
    "            \"preco\": product.get(\"preco\"),\n",
    "            \"beneficios\": product.get(\"beneficios\"),\n",
    "            \"ingredientes\": product.get(\"ingredientes\"),\n",
    "            \"tipo_pele\": product.get(\"tipo_pele\"),\n",
    "            \"imagem\": product.get(\"imagem\"),\n",
    "        })\n",
    "\n",
    "    with open(\"beyoung_products.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(cleaned_products, json_file, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"JSON: beyoung_products.json ({len(cleaned_products)} produtos)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        scraped_products_data = execute_beyoung_scraper()\n",
    "        save_data(scraped_products_data)\n",
    "        print(\"____________________________________________________________________________________________________________\")\n",
    "        print(f\"\\nFim da execução! Produtos extraídos: {len(scraped_products_data)}\")\n",
    "        print(f\"Imagens salvas em: {IMAGES_DIR.resolve()}\")\n",
    "        print(\"____________________________________________________________________________________________________________\")\n",
    "\n",
    "    except Exception as execution_error:\n",
    "        print(f\"\\nERRO: {execution_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165469a0",
   "metadata": {},
   "source": [
    "## Conversão JSON para CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40bb8b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV gerado: beyoung_products.csv (16 linhas)\n",
      "A partir do JSON: beyoung_products.json\n"
     ]
    }
   ],
   "source": [
    "def json_to_csv(json_file=\"beyoung_products.json\", csv_file=\"beyoung_products.csv\"):\n",
    "\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Nenhum dado encontrado no arquivo {json_file}\")\n",
    "            return\n",
    "        \n",
    "        cols = [\"marca\", \"nome\", \"subtitulo\", \"categoria\", \"quantidade\", \"preco\", \"beneficios\", \"ingredientes\", \"tipo_pele\", \"imagem\"]\n",
    "        \n",
    "        with open(csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=cols)\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "\n",
    "                csv_row = {k: (row.get(k) or \"\") for k in cols}\n",
    "                writer.writerow(csv_row)\n",
    "        \n",
    "        print(f\"CSV gerado: {csv_file} ({len(data)} linhas)\")\n",
    "        print(f\"A partir do JSON: {json_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Arquivo {json_file} não encontrado!\")\n",
    "        \n",
    "        import glob\n",
    "        json_files = glob.glob(\"*.json\")\n",
    "        if json_files:\n",
    "            for f in json_files:\n",
    "                print(f\"   - {f}\")\n",
    "        else:\n",
    "            print(\"   Nenhum arquivo .json encontrado\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter JSON para CSV: {e}\")\n",
    "\n",
    "\n",
    "json_to_csv(\"beyoung_products.json\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
