{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badd592f",
   "metadata": {},
   "source": [
    "# Beyoung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b278f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import time, sys, os, unicodedata, json\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/usuario/Área de trabalho/Dados/models\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "\n",
    "from category import CATEGORY_HINTS, CATEGORY_CANONICAL_ORDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04301229",
   "metadata": {},
   "source": [
    "## Informações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb6805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://www.beyoung.com.br\"\n",
    "COLLECTION_URL = \"https://www.beyoung.com.br/collections/skincare\"\n",
    "SITE_LABEL = \"beyoung\"\n",
    "OUT_CSV = \"beyoung_skincare.csv\"\n",
    "OUT_JSON = \"beyoung_skincare.json\"\n",
    "IMAGES_DIR = \"images\"   \n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.8\"\n",
    "}\n",
    "\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83782dad",
   "metadata": {},
   "source": [
    "## Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "339a79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_html(url: str) -> Optional[HTMLParser]:\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        if resp.ok:\n",
    "            return HTMLParser(resp.text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def text(node) -> str:\n",
    "    return (node.text().strip() if node else \"\").strip()\n",
    "\n",
    "def norm_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def norm_for_match(s: str) -> str:\n",
    "    s = norm_spaces(s).lower()\n",
    "    s = strip_accents(s)\n",
    "    return s\n",
    "\n",
    "def price_to_float(s: str) -> Optional[float]:\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"(\\d{1,3}(?:\\.\\d{3})*|\\d+)(?:,(\\d{2}))?\", s.replace(\"\\xa0\",\" \").replace(\"\\n\",\" \"))\n",
    "    if not m:\n",
    "        return None\n",
    "    inteiro = m.group(1).replace(\".\", \"\")\n",
    "    centavos = m.group(2) or \"00\"\n",
    "    try:\n",
    "        return float(f\"{inteiro}.{centavos}\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def has_excluded_keyword(name: str) -> bool:\n",
    "    n = norm_for_match(name)\n",
    "    for kw in EXCLUDE_KEYWORDS:\n",
    "        if kw and norm_for_match(kw) in n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def guess_quantity(name: str, fallback: str = \"\") -> str:\n",
    "    patterns = [\n",
    "        r\"(\\d+)\\s*(ml|g|kg|l|L)\\b\",\n",
    "        r\"(\\d+,\\d+)\\s*(ml|g|kg|l|L)\\b\",\n",
    "        r\"\\b(\\d{2,4})\\s*(ml)\\b\",\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, name, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            return \"\".join(m.groups()).replace(\" \", \"\")\n",
    "    return fallback.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8b26d",
   "metadata": {},
   "source": [
    "## Imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fd1b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_best_from_srcset(srcset: str) -> Optional[str]:\n",
    "    best_url, best_w = None, -1\n",
    "    for part in srcset.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        m = re.match(r\"(.+?)\\s+(\\d+)w\", part)\n",
    "        if m:\n",
    "            url, w = m.group(1).strip(), int(m.group(2))\n",
    "            if w > best_w:\n",
    "                best_url, best_w = url, w\n",
    "        else:\n",
    "            if best_url is None:\n",
    "                best_url = part\n",
    "    return best_url\n",
    "\n",
    "def abs_url(href: str) -> str:\n",
    "    if not href:\n",
    "        return \"\"\n",
    "    if href.startswith(\"//\"):\n",
    "        return \"https:\" + href\n",
    "    if href.startswith(\"http\"):\n",
    "        return href\n",
    "    return urljoin(BASE, href)\n",
    "\n",
    "def infer_ext_from_url(u: str) -> str:\n",
    "    path = urlparse(u).path\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".jpg\", \".jpeg\", \".png\", \".webp\", \".gif\"):\n",
    "        return \".jpg\" if ext == \".jpeg\" else ext\n",
    "    return \".jpg\"\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    base = norm_for_match(name)\n",
    "    base = re.sub(r\"[^a-z0-9]+\", \"-\", base).strip(\"-\")\n",
    "    base = re.sub(r\"-{2,}\", \"-\", base)\n",
    "    return base or \"produto\"\n",
    "\n",
    "def download_image(image_url: str, product_name: str) -> Optional[str]:\n",
    "    if not image_url:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(image_url, headers=HEADERS, timeout=40)\n",
    "        r.raise_for_status()\n",
    "        ext = infer_ext_from_url(image_url)\n",
    "        base = sanitize_filename(product_name)\n",
    "        filename = f\"{base}{ext}\"\n",
    "        dest = os.path.join(IMAGES_DIR, filename)\n",
    "        counter = 1\n",
    "        while os.path.exists(dest):\n",
    "            filename = f\"{base}-{counter}{ext}\"\n",
    "            dest = os.path.join(IMAGES_DIR, filename)\n",
    "            counter += 1\n",
    "        with open(dest, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return filename\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_main_image_url(doc: HTMLParser) -> Optional[str]:\n",
    "\n",
    "    og = doc.css_first('meta[property=\"og:image\"]')\n",
    "    if og:\n",
    "        content = og.attributes.get(\"content\", \"\")\n",
    "        if content:\n",
    "            return abs_url(content)\n",
    "\n",
    "    candidates = []\n",
    "    for sel in [\n",
    "        \"img.product__media\", \"img.product__image\", \"img\",  \n",
    "    ]:\n",
    "        for im in doc.css(sel):\n",
    "            srcset = im.attributes.get(\"srcset\", \"\")\n",
    "            src = im.attributes.get(\"src\", \"\")\n",
    "            if srcset:\n",
    "                best = pick_best_from_srcset(srcset)\n",
    "                if best:\n",
    "                    candidates.append(best)\n",
    "            if src:\n",
    "                candidates.append(src)\n",
    "    for u in candidates:\n",
    "        u = abs_url(u)\n",
    "        if u:\n",
    "            return u\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215deb29",
   "metadata": {},
   "source": [
    "## Benefícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1272846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_urls_from_collection(doc: HTMLParser) -> List[str]:\n",
    "    urls = set()\n",
    "    for a in doc.css(\"a.full-unstyled-link\"):\n",
    "        href = a.attributes.get(\"href\", \"\")\n",
    "        if href and \"/products/\" in href:\n",
    "            urls.add(urljoin(BASE, href))\n",
    "    for a in doc.css(\"a.card__heading, a.product-grid-item, a.product-item\"):\n",
    "        href = a.attributes.get(\"href\", \"\")\n",
    "        if href and \"/products/\" in href:\n",
    "            urls.add(urljoin(BASE, href))\n",
    "    for a in doc.css(\"a\"):\n",
    "        href = a.attributes.get(\"href\", \"\")\n",
    "        if href and \"/products/\" in href:\n",
    "            urls.add(urljoin(BASE, href))\n",
    "    return sorted(urls)\n",
    "\n",
    "def collect_benefits_text(doc: HTMLParser) -> str:\n",
    "    parts = []\n",
    "    for sel in [\n",
    "        \".product__description\", \".product__description.rte\", \".rte\",\n",
    "        \".product__accordion\", \".accordion__content\", \".product__text\",\n",
    "        \"section, article\"\n",
    "    ]:\n",
    "        for n in doc.css(sel):\n",
    "            t = norm_spaces(n.text())\n",
    "            if t and len(t) > 40:\n",
    "                parts.append(t)\n",
    "    joined = \" \".join(parts)\n",
    "    if not joined:\n",
    "        joined = norm_spaces(doc.body.text() if doc.body else \"\")\n",
    "    return joined\n",
    "\n",
    "def classify_benefits(doc: HTMLParser) -> str:\n",
    "    txt = norm_for_match(collect_benefits_text(doc))\n",
    "    found = set()\n",
    "    for canonical, synonyms in BENEFIT_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            if syn and norm_for_match(syn) in txt:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "    if not found:\n",
    "        return \"\"\n",
    "    ordered = [b for b in BENEFIT_CANONICAL_ORDER if b in found]\n",
    "    return \", \".join(ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7d77d",
   "metadata": {},
   "source": [
    "## Ingredientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c09f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_ingredients_text(doc: HTMLParser) -> str:\n",
    "    parts = []\n",
    "    anchors = (\"COMPOSIÇÃO\", \"COMPOSICAO\", \"INGREDIENTES\", \"ATIVOS\", \"PRINCIPAIS ATIVOS\")\n",
    "    for node in doc.css(\"strong, b, h1, h2, h3\"):\n",
    "        title = norm_spaces(node.text()).upper()\n",
    "        if any(a in title for a in anchors):\n",
    "            hops, cur = 0, node\n",
    "            while cur and hops < 12:\n",
    "                cur = cur.next\n",
    "                if not cur: break\n",
    "                try:\n",
    "                    t = getattr(cur, \"text\", None)\n",
    "                    if t:\n",
    "                        val = norm_spaces(cur.text())\n",
    "                        if val:\n",
    "                            parts.append(val)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                hops += 1\n",
    "    if not parts:\n",
    "        parts = [norm_spaces(doc.body.text() if doc.body else \"\")]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def filter_ingredients(doc: HTMLParser) -> str:\n",
    "    txt = norm_for_match(collect_ingredients_text(doc))\n",
    "    hits = set()\n",
    "    for ing in INGREDIENTES_VALIDOS:\n",
    "        if norm_for_match(ing) in txt:\n",
    "            hits.add(ing)\n",
    "    if not hits:\n",
    "        return \"\"\n",
    "    return \", \".join(sorted(hits, key=lambda s: strip_accents(s).lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea87c8",
   "metadata": {},
   "source": [
    "## Tipos de pele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "499f752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_skin_text(doc: HTMLParser) -> str:\n",
    "    parts = []\n",
    "    anchors = (\n",
    "        \"PARA QUAIS TIPOS DE PELE\",\"TIPO DE PELE\",\"TIPOS DE PELE\",\n",
    "        \"PELE OLEOSA\",\"PELE MISTA\",\"PELE SECA\",\"PELE SENSÍVEL\",\"PELE SENSIVEL\",\n",
    "        \"TODOS OS TIPOS\"\n",
    "    )\n",
    "    for node in doc.css(\"strong, b, h1, h2, h3\"):\n",
    "        title = norm_spaces(node.text()).upper()\n",
    "        if any(a in title for a in anchors):\n",
    "            hops, cur = 0, node\n",
    "            while cur and hops < 10:\n",
    "                cur = cur.next\n",
    "                if not cur: break\n",
    "                try:\n",
    "                    t = getattr(cur, \"text\", None)\n",
    "                    if t:\n",
    "                        val = norm_spaces(cur.text())\n",
    "                        if val:\n",
    "                            parts.append(val)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                hops += 1\n",
    "    if not parts:\n",
    "        parts = [norm_spaces(doc.body.text() if doc.body else \"\")]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def classify_skin_types(doc: HTMLParser) -> str:\n",
    "    txt = norm_for_match(collect_skin_text(doc))\n",
    "    found = set()\n",
    "    for canonical, synonyms in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            if syn and norm_for_match(syn) in txt:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "    if not found:\n",
    "        return \"\"\n",
    "    ordered = [k for k in SKIN_TYPE_CANONICAL_ORDER if k in found]\n",
    "    return \", \".join(ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64a0cd",
   "metadata": {},
   "source": [
    "## Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b002490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_CATEGORY_ORDER_MAP = {name: i for i, name in enumerate(CATEGORY_CANONICAL_ORDER)}\n",
    "\n",
    "def classify_category_from_name(name: str, subtitle: str | None = None) -> str:\n",
    "    if not CATEGORY_HINTS:\n",
    "        return \"\"\n",
    "    txt = norm_for_match(f\"{name or ''} {subtitle or ''}\")\n",
    "    hits = []\n",
    "    for cat, needles in CATEGORY_HINTS.items():\n",
    "        for n in needles:\n",
    "            if n and norm_for_match(n) in txt:\n",
    "                hits.append(cat)\n",
    "                break\n",
    "    if not hits:\n",
    "        return \"\"\n",
    "    hits.sort(key=lambda c: _CATEGORY_ORDER_MAP.get(c, 10_000))\n",
    "    return hits[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2dce7",
   "metadata": {},
   "source": [
    "## Paginação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e373389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_data(url: str) -> Optional[Dict]:\n",
    "    doc = get_html(url)\n",
    "    if not doc:\n",
    "        return None\n",
    "\n",
    "    # Nome\n",
    "    name_node = doc.css_first(\"h1.product__title\") or doc.css_first(\"h1.product__title.hd3\")\n",
    "    name = text(name_node)\n",
    "\n",
    "    # Subtítulo\n",
    "    subtitle_node = doc.css_first(\"p.product__text.inline-richtext\") or doc.css_first(\".product__subtitle, .product__text\")\n",
    "    subtitle = text(subtitle_node)\n",
    "\n",
    "    # Preço\n",
    "    price_selectors = [\n",
    "        \"span.f-price-item.f-price-item--sale\",\n",
    "        \"span.price-item.price-item--sale\",\n",
    "        \"span.price-item.price-item--regular\",\n",
    "        \"span.money\",\n",
    "        \"[data-product-price] .price-item--sale\",\n",
    "        \".price__container .price-item--sale\",\n",
    "        \".price__regular .price-item--regular\",\n",
    "    ]\n",
    "\n",
    "    raw_price = \"\"\n",
    "    for sel in price_selectors:\n",
    "        node = doc.css_first(sel)\n",
    "        if node and node.text().strip():\n",
    "            raw_price = node.text().strip()\n",
    "            break\n",
    "    price = price_to_float(raw_price)\n",
    "\n",
    "    # Quantidade\n",
    "    qty_node = doc.css_first('[data-selected-swatch-value=\"Tamanho\"]') or doc.css_first(\"label[for*='template'][for*='main'][for*='-0']\")\n",
    "    quantity = text(qty_node) or guess_quantity(name, \"\")\n",
    "\n",
    "    if not name or has_excluded_keyword(name):\n",
    "        return None\n",
    "\n",
    "    # Benefícios / Ingredientes / Tipo de pele\n",
    "    beneficios = classify_benefits(doc)\n",
    "    ingredientes = filter_ingredients(doc)\n",
    "    tipo_pele = classify_skin_types(doc)\n",
    "\n",
    "    # Categoria\n",
    "    categoria = classify_category_from_name(name, subtitle)\n",
    "\n",
    "    # Imagem\n",
    "    image_url = extract_main_image_url(doc)\n",
    "    image_filename = download_image(image_url, name) if image_url else None\n",
    "\n",
    "    return {\n",
    "        \"marca\": SITE_LABEL,\n",
    "        \"nome\": name,\n",
    "        \"subtitulo\": subtitle,\n",
    "        \"categoria\": categoria,\n",
    "        \"quantidade\": quantity,\n",
    "        \"preco\": f\"{price:.2f}\" if price is not None else \"\",\n",
    "        \"beneficios\": beneficios,\n",
    "        \"ingredientes\": ingredientes,\n",
    "        \"tipo_pele\": tipo_pele,\n",
    "        \"imagem\": image_filename or \"\",   \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2debd6d",
   "metadata": {},
   "source": [
    "## Paginação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d11bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paginate_collection(base_url: str, sleep_s: float = 0.8, max_pages: int = 50) -> List[str]:\n",
    "    all_urls = set()\n",
    "    page = 1\n",
    "    while page <= max_pages:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        doc = get_html(url)\n",
    "        if not doc:\n",
    "            break\n",
    "        urls = extract_product_urls_from_collection(doc)\n",
    "        if not urls:\n",
    "            if page == 1 and base_url != url:\n",
    "                doc0 = get_html(base_url)\n",
    "                if doc0:\n",
    "                    urls0 = extract_product_urls_from_collection(doc0)\n",
    "                    for u in urls0:\n",
    "                        all_urls.add(u)\n",
    "            break\n",
    "        for u in urls:\n",
    "            all_urls.add(u)\n",
    "        page += 1\n",
    "        time.sleep(sleep_s)\n",
    "    return sorted(all_urls)\n",
    "\n",
    "def write_csv(rows: List[Dict], path: str):\n",
    "    fieldnames = [\n",
    "        \"marca\",\"nome\",\"subtitulo\", \"categoria\", \"quantidade\", \"preco\",\n",
    "        \"beneficios\",\"ingredientes\",\"tipo_pele\",\"imagem\"\n",
    "    ]\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "\n",
    "def write_json(rows: List[Dict], path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rows, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643791d0",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f1948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs de produtos encontrados na coleção: 25\n",
      "Registros válidos coletados: 23\n",
      "CSV salvo em:  beyoung_skincare.csv\n",
      "JSON salvo em: beyoung_skincare.json\n",
      "Imagens salvas em: /home/usuario/Área de trabalho/Dados/Beyoung/images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_product_urls = paginate_collection(COLLECTION_URL)\n",
    "print(f\"URLs de produtos encontrados na coleção: {len(all_product_urls)}\")\n",
    "\n",
    "rows = []\n",
    "for i, purl in enumerate(all_product_urls, 1):\n",
    "    data = extract_product_data(purl)\n",
    "    if data:\n",
    "        rows.append(data)\n",
    "    time.sleep(0.6 if i % 3 else 1.0)\n",
    "\n",
    "write_csv(rows, OUT_CSV)\n",
    "write_json(rows, OUT_JSON)\n",
    "\n",
    "print(f\"Registros válidos coletados: {len(rows)}\")\n",
    "print(f\"CSV salvo em:  {OUT_CSV}\")\n",
    "print(f\"JSON salvo em: {OUT_JSON}\")\n",
    "print(f\"Imagens salvas em: {os.path.abspath(IMAGES_DIR)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
