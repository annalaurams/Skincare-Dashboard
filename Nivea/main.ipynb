{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d3e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.12/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NIVEA] Coletando links da listagem…\n",
      "[NIVEA] Total de links coletados: 76\n",
      "[1/76] ok - https://www.nivea.com.br/produtos/nivea-agua-micelar-solu%c3%a7%c3%a3o-de-limpeza-7-em-1-efeito-matte-40059006620190033.html\n",
      "[2/76] ok - https://www.nivea.com.br/produtos/nivea-cellular-luminous-630-antispot-antiolheiras-40059009293580033.html\n",
      "[3/76] ok - https://www.nivea.com.br/produtos/nivea-complexo-de-repara%c3%a7%c3%a3o-noturna-luminous-630-40059009865590033.html\n",
      "[4/76] ok - https://www.nivea.com.br/produtos/nivea-creme-antissinais-contorno-dos-olhos-q10-40059009159000033.html\n",
      "[5/76] ok - https://www.nivea.com.br/produtos/nivea-creme-facial-antissinais-100g-423604140033.html\n",
      "[6/76] ok - https://www.nivea.com.br/produtos/nivea-creme-facial-antissinais-dia-cellular-40059001398490033.html\n",
      "[7/76] ok - https://www.nivea.com.br/produtos/nivea-creme-facial-antissinais-noite-cellular-40059001398560033.html\n",
      "[8/76] ok - https://www.nivea.com.br/produtos/nivea-creme-facial-antissinais-q10-energy-noite-40059007794580033.html\n",
      "[9/76] ok - https://www.nivea.com.br/produtos/nivea-creme-facial-antissinais-q10-power-dia-fps-30-47g-40058088128750033.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 527\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mItens válidos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 509\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    507\u001b[39m seen_urls.add(url)\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m     item = \u001b[43mparse_nivea_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m     status = \u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdescartado\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m item:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 477\u001b[39m, in \u001b[36mparse_nivea_product\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m    474\u001b[39m category  = classify_category(name, subtitle)\n\u001b[32m    476\u001b[39m imgs      = extract_all_images(soup)\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m saved     = \u001b[43mdownload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferer\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m main_img  = saved[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m saved \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    481\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmarca\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnivea\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    482\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnome\u001b[39m\u001b[33m\"\u001b[39m: name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    491\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: url,\n\u001b[32m    492\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 442\u001b[39m, in \u001b[36mdownload_images\u001b[39m\u001b[34m(img_urls, product_name, referer)\u001b[39m\n\u001b[32m    440\u001b[39m headers = SESSION.headers.copy()\n\u001b[32m    441\u001b[39m headers[\u001b[33m\"\u001b[39m\u001b[33mReferer\u001b[39m\u001b[33m\"\u001b[39m] = referer\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m r = \u001b[43mSESSION\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    444\u001b[39m     ctype = (r.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:791\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    788\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    807\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:537\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    539\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3/dist-packages/urllib3/connection.py:461\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    464\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# === NIVEA (Rosto) Scraper - célula única para Jupyter/IPYNB ===\n",
    "import sys, os, re, json, time, unicodedata, io\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urljoin, urlparse, urlencode, parse_qs, urlunparse\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ===== seus dicionários / regras =====\n",
    "sys.path.append(os.path.abspath(\"/home/usuario/Área de trabalho/Dados/models\"))\n",
    "from skin import SKIN_TYPE_CANONICAL_ORDER, SKIN_TYPE_SYNONYMS_PT\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "from category import CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS\n",
    "\n",
    "# ================== Config ==================\n",
    "BASE_URL = \"https://www.nivea.com.br\"\n",
    "LISTING_URL = \"https://www.nivea.com.br/produtos/rosto\"\n",
    "\n",
    "JSON_PATH = Path(\"nivea_products.json\")\n",
    "CSV_PATH  = Path(\"nivea_products.csv\")\n",
    "\n",
    "IMG_DIR = Path(\"./images/nivea\")\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.1\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "})\n",
    "\n",
    "NBSP = \"\\xa0\"\n",
    "CURRENCY_RE = re.compile(r\"R\\$\\s*([\\d\\.]+,\\d{2})\")\n",
    "QTY_RE      = re.compile(r\"(\\d+(?:[.,]\\d+)?)\\s*(ml|g|l|kg|un|unid|unidades)\\b\", re.I)\n",
    "SRCSET_RE   = re.compile(r\"\\s*(\\S+)\\s+(\\d+)w\\s*\")\n",
    "\n",
    "# ================== Utils ==================\n",
    "def _strip(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = (s or \"\").replace(NBSP, \" \")\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = re.sub(r\"[\\s_/,-]+\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    s = _norm(s)\n",
    "    s = re.sub(r\"[^a-z0-9\\._-]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s or \"img\"\n",
    "\n",
    "def br_money_to_float_str(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    m = CURRENCY_RE.search(text)\n",
    "    if not m:\n",
    "        return None\n",
    "    raw = m.group(1).replace(\".\", \"\").replace(\",\", \".\")\n",
    "    try:\n",
    "        return f\"{float(raw):.2f}\"\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def extract_quantity(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    m = QTY_RE.search(text)\n",
    "    if not m:\n",
    "        return None\n",
    "    val = m.group(1).replace(\",\", \".\")\n",
    "    unit = m.group(2).lower()\n",
    "    unit = {\"unid\": \"un\", \"unidades\": \"un\"}.get(unit, unit)\n",
    "    return f\"{val}{unit}\" if unit in {\"ml\",\"g\",\"l\",\"kg\"} else f\"{val} {unit}\"\n",
    "\n",
    "def should_exclude(name: str) -> bool:\n",
    "    n = _norm(name)\n",
    "    for kw in list(EXCLUDE_KEYWORDS) + [\"kit\", \"refil\", \"refill\", \"combo\", \"duo\", \"trio\", \"necessaire\", \"presente\", \"gift\"]:\n",
    "        if _norm(kw) in n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_soup(url: str, referer: Optional[str] = None) -> Optional[BeautifulSoup]:\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            headers = SESSION.headers.copy()\n",
    "            if referer:\n",
    "                headers[\"Referer\"] = referer\n",
    "            r = SESSION.get(url, timeout=40, headers=headers)\n",
    "            if r.status_code == 200:\n",
    "                return BeautifulSoup(r.text, \"lxml\")\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(1 + i)\n",
    "    return None\n",
    "\n",
    "# ================== Paginação (Carregar mais) ==================\n",
    "def discover_ajax_url(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Captura o 'data-ajax-url' do botão Carregar mais e retorna URL absoluta.\n",
    "    \"\"\"\n",
    "    btn = soup.select_one(\"button.nx-btn--load-more[data-ajax-url]\")\n",
    "    if not btn:\n",
    "        return None\n",
    "    rel = btn.get(\"data-ajax-url\", \"\").strip()\n",
    "    if not rel:\n",
    "        return None\n",
    "    if rel.startswith(\"/\"):\n",
    "        return urljoin(BASE_URL, rel)\n",
    "    return rel\n",
    "\n",
    "def update_skip_param(url: str, skip: int) -> str:\n",
    "    pr = urlparse(url)\n",
    "    qs = parse_qs(pr.query)\n",
    "    qs[\"skip\"] = [str(skip)]\n",
    "    new_q = urlencode({k: v[0] if isinstance(v, list) else v for k, v in qs.items()})\n",
    "    return urlunparse((pr.scheme, pr.netloc, pr.path, pr.params, new_q, pr.fragment))\n",
    "\n",
    "def fetch_listing_html(url: str, referer: Optional[str] = None) -> Optional[str]:\n",
    "    try:\n",
    "        headers = SESSION.headers.copy()\n",
    "        if referer:\n",
    "            headers[\"Referer\"] = referer\n",
    "        r = SESSION.get(url, timeout=40, headers=headers)\n",
    "        if r.status_code == 200:\n",
    "            return r.text\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def collect_product_urls() -> List[str]:\n",
    "    \"\"\"\n",
    "    Coleta todos os links de produto via listagem + chamadas \"Carregar mais\".\n",
    "    \"\"\"\n",
    "    # 1) primeira página\n",
    "    soup = get_soup(LISTING_URL)\n",
    "    if not soup:\n",
    "        return []\n",
    "    product_urls = set()\n",
    "\n",
    "    def collect_from_soup(sp: BeautifulSoup):\n",
    "        for a in sp.select(\"a.nx-product-teaser__link-wrapper[href]\"):\n",
    "            href = a[\"href\"].strip()\n",
    "            if href.startswith(\"/\"):\n",
    "                href = urljoin(BASE_URL, href)\n",
    "            if re.search(r\"/produtos/.*\\.html(\\?|$)\", href):\n",
    "                product_urls.add(href)\n",
    "\n",
    "    collect_from_soup(soup)\n",
    "\n",
    "    # 2) paginação via data-ajax-url\n",
    "    ajax_base = discover_ajax_url(soup)\n",
    "    if not ajax_base:\n",
    "        # pode ser que tudo esteja na primeira página\n",
    "        return sorted(product_urls)\n",
    "\n",
    "    # estima incremento 'skip' pelo número inicial de cards\n",
    "    initial_count = len(product_urls)\n",
    "    skip = initial_count if initial_count > 0 else 21  # padrão visto no snippet\n",
    "    max_rounds = 20  # proteção\n",
    "    rounds = 0\n",
    "\n",
    "    while rounds < max_rounds:\n",
    "        rounds += 1\n",
    "        page_url = update_skip_param(ajax_base, skip)\n",
    "        html = fetch_listing_html(page_url, referer=LISTING_URL)\n",
    "        if not html or \"<\" not in html:\n",
    "            break\n",
    "        frag = BeautifulSoup(html, \"lxml\")\n",
    "        before = len(product_urls)\n",
    "        collect_from_soup(frag)\n",
    "        after = len(product_urls)\n",
    "        if after <= before:\n",
    "            break\n",
    "        # próximo incremento (usa delta real desta chamada)\n",
    "        skip += (after - before)\n",
    "\n",
    "    return sorted(product_urls)\n",
    "\n",
    "# ================== PDP: campos ==================\n",
    "def extract_name(soup: BeautifulSoup) -> Optional[str]:\n",
    "    h1 = soup.select_one(\"h1.nx-product-stage__headline\")\n",
    "    if not h1:\n",
    "        return None\n",
    "    return _strip(h1.get_text(\" \", strip=True))\n",
    "\n",
    "def extract_subtitle(soup: BeautifulSoup) -> Optional[str]:\n",
    "    p = soup.select_one(\"p.nx-product-information__description\")\n",
    "    if p:\n",
    "        return _strip(p.get_text(\" \", strip=True))\n",
    "    return None\n",
    "\n",
    "def extract_quantity_from_variation(soup: BeautifulSoup) -> Optional[str]:\n",
    "    # variações:\n",
    "    sizes = [ _strip(el.get_text(\" \", strip=True)) for el in soup.select(\".product-variation__size\") ]\n",
    "    for s in sizes:\n",
    "        q = extract_quantity(s)\n",
    "        if q:\n",
    "            return q\n",
    "    # fallback: tenta do nome\n",
    "    name = extract_name(soup)\n",
    "    return extract_quantity(name or \"\")\n",
    "\n",
    "def extract_price(soup: BeautifulSoup) -> Optional[str]:\n",
    "    # site BR de NIVEA costuma não exibir preço direto (vende via parceiros)\n",
    "    # mas deixo um fallback genérico, caso apareça:\n",
    "    for sel in [\"[class*='price']\", \".nx-price\", \".price\"]:\n",
    "        for el in soup.select(sel):\n",
    "            val = br_money_to_float_str(el.get_text(\" \", strip=True))\n",
    "            if val:\n",
    "                return val\n",
    "    return None\n",
    "\n",
    "def extract_skin_text(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Tenta capturar bloco(s) com 'Tipo de pele' e, se não achar, usa o texto descritivo.\n",
    "    \"\"\"\n",
    "    txts = []\n",
    "    # título \"Tipo de pele\"\n",
    "    title = soup.find(lambda t: t.name in [\"h5\",\"h4\",\"h3\"] and _norm(t.get_text()).startswith(\"tipo de pele\"))\n",
    "    if title:\n",
    "        # pega o container pai/irmão que contenha os chips/labels\n",
    "        cont = title.find_next()\n",
    "        if cont:\n",
    "            txts.append(_strip(cont.get_text(\" \", strip=True)))\n",
    "    # descrição geral\n",
    "    desc = extract_subtitle(soup)\n",
    "    if desc:\n",
    "        txts.append(desc)\n",
    "    return \" | \".join(t for t in txts if t)\n",
    "\n",
    "def map_skin_types_from_text(text: str) -> List[str]:\n",
    "    t = _norm(text)\n",
    "    if \"todos os tipos de pele\" in t:\n",
    "        return [\"todos os tipos\"]\n",
    "    out = set()\n",
    "    for canonical, syns in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for s in syns + [canonical]:\n",
    "            if _norm(s) in t:\n",
    "                out.add(canonical)\n",
    "                break\n",
    "    ordered = [s for s in SKIN_TYPE_CANONICAL_ORDER if s in out]\n",
    "    return ordered or ([\"todos os tipos\"] if \"pele\" in t else [])\n",
    "\n",
    "def extract_benefits(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"\n",
    "    Benefícios em labels e listas:\n",
    "      - h5.nx-benefit__label\n",
    "      - .nx-benefits-list__item (li)\n",
    "    Normalização via BENEFIT_SYNONYMS_PT.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for el in soup.select(\"h5.nx-benefit__label, .nx-benefits-list__item, li\"):\n",
    "        txt = _strip(el.get_text(\" \", strip=True))\n",
    "        if not txt:\n",
    "            continue\n",
    "        # aceita bullets genéricos com termos úteis\n",
    "        if any(k in _norm(txt) for k in [\"hidrata\", \"prote\", \"uva\", \"uvb\", \"absor\", \"matte\", \"oleos\", \"antissinais\", \"uniformiza\", \"acalma\", \"limpa\", \"renova\", \"esfolia\"]):\n",
    "            candidates.append(txt)\n",
    "    # normaliza\n",
    "    found = set()\n",
    "    joined = \" \" + \" ; \".join(_norm(x) for x in candidates) + \" \"\n",
    "    for canonical in BENEFIT_CANONICAL_ORDER:\n",
    "        syns = BENEFIT_SYNONYMS_PT.get(canonical, [])\n",
    "        if any(re.search(rf\"\\b{re.escape(_norm(s))}\\b\", joined) for s in syns + [canonical]):\n",
    "            found.add(canonical)\n",
    "    return [b for b in BENEFIT_CANONICAL_ORDER if b in found]\n",
    "\n",
    "def extract_ingredients_text(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Seção 'Lista de ingredientes' (abre/expande via HTML; pegamos o texto renderizado).\n",
    "    \"\"\"\n",
    "    # título “Lista de ingredientes”\n",
    "    title = soup.find(lambda t: t.name in [\"h2\",\"h3\",\"h4\"] and \"lista de ingredientes\" in _norm(t.get_text()))\n",
    "    chunks = []\n",
    "    if title:\n",
    "        # container da seção\n",
    "        container = title.find_parent(class_=\"nx-expand-section__container\")\n",
    "        if not container:\n",
    "            container = title.find_parent()\n",
    "        if container:\n",
    "            for p in container.select(\".nx-ingredients__section, p\"):\n",
    "                t = _strip(p.get_text(\" \", strip=True))\n",
    "                if t and (t.count(\",\") >= 3 or \"/\" in t):\n",
    "                    chunks.append(t)\n",
    "    # fallback: qualquer bloco químico longo\n",
    "    if not chunks:\n",
    "        best = \"\"\n",
    "        for el in soup.select(\"p, div, span\"):\n",
    "            tt = _strip(el.get_text(\" \", strip=True))\n",
    "            if (tt.count(\",\") >= 6 or \"/\" in tt) and len(tt) > len(best):\n",
    "                best = tt\n",
    "        if best:\n",
    "            chunks.append(best)\n",
    "    # retorna o maior\n",
    "    chunks.sort(key=len, reverse=True)\n",
    "    return chunks[0] if chunks else \"\"\n",
    "\n",
    "def tokenize_and_filter_ingredients(raw_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokeniza por vírgula e '/', normaliza e filtra ESTRITAMENTE contra INGREDIENTES_VALIDOS (português).\n",
    "    \"\"\"\n",
    "    if not raw_text:\n",
    "        return []\n",
    "    parts: List[str] = []\n",
    "    for chunk in re.split(r\",\", raw_text):\n",
    "        parts.extend(re.split(r\"/\", chunk))\n",
    "    valid_norm_map = {_norm(v): v for v in INGREDIENTES_VALIDOS}\n",
    "    out, seen = [], set()\n",
    "    for p in parts:\n",
    "        tok = _strip(p).strip().strip(\".:;\")\n",
    "        if not tok:\n",
    "            continue\n",
    "        key = _norm(tok)\n",
    "        matched = None\n",
    "        if key in valid_norm_map:\n",
    "            matched = valid_norm_map[key]\n",
    "        else:\n",
    "            # aproximação por contenção (sirva p/ “acetato de tocoferila” vs “tocoferol”, etc.)\n",
    "            for k_valid, v_canon in valid_norm_map.items():\n",
    "                if k_valid and (k_valid in key or key in k_valid) and len(k_valid) >= 4:\n",
    "                    matched = v_canon\n",
    "                    break\n",
    "        if matched:\n",
    "            k2 = _norm(matched)\n",
    "            if k2 not in seen:\n",
    "                seen.add(k2)\n",
    "                out.append(matched)\n",
    "    return out\n",
    "\n",
    "def classify_category(name: Optional[str], subtitle: Optional[str]) -> Optional[str]:\n",
    "    base = f\"{name or ''} {subtitle or ''}\"\n",
    "    text = _norm(base)\n",
    "    hits = []\n",
    "    for cat, hints in CATEGORY_HINTS.items():\n",
    "        if any(_norm(h) in text for h in hints):\n",
    "            hits.append(cat)\n",
    "    if not hits:\n",
    "        return None\n",
    "    for cat in CATEGORY_CANONICAL_ORDER:\n",
    "        if cat in hits:\n",
    "            return cat\n",
    "    return hits[0]\n",
    "\n",
    "# ================== Imagens ==================\n",
    "def _parse_srcset(srcset: str) -> List[Tuple[str, int]]:\n",
    "    out = []\n",
    "    if not srcset:\n",
    "        return out\n",
    "    for part in srcset.split(\",\"):\n",
    "        part = part.strip()\n",
    "        m = SRCSET_RE.match(part)\n",
    "        if m:\n",
    "            url, w = m.group(1), int(m.group(2))\n",
    "            out.append((url, w))\n",
    "        else:\n",
    "            url = part.split()[0]\n",
    "            if url:\n",
    "                out.append((url, 0))\n",
    "    return out\n",
    "\n",
    "def _canonicalize_url(u: str) -> str:\n",
    "    pr = urlparse(u)\n",
    "    return urlunparse((pr.scheme or \"https\", pr.netloc, pr.path, \"\", pr.query, \"\"))\n",
    "\n",
    "def extract_all_images(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"\n",
    "    Coleta todas as imagens de produto (src + srcset) de picture/img (host img.nivea.com),\n",
    "    ordenadas por largura desc e remove bad assets (logos, icons).\n",
    "    \"\"\"\n",
    "    blacklist = (\"logo\", \"icon\", \"icone\", \"sprite\", \"favicon\", \"/icons/\")\n",
    "    def _bad(u: str) -> bool:\n",
    "        lu = (u or \"\").lower()\n",
    "        return any(b in lu for b in blacklist)\n",
    "\n",
    "    candidates: Dict[str, int] = {}\n",
    "    # picture > source + img\n",
    "    for pic in soup.select(\"picture\"):\n",
    "        # sources (webp/jpg)\n",
    "        for s in pic.select(\"source[srcset]\"):\n",
    "            for url, w in _parse_srcset(s.get(\"srcset\", \"\")):\n",
    "                if \"img.nivea.com\" in url and not _bad(url):\n",
    "                    candidates[_canonicalize_url(url)] = max(candidates.get(url, 0), w)\n",
    "        # img fallback\n",
    "        img = pic.find(\"img\")\n",
    "        if img:\n",
    "            s = img.get(\"src\")\n",
    "            if s and \"img.nivea.com\" in s and not _bad(s):\n",
    "                candidates[_canonicalize_url(s)] = max(candidates.get(s, 0), 0)\n",
    "            for url, w in _parse_srcset(img.get(\"srcset\", \"\")):\n",
    "                if \"img.nivea.com\" in url and not _bad(url):\n",
    "                    candidates[_canonicalize_url(url)] = max(candidates.get(url, 0), w)\n",
    "\n",
    "    # imagens soltas também\n",
    "    for img in soup.select(\"img\"):\n",
    "        s = img.get(\"src\") or \"\"\n",
    "        if \"img.nivea.com\" in s and not _bad(s):\n",
    "            candidates[_canonicalize_url(s)] = max(candidates.get(s, 0), 0)\n",
    "        for url, w in _parse_srcset(img.get(\"srcset\", \"\")):\n",
    "            if \"img.nivea.com\" in url and not _bad(url):\n",
    "                candidates[_canonicalize_url(url)] = max(candidates.get(url, 0), w)\n",
    "\n",
    "    # ordena por largura desc, depois por path len\n",
    "    def score(u: str) -> Tuple[int, int]:\n",
    "        pr = urlparse(u)\n",
    "        return (candidates.get(u, 0), len(pr.path))\n",
    "    ordered = sorted(set(candidates.keys()), key=score, reverse=True)\n",
    "    return ordered\n",
    "\n",
    "def _ext_from_url(u: str) -> str:\n",
    "    p = urlparse(u).path.lower()\n",
    "    for ext in (\".jpg\", \".jpeg\", \".png\", \".webp\"):\n",
    "        if p.endswith(ext):\n",
    "            return ext\n",
    "    return \".jpg\"\n",
    "\n",
    "def download_images(img_urls: List[str], product_name: str, referer: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Baixa TODAS as imagens e retorna a lista de filenames salvos.\n",
    "    \"\"\"\n",
    "    saved = []\n",
    "    base = sanitize_filename(product_name or \"produto\")\n",
    "    for idx, url in enumerate(img_urls, 1):\n",
    "        if not url:\n",
    "            continue\n",
    "        if url.startswith(\"//\"):\n",
    "            url = \"https:\" + url\n",
    "        elif url.startswith(\"/\"):\n",
    "            url = urljoin(BASE_URL, url)\n",
    "        ext = _ext_from_url(url)\n",
    "        fname = f\"{base}-{idx}{ext}\"\n",
    "        fpath = IMG_DIR / fname\n",
    "        try:\n",
    "            headers = SESSION.headers.copy()\n",
    "            headers[\"Referer\"] = referer\n",
    "            r = SESSION.get(url, timeout=40, headers=headers)\n",
    "            if r.status_code == 200:\n",
    "                ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "                if not ctype.startswith(\"image\"):\n",
    "                    continue\n",
    "                if len(r.content) < 6000:   # evita placeholders muito pequenos\n",
    "                    continue\n",
    "                with open(fpath, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                saved.append(fname)\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return saved\n",
    "\n",
    "# ================== Parse completo de um PDP ==================\n",
    "def parse_nivea_product(url: str) -> Optional[Dict]:\n",
    "    soup = get_soup(url, referer=LISTING_URL)\n",
    "    if not soup:\n",
    "        return None\n",
    "\n",
    "    name = extract_name(soup)\n",
    "    if not name or should_exclude(name):\n",
    "        return None\n",
    "\n",
    "    subtitle  = extract_subtitle(soup)\n",
    "    quantity  = extract_quantity_from_variation(soup)\n",
    "    price     = extract_price(soup)  # pode ser None (sem preço no site)\n",
    "    skin_txt  = extract_skin_text(soup)\n",
    "    skin      = map_skin_types_from_text(skin_txt)\n",
    "    benefits  = extract_benefits(soup)\n",
    "    ing_text  = extract_ingredients_text(soup)\n",
    "    ingred    = tokenize_and_filter_ingredients(ing_text)\n",
    "    category  = classify_category(name, subtitle)\n",
    "\n",
    "    imgs      = extract_all_images(soup)\n",
    "    saved     = download_images(imgs, name, referer=url)\n",
    "    main_img  = saved[0] if saved else None\n",
    "\n",
    "    return {\n",
    "        \"marca\": \"nivea\",\n",
    "        \"nome\": name,\n",
    "        \"subtitulo\": subtitle if subtitle else None,\n",
    "        \"categoria\": category,\n",
    "        \"quantidade\": quantity,\n",
    "        \"preco\": price,\n",
    "        \"beneficios\": \"; \".join(benefits) if benefits else None,\n",
    "        \"ingredientes\": \"; \".join(ingred) if ingred else None,\n",
    "        \"tipo_pele\": \"; \".join(skin) if skin else None,\n",
    "        \"imagem\": main_img,  # primeira salva (demais ficaram em ./images/nivea)\n",
    "        \"url\": url,\n",
    "    }\n",
    "\n",
    "# ================== Main ==================\n",
    "def main():\n",
    "    print(\"[NIVEA] Coletando links da listagem…\")\n",
    "    product_urls = collect_product_urls()\n",
    "    product_urls = sorted(set(product_urls))\n",
    "    print(f\"[NIVEA] Total de links coletados: {len(product_urls)}\")\n",
    "\n",
    "    results = []\n",
    "    seen_urls = set()\n",
    "    for i, url in enumerate(product_urls, 1):\n",
    "        if url in seen_urls:\n",
    "            print(f\"[{i}/{len(product_urls)}] pulado (URL repetida) - {url}\")\n",
    "            continue\n",
    "        seen_urls.add(url)\n",
    "        try:\n",
    "            item = parse_nivea_product(url)\n",
    "            status = \"ok\" if item else \"descartado\"\n",
    "            if item:\n",
    "                results.append(item)\n",
    "        except Exception as e:\n",
    "            status = f\"erro:{e.__class__.__name__}\"\n",
    "        print(f\"[{i}/{len(product_urls)}] {status} - {url}\")\n",
    "        time.sleep(0.15)\n",
    "\n",
    "    with open(JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    pd.DataFrame(results).to_csv(CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Salvo JSON: {JSON_PATH}\")\n",
    "    print(f\"Salvo CSV : {CSV_PATH}\")\n",
    "    print(f\"Itens válidos: {len(results)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
