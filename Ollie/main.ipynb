{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2168fbfd",
   "metadata": {},
   "source": [
    "# Ollie - Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c758145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, json, time, random, logging, unicodedata, sys\n",
    "from urllib.parse import urljoin, urlencode, urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "\n",
    "from exclude import (\n",
    "    EXCLUDE_KEYWORDS,\n",
    ")\n",
    "\n",
    "from ingredient import (\n",
    "    INGREDIENTES_VALIDOS,\n",
    ")\n",
    "\n",
    "from benefits import (\n",
    "    BENEFIT_SYNONYMS_PT,\n",
    "    BENEFIT_CANONICAL_ORDER,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"ollie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cb565",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75673ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://meuollie.com.br\"\n",
    "COLLECTION_PATH = \"/collections/loja-produtos-ollie\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593cfd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746d4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_session(max_retries=3, backoff=0.5, timeout=20):\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=max_retries, read=max_retries, connect=max_retries,\n",
    "        backoff_factor=backoff, status_forcelist=[429,500,502,503,504],\n",
    "        allowed_methods=frozenset([\"GET\"]), raise_on_status=False\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.headers.update(HEADERS)\n",
    "    s.timeout = timeout\n",
    "    return s\n",
    "\n",
    "def fetch_html(session, url, delay_range=(0.6,1.1)):\n",
    "    time.sleep(random.uniform(*delay_range))\n",
    "    r = session.get(url, timeout=session.timeout)\n",
    "    if r.status_code >= 400:\n",
    "        logger.warning(\"HTTP %s em %s\", r.status_code, url)\n",
    "    return r.text, r.status_code\n",
    "\n",
    "def norm_space(t):\n",
    "    if not t: return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def norm_text_for_match(t: str) -> str:\n",
    "    t = t or \"\"\n",
    "    t = strip_accents(t.lower())\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def norm_price(t):\n",
    "    if not t: return \"\"\n",
    "    t = t.replace(\"R$\", \"\").strip().replace(\".\",\"\").replace(\",\",\".\")\n",
    "    m = re.findall(r\"[0-9.]+\", t)\n",
    "    return m[0] if m else \"\"\n",
    "\n",
    "def slugify(text):\n",
    "    t = text.lower().strip()\n",
    "    t = re.sub(r\"[^a-z0-9\\-\\_\\sáàâãäéèêëíìîïóòôõöúùûüç]\", \"\", t)\n",
    "    t = t.replace(\" \", \"-\")\n",
    "    repl = ((\"á\",\"a\"),(\"à\",\"a\"),(\"â\",\"a\"),(\"ã\",\"a\"),(\"ä\",\"a\"),\n",
    "            (\"é\",\"e\"),(\"è\",\"e\"),(\"ê\",\"e\"),(\"ë\",\"e\"),\n",
    "            (\"í\",\"i\"),(\"ì\",\"i\"),(\"î\",\"i\"),(\"ï\",\"i\"),\n",
    "            (\"ó\",\"o\"),(\"ò\",\"o\"),(\"ô\",\"o\"),(\"õ\",\"o\"),(\"ö\",\"o\"),\n",
    "            (\"ú\",\"u\"),(\"ù\",\"u\"),(\"û\",\"u\"),(\"ü\",\"u\"),\n",
    "            (\"ç\",\"c\"))\n",
    "    for a,b in repl: t = t.replace(a,b)\n",
    "    t = re.sub(r\"-+\", \"-\", t)\n",
    "    return t.strip(\"-\")\n",
    "\n",
    "def get_image_filename(url):\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    parsed = urlparse(url)\n",
    "    filename = os.path.basename(parsed.path)\n",
    "    return filename.lower() if filename else \"\"\n",
    "\n",
    "def build_listing_url(page=1, phcursor=None):\n",
    "    base = urljoin(BASE_URL, COLLECTION_PATH)\n",
    "    params = []\n",
    "    if page and page > 1:\n",
    "        params.append((\"page\", str(page)))\n",
    "    if phcursor:\n",
    "        params.append((\"phcursor\", phcursor))\n",
    "    return base + (\"?\" + urlencode(params) if params else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d0ad7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0bdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_listing(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    product_urls = []\n",
    "    for a in soup.select(\"a[href*='/products/']\"):\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        if \"/products/\" in href:\n",
    "            product_urls.append(urljoin(BASE_URL, href))\n",
    "\n",
    "    next_url = None\n",
    "    a_next = soup.find(\"a\", attrs={\"title\": lambda x: x and \"Avançar\" in x})\n",
    "    if a_next and a_next.get(\"href\"):\n",
    "        next_url = urljoin(BASE_URL, a_next[\"href\"])\n",
    "    if not next_url:\n",
    "        for a in soup.select(\"a[href*='?page=']\"):\n",
    "            next_url = urljoin(BASE_URL, a.get(\"href\"))\n",
    "            break\n",
    "\n",
    "    seen, uniq = set(), []\n",
    "    for u in product_urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u); uniq.append(u)\n",
    "    return uniq, next_url\n",
    "\n",
    "def _collect_section_text(soup, anchors=(\"PRINCIPAIS BENEFÍCIOS\",\"BENEFÍCIOS\",\"BENEFICIOS\",\"RESULTADOS\",\"POR QUE AMAR\")):\n",
    "    \"\"\"\n",
    "    Varre o HTML procurando blocos com títulos relacionados a benefícios\n",
    "    e concatena o texto logo após esses títulos. Fallback: texto inteiro.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    big_text = norm_space(soup.get_text(\" \"))\n",
    "    for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "        title = norm_space(b.get_text()).upper()\n",
    "        if any(a in title for a in anchors):\n",
    "            # Coletar alguns irmãos/seguidores próximos\n",
    "            cur = b.parent if b.parent else b\n",
    "            hops, acc = 0, []\n",
    "            while cur and hops < 12:\n",
    "                cur = cur.find_next_sibling()\n",
    "                if not cur: break\n",
    "                if cur.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    acc.append(norm_space(cur.get_text(\" \")))\n",
    "                elif cur.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                hops += 1\n",
    "            if acc:\n",
    "                chunks.append(\" \".join(acc))\n",
    "    if chunks:\n",
    "        return \" \".join(chunks)\n",
    "    return big_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275cfdb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67b6e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_benefits(soup):\n",
    "    \"\"\"\n",
    "    Classifica benefícios do produto em rótulos canônicos com base em BENEFIT_SYNONYMS_PT.\n",
    "    Retorna rótulos separados por vírgula, ordenados por BENEFIT_CANONICAL_ORDER.\n",
    "    \"\"\"\n",
    "    text = _collect_section_text(soup)\n",
    "    text_norm = norm_text_for_match(text)\n",
    "\n",
    "    found = set()\n",
    "    # percorrer dicionário de sinônimos -> rótulo canônico\n",
    "    for canonical, synonyms in BENEFIT_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            syn_norm = norm_text_for_match(syn)\n",
    "            if syn_norm and syn_norm in text_norm:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "\n",
    "    if not found:\n",
    "        # fallback: cortar uma amostra do texto bruto (útil p/ inspeção)\n",
    "        snippet = norm_space(text)[:220]\n",
    "        return snippet\n",
    "\n",
    "    # Ordenar pelo canônico definido\n",
    "    ordered = [b for b in BENEFIT_CANONICAL_ORDER if b in found]\n",
    "    return \", \".join(ordered)\n",
    "\n",
    "def extract_skin_types(soup):\n",
    "    \"\"\"\n",
    "    Extrai e classifica tipos/condições de pele com base em SKIN_TYPE_SYNONYMS_PT.\n",
    "    Procura seção específica; fallback: corpo inteiro.\n",
    "    \"\"\"\n",
    "    # Tentar achar âncoras típicas\n",
    "    anchors = (\"PARA QUAIS TIPOS DE PELE\",\"TIPO DE PELE\",\"TIPOS DE PELE\",\"PELE\")\n",
    "    full_text = \"\"\n",
    "    for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "        t = norm_space(b.get_text()).upper()\n",
    "        if any(a in t for a in anchors):\n",
    "            cur = b.parent if b.parent else b\n",
    "            acc, hops = [], 0\n",
    "            while cur and hops < 10:\n",
    "                cur = cur.find_next_sibling()\n",
    "                if not cur: break\n",
    "                if cur.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    acc.append(norm_space(cur.get_text(\" \")))\n",
    "                elif cur.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                hops += 1\n",
    "            if acc:\n",
    "                full_text = \" \".join(acc)\n",
    "                break\n",
    "\n",
    "    if not full_text:\n",
    "        full_text = norm_space(soup.get_text(\" \"))\n",
    "\n",
    "    txt = norm_text_for_match(full_text)\n",
    "    found = set()\n",
    "    # mapeia sinônimos -> rótulos\n",
    "    for canonical, synonyms in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            syn_norm = norm_text_for_match(syn)\n",
    "            if syn_norm and syn_norm in txt:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "\n",
    "    if found:\n",
    "        ordered = [c for c in SKIN_TYPE_CANONICAL_ORDER if c in found]\n",
    "        return \", \".join(ordered)\n",
    "    else:\n",
    "        # fallback: primeira frase/trecho\n",
    "        return full_text[:200] + (\"...\" if len(full_text) > 200 else \"\")\n",
    "\n",
    "def extract_active_ingredients(soup):\n",
    "    \"\"\"\n",
    "    Extrai ingredientes ativos e filtra por INGREDIENTES_VALIDOS (case/acentos-insensitive).\n",
    "    Tenta seções 'Principais ativos' / 'Composição'; fallback: texto inteiro.\n",
    "    \"\"\"\n",
    "    # Seção \"Principais ativos\"\n",
    "    text = \"\"\n",
    "    for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "        title = norm_space(b.get_text()).upper()\n",
    "        if \"PRINCIPAIS ATIVOS\" in title or \"ATIVOS\" in title:\n",
    "            cur = b.parent if b.parent else b\n",
    "            hops, acc = 0, []\n",
    "            while cur and hops < 12:\n",
    "                cur = cur.find_next_sibling()\n",
    "                if not cur: break\n",
    "                if cur.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    acc.append(norm_space(cur.get_text(\" \")))\n",
    "                elif cur.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                hops += 1\n",
    "            if acc:\n",
    "                text = \" \".join(acc)\n",
    "                break\n",
    "\n",
    "    # Seção \"Composição\"\n",
    "    if not text:\n",
    "        for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "            title = norm_space(b.get_text()).upper()\n",
    "            if \"COMPOSIÇÃO\" in title or \"COMPOSICAO\" in title or \"INGREDIENTES\" in title:\n",
    "                p = b.find_next(\"p\")\n",
    "                if p:\n",
    "                    text = norm_space(p.get_text(\" \"))\n",
    "                break\n",
    "\n",
    "    if not text:\n",
    "        text = norm_space(soup.get_text(\" \"))\n",
    "\n",
    "    text_norm = norm_text_for_match(text)\n",
    "    found = set()\n",
    "    for ing in INGREDIENTES_VALIDOS:\n",
    "        if norm_text_for_match(ing) in text_norm:\n",
    "            found.add(ing)\n",
    "\n",
    "    if found:\n",
    "        return \", \".join(sorted(found, key=lambda x: strip_accents(x).lower()))\n",
    "    return text[:300] + (\"...\" if len(text) > 300 else \"\")\n",
    "\n",
    "\n",
    "def extract_image_best(soup):\n",
    "    og = soup.select_one('meta[property=\"og:image\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        url = og[\"content\"]\n",
    "        if url.startswith(\"//\"): url = \"https:\" + url\n",
    "        return url\n",
    "\n",
    "    best_url, best_w = \"\", -1\n",
    "    for img in soup.select(\"img\"):\n",
    "        srcset = img.get(\"srcset\") or \"\"\n",
    "        src = img.get(\"src\") or \"\"\n",
    "\n",
    "        def absurl(u):\n",
    "            if not u: return \"\"\n",
    "            if u.startswith(\"//\"): return \"https:\" + u\n",
    "            return urljoin(BASE_URL, u)\n",
    "\n",
    "        if srcset:\n",
    "            for part in srcset.split(\",\"):\n",
    "                bits = part.strip().split()\n",
    "                if not bits: \n",
    "                    continue\n",
    "                cand, w = bits[0], -1\n",
    "                if len(bits) > 1 and bits[1].endswith(\"w\"):\n",
    "                    try: w = int(bits[1][:-1])\n",
    "                    except: w = -1\n",
    "                if w > best_w:\n",
    "                    best_w = w\n",
    "                    best_url = absurl(cand)\n",
    "        elif src:\n",
    "            if best_w < 0:\n",
    "                best_url = absurl(src)\n",
    "                best_w = 0\n",
    "\n",
    "    if best_url:\n",
    "        if best_url.startswith(\"//\"):\n",
    "            best_url = \"https:\" + best_url\n",
    "        return best_url\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052c252",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be76ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== PRODUTO ======\n",
    "def parse_product(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Nome\n",
    "    nome = \"\"\n",
    "    name = soup.select_one(\"h1.h2.product-single__title\") or soup.select_one(\"h1.product-single__title\")\n",
    "    if name: nome = norm_space(name.get_text())\n",
    "\n",
    "    # Excluir termos indesejados\n",
    "    if any(k in (nome or \"\").lower() for k in EXCLUDE_KEYWORDS):\n",
    "        return None\n",
    "\n",
    "    # Preço\n",
    "    preco = \"\"\n",
    "    price = soup.select_one(\"span.product__price\") or soup.select_one(\"span[data-product-price]\")\n",
    "    if price: preco = norm_price(price.get_text())\n",
    "\n",
    "    # Tipos de pele\n",
    "    tipos_de_pele = extract_skin_types(soup)\n",
    "\n",
    "    # Ingredientes\n",
    "    ingredientes = extract_active_ingredients(soup)\n",
    "\n",
    "    # Benefícios (NOVIDADE)\n",
    "    beneficios = extract_benefits(soup)\n",
    "\n",
    "    # Imagem\n",
    "    imagem_url = extract_image_best(soup)\n",
    "    imagem_filename = get_image_filename(imagem_url)\n",
    "\n",
    "    data = {\n",
    "        \"site\": \"ollie\",\n",
    "        \"categoria_textura\": \"\",\n",
    "        \"nome\": nome,\n",
    "        \"subtitulo\": \"\",\n",
    "        \"preco\": preco,\n",
    "        \"beneficios\": beneficios,          # <-- preenchido\n",
    "        \"ingredientes\": ingredientes,\n",
    "        \"tamanho\": \"\",\n",
    "        \"tipos_de_pele\": tipos_de_pele,\n",
    "        \"imagem\": imagem_filename,\n",
    "        \"_imagem_url\": imagem_url,\n",
    "        \"_url_produto\": url\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# ====== DOWNLOAD IMG ======\n",
    "def download_image(session, url, dest_dir, slug):\n",
    "    if not url: return \"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    m = re.search(r\"[?&]width=(\\d+)\", url)\n",
    "    width = m.group(1) if m else \"\"\n",
    "    fname = f\"{slug}__{width}.jpg\" if width else f\"{slug}.jpg\"\n",
    "    path = os.path.join(dest_dir, fname)\n",
    "    try:\n",
    "        r = session.get(url, timeout=session.timeout, headers=HEADERS)\n",
    "        if r.status_code == 200:\n",
    "            with open(path, \"wb\") as f: f.write(r.content)\n",
    "            return path\n",
    "        logging.warning(\"Falha ao baixar imagem %s (HTTP %s)\", url, r.status_code)\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Erro ao baixar imagem %s: %s\", url, e)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c547e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43ae5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listing(session, page_cap=20):\n",
    "    all_urls, seen, page = [], set(), 1\n",
    "    next_url = build_listing_url(page=1)\n",
    "    prev_set = set()\n",
    "\n",
    "    while next_url and page <= page_cap:\n",
    "        html, _ = fetch_html(session, next_url)\n",
    "        product_urls, hinted_next = parse_listing(html)\n",
    "        logging.info(\"Página %d | %d produtos\", page, len(product_urls))\n",
    "\n",
    "        if not product_urls:\n",
    "            logging.info(\"Sem produtos. Encerrando.\")\n",
    "            break\n",
    "\n",
    "        cur_set = set(product_urls)\n",
    "        if cur_set == prev_set:\n",
    "            logging.info(\"Página repetida. Encerrando.\")\n",
    "            break\n",
    "        prev_set = cur_set\n",
    "\n",
    "        added = 0\n",
    "        for u in product_urls:\n",
    "            if u not in seen:\n",
    "                seen.add(u); all_urls.append(u); added += 1\n",
    "        if added == 0:\n",
    "            logging.info(\"Nenhum novo produto. Encerrando.\")\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        next_url = hinted_next or build_listing_url(page=page)\n",
    "    return all_urls\n",
    "\n",
    "# ====== OUTPUTS ======\n",
    "def write_outputs(products_data, out_csv, out_json):\n",
    "    csv_columns = [\"site\", \"categoria_textura\", \"nome\", \"subtitulo\", \"preco\", \n",
    "                   \"beneficios\", \"ingredientes\", \"tamanho\", \"tipos_de_pele\", \"imagem\"]\n",
    "    # CSV\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "        w.writeheader()\n",
    "        for product in products_data:\n",
    "            w.writerow({k: product.get(k, \"\") for k in csv_columns})\n",
    "\n",
    "    # JSON\n",
    "    json_data = [{k: product.get(k, \"\") for k in csv_columns} for product in products_data]\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709148c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e72d859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scraper(out_csv=\"ollie_produtos.csv\", out_json=\"ollie_produtos.json\",\n",
    "                images_dir=\"images/\", max_retries=3, timeout=20, max_products=50):\n",
    "    s = make_session(max_retries=max_retries, timeout=timeout)\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(out_json) or \".\", exist_ok=True)\n",
    "\n",
    "    product_urls = scrape_listing(s, page_cap=20)\n",
    "    logging.info(\"Total de URLs: %d\", len(product_urls))\n",
    "\n",
    "    products_data = []\n",
    "    seen_names = set()\n",
    "\n",
    "    for i, url in enumerate(product_urls, 1):\n",
    "        if len(products_data) >= max_products:\n",
    "            logger.info(\"Limite de %d produtos atingido. Parando.\", max_products)\n",
    "            break\n",
    "\n",
    "        html, _ = fetch_html(s, url)\n",
    "        data = parse_product(html, url)\n",
    "        if data is None:\n",
    "            logging.info(\"Skip (exclusão): %s\", url)\n",
    "            continue\n",
    "\n",
    "        nome = (data.get(\"nome\") or \"\").strip().lower()\n",
    "        if nome in seen_names:\n",
    "            logging.info(\"Skip (duplicado): %s\", nome)\n",
    "            continue\n",
    "        seen_names.add(nome)\n",
    "\n",
    "        slug = slugify(data.get(\"nome\") or os.path.basename(urlparse(url).path))\n",
    "        download_image(s, data.get(\"_imagem_url\",\"\"), images_dir, slug)\n",
    "\n",
    "        # limpar campos internos\n",
    "        clean_data = {k: v for k, v in data.items() if not k.startswith(\"_\")}\n",
    "        products_data.append(clean_data)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            write_outputs(products_data, out_csv, out_json)\n",
    "            logging.info(\"Parcial salva (%d itens).\", len(products_data))\n",
    "\n",
    "    write_outputs(products_data, out_csv, out_json)\n",
    "    logging.info(\"Finalizado: %d itens\", len(products_data))\n",
    "    return len(products_data), out_csv, out_json, images_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249f837",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366d381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 19:48:53,380 | INFO | Página 1 | 33 produtos\n",
      "2025-09-22 19:48:54,460 | INFO | Página 2 | 6 produtos\n",
      "2025-09-22 19:48:56,146 | INFO | Página 3 | 33 produtos\n",
      "2025-09-22 19:48:56,146 | INFO | Nenhum novo produto. Encerrando.\n",
      "2025-09-22 19:48:56,147 | INFO | Total de URLs: 39\n",
      "2025-09-22 19:48:58,662 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-09-22 19:49:00,010 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-09-22 19:49:01,212 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-hidratante-corporal-fps-60\n",
      "2025-09-22 19:49:04,009 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-22 19:49:05,487 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-22 19:49:06,918 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-22 19:49:08,159 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-22 19:49:09,587 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-22 19:49:11,016 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-22 19:49:12,459 | INFO | Parcial salva (3 itens).\n",
      "2025-09-22 19:49:14,531 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-22 19:49:15,725 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-22 19:49:17,167 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-22 19:49:18,366 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-22 19:49:19,533 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-22 19:49:20,764 | INFO | Parcial salva (4 itens).\n",
      "2025-09-22 19:49:24,210 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/iluminador-corporal-fps-40\n",
      "2025-09-22 19:49:25,576 | INFO | Parcial salva (6 itens).\n",
      "2025-09-22 19:49:29,966 | INFO | Parcial salva (9 itens).\n",
      "2025-09-22 19:49:32,656 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-pratica\n",
      "2025-09-22 19:49:34,285 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-iluminada\n",
      "2025-09-22 19:49:35,654 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-iluminada\n",
      "2025-09-22 19:49:37,020 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-de-verao-o-ano-todo\n",
      "2025-09-22 19:49:38,156 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/loucos-por-esporte\n",
      "2025-09-22 19:49:39,769 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-resistencia\n",
      "2025-09-22 19:49:40,951 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-performance\n",
      "2025-09-22 19:49:42,449 | INFO | Parcial salva (11 itens).\n",
      "2025-09-22 19:49:45,068 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-spray-corporal\n",
      "2025-09-22 19:49:47,080 | INFO | Parcial salva (13 itens).\n",
      "2025-09-22 19:49:48,877 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-completona\n",
      "2025-09-22 19:49:50,890 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-para-peles-oleosas\n",
      "2025-09-22 19:49:52,824 | INFO | Parcial salva (14 itens).\n",
      "2025-09-22 19:49:52,826 | INFO | Finalizado: 14 itens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCRAPING CONCLUÍDO ===\n",
      "Itens coletados: 14\n",
      "CSV: /home/usuario/Área de trabalho/Dados/Ollie/ollie_produtos.csv\n",
      "JSON: /home/usuario/Área de trabalho/Dados/Ollie/ollie_produtos.json\n",
      "Imagens: /home/usuario/Área de trabalho/Dados/Ollie/images\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OUT_CSV = \"ollie_produtos.csv\"\n",
    "    OUT_JSON = \"ollie_produtos.json\"\n",
    "    IMAGES_DIR = \"images/\"\n",
    "    MAX_RETRIES = 3\n",
    "    TIMEOUT = 20\n",
    "    MAX_PRODUCTS = 50\n",
    "\n",
    "    try:\n",
    "        n_final, csv_path, json_path, img_dir = run_scraper(\n",
    "            out_csv=OUT_CSV,\n",
    "            out_json=OUT_JSON,\n",
    "            images_dir=IMAGES_DIR,\n",
    "            max_retries=MAX_RETRIES,\n",
    "            timeout=TIMEOUT,\n",
    "            max_products=MAX_PRODUCTS,\n",
    "        )\n",
    "        print(\"\\n=== SCRAPING CONCLUÍDO ===\")\n",
    "        print(\"Itens coletados:\", n_final)\n",
    "        print(\"CSV:\", os.path.abspath(csv_path))\n",
    "        print(\"JSON:\", os.path.abspath(json_path))\n",
    "        print(\"Imagens:\", os.path.abspath(img_dir))\n",
    "    except Exception as e:\n",
    "        print(\"Erro durante o scraping:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
