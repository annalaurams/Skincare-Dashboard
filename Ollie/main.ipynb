{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bcebce",
   "metadata": {},
   "source": [
    "# Ollie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df346d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, json, time, random, logging, unicodedata, sys\n",
    "from urllib.parse import urljoin, urlencode, urlparse\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(\"./../models\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "from category import CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9650f5",
   "metadata": {},
   "source": [
    "### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cad34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"ollie\")\n",
    "\n",
    "BASE_URL = \"https://meuollie.com.br\"\n",
    "COLLECTION_PATH = \"/collections/loja-produtos-ollie\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065f909",
   "metadata": {},
   "source": [
    "### Utilitários\n",
    "\n",
    "### Funções auxiliares para normalização de texto, remoção de acentos, tokenização dos ingredientes, nomes de arquivos e formatação de preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfc8290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(input_string: str) -> str:\n",
    "\n",
    "    if not input_string:\n",
    "        return \"\"\n",
    "    \n",
    "    normalized_chars = unicodedata.normalize(\"NFD\", input_string)\n",
    "    accent_free_string = \"\".join(\n",
    "        char for char in normalized_chars \n",
    "        if unicodedata.category(char) != \"Mn\"  \n",
    "    )\n",
    "    \n",
    "    return accent_free_string\n",
    "\n",
    "def normalize_text(input_string: str) -> str:\n",
    "\n",
    "    if not input_string:\n",
    "        return \"\"\n",
    "    \n",
    "    processed_text = strip_accents(input_string.strip().lower())\n",
    "    \n",
    "    processed_text = processed_text.replace(\"-\", \" \")\n",
    "    \n",
    "    processed_text = re.sub(r\"[^\\w\\s]\", \" \", processed_text)\n",
    "    \n",
    "    normalized_result = re.sub(r\"\\s+\", \" \", processed_text).strip()\n",
    "    \n",
    "    return normalized_result\n",
    "\n",
    "def normalize_space(input_text: str) -> str:\n",
    " \n",
    "    if not input_text:\n",
    "        return \"\"\n",
    "    \n",
    "    space_normalized = re.sub(r\"\\s+\", \" \", input_text).strip()\n",
    "    \n",
    "    return space_normalized\n",
    "\n",
    "def slugify(input_text: str) -> str:\n",
    "  \n",
    "    if not input_text:\n",
    "        return \"produto\"  \n",
    "\n",
    "    normalized_slug = normalize_text(input_text)\n",
    "    \n",
    "    slug_with_hyphens = re.sub(r\"[^a-z0-9]+\", \"-\", normalized_slug).strip(\"-\")\n",
    "    \n",
    "    clean_slug = re.sub(r\"-{2,}\", \"-\", slug_with_hyphens)\n",
    "    \n",
    "    return clean_slug or \"produto\"\n",
    "\n",
    "def normalize_price(price_text: str) -> str:\n",
    "\n",
    "    if not price_text:\n",
    "        return \"\"\n",
    "    \n",
    "    sanitized_text = price_text.replace(\"R$\", \"\").replace(\"\\xa0\", \" \").strip()\n",
    "    sanitized_text = sanitized_text.replace(\" \", \"\")\n",
    "   \n",
    "    brazilian_format_match = re.search(r\"(\\d{1,3}(?:\\.\\d{3})*,\\d{2})\", sanitized_text)\n",
    "    if brazilian_format_match:\n",
    "        extracted_number = brazilian_format_match.group(1)\n",
    "    \n",
    "        decimal_number = extracted_number.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try:\n",
    "            formatted_price = f\"{float(decimal_number):.2f}\"\n",
    "            return formatted_price\n",
    "        except ValueError:\n",
    "            pass  \n",
    "\n",
    "    simple_format_match = re.search(r\"(\\d+(?:\\.\\d{1,2})?)\", sanitized_text)\n",
    "    if simple_format_match:\n",
    "        try:\n",
    "            extracted_value = float(simple_format_match.group(1))\n",
    "            return f\"{extracted_value:.2f}\"\n",
    "        except ValueError:\n",
    "            pass  \n",
    "    \n",
    "    return \"\"  \n",
    "\n",
    "def get_image_filename(image_url: str) -> str:\n",
    "\n",
    "    if not image_url:\n",
    "        return \"\"\n",
    "    \n",
    "    parsed_url = urlparse(image_url)\n",
    "    \n",
    "    filename = os.path.basename(parsed_url.path)\n",
    "    \n",
    "    return filename.lower() if filename else \"\"\n",
    "\n",
    "\n",
    "def safe_join_url(url_input: str) -> str:\n",
    "\n",
    "    if not url_input:\n",
    "        return url_input\n",
    "    \n",
    "    if url_input.startswith(\"//\"):\n",
    "        return \"https:\" + url_input\n",
    "    \n",
    "    return url_input\n",
    "\n",
    "def infer_image_extension(image_url: str) -> str:\n",
    "\n",
    "    if not image_url:\n",
    "        return \".jpg\"  \n",
    "    \n",
    "    parsed_url = urlparse(image_url)\n",
    "    file_extension = os.path.splitext(parsed_url.path)[1].lower()\n",
    "    \n",
    "    supported_extensions = [\".jpg\", \".jpeg\", \".png\", \".webp\"]\n",
    "    \n",
    "    if file_extension in supported_extensions:\n",
    "\n",
    "        return \".jpg\" if file_extension == \".jpeg\" else file_extension\n",
    "    \n",
    "    return \".jpg\" \n",
    "\n",
    "def sentence_case(input_text: str) -> str:\n",
    "\n",
    "    if not input_text:\n",
    "        return input_text\n",
    "    \n",
    "    lowercased_text = input_text.lower()\n",
    "    \n",
    "    if len(lowercased_text) > 1:\n",
    "        sentence_cased = lowercased_text[0].upper() + lowercased_text[1:]\n",
    "    else:\n",
    "        sentence_cased = lowercased_text.upper()\n",
    "    \n",
    "    return sentence_cased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38e140",
   "metadata": {},
   "source": [
    "## Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78963379",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CATEGORY_ORDER_MAP = {name: i for i, name in enumerate(CATEGORY_CANONICAL_ORDER)}\n",
    "\n",
    "def classify_category_from_name(name: str, description: str | None = None) -> str | None:\n",
    "    txt = normalize_text(f\"{name or ''} {description or ''}\")\n",
    "    hits = []\n",
    "    for cat, needles in CATEGORY_HINTS.items():\n",
    "        for n in needles:\n",
    "            if normalize_text(n) in txt:\n",
    "                hits.append(cat)\n",
    "                break\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits.sort(key=lambda c: _CATEGORY_ORDER_MAP.get(c, 10_000))\n",
    "    return hits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71292cd0",
   "metadata": {},
   "source": [
    "## Sessões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c1d3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_session(max_retries=3, backoff=0.5, timeout=20):\n",
    "    \n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=max_retries, read=max_retries, connect=max_retries,\n",
    "        backoff_factor=backoff, status_forcelist=[429,500,502,503,504],\n",
    "        allowed_methods=frozenset([\"GET\"]), raise_on_status=False\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.headers.update(HEADERS)\n",
    "    s.timeout = timeout\n",
    "    return s\n",
    "\n",
    "def fetch_html(session, url, delay_range=(0.6,1.1)):\n",
    "\n",
    "    time.sleep(random.uniform(*delay_range))\n",
    "    r = session.get(url, timeout=session.timeout)\n",
    "    if r.status_code >= 400:\n",
    "        logger.warning(\"HTTP %s em %s\", r.status_code, url)\n",
    "    return r.text, r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8d65f",
   "metadata": {},
   "source": [
    "## Paginação e Listagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_listing_url(page_number=1, pagination_cursor=None):\n",
    "\n",
    "    base_collection_url = urljoin(BASE_URL, COLLECTION_PATH)\n",
    "    url_parameters = []\n",
    "    \n",
    "    if page_number and page_number > 1:\n",
    "        url_parameters.append((\"page\", str(page_number)))\n",
    "    if pagination_cursor:\n",
    "        url_parameters.append((\"phcursor\", pagination_cursor))\n",
    "    \n",
    "    query_string = urlencode(url_parameters) if url_parameters else \"\"\n",
    "    return base_collection_url + (\"?\" + query_string if query_string else \"\")\n",
    "\n",
    "def parse_listing(html_content):\n",
    "  \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    product_urls = []\n",
    "    product_links = soup.select(\"a[href*='/products/']\")\n",
    "    for link_element in product_links:\n",
    "        href_attribute = link_element.get(\"href\") or \"\"\n",
    "        if \"/products/\" in href_attribute:\n",
    "            absolute_url = urljoin(BASE_URL, href_attribute)\n",
    "            product_urls.append(absolute_url)\n",
    "\n",
    "    next_page_url = None\n",
    "    next_button = soup.find(\"a\", attrs={\"title\": lambda x: x and \"Avançar\" in x})\n",
    "    if next_button and next_button.get(\"href\"):\n",
    "        next_page_url = urljoin(BASE_URL, next_button[\"href\"])\n",
    "    \n",
    "    if not next_page_url:\n",
    "        pagination_links = soup.select(\"a[href*='?page=']\")\n",
    "        if pagination_links:\n",
    "            next_page_url = urljoin(BASE_URL, pagination_links[0].get(\"href\"))\n",
    "\n",
    "    unique_urls = []\n",
    "    seen_urls = set()\n",
    "    for url in product_urls:\n",
    "        if url not in seen_urls:\n",
    "            seen_urls.add(url)\n",
    "            unique_urls.append(url)\n",
    "    \n",
    "    return unique_urls, next_page_url\n",
    "\n",
    "def scrape_listing(session, max_pages=20):\n",
    "\n",
    "    collected_urls = []\n",
    "    processed_urls = set()\n",
    "    current_page = 1\n",
    "    current_url = build_listing_url(page_number=1)\n",
    "    previous_products_set = set()\n",
    "    \n",
    "    while current_url and current_page <= max_pages:\n",
    "        html_content, _ = fetch_html(session, current_url)\n",
    "        page_products, suggested_next_url = parse_listing(html_content)\n",
    "        \n",
    "        logging.info(\"Página %d | %d produtos\", current_page, len(page_products))\n",
    "        \n",
    "        if not page_products:\n",
    "            logging.info(\"Sem produtos. Encerrando.\")\n",
    "            break\n",
    "        \n",
    "        current_products_set = set(page_products)\n",
    "        if current_products_set == previous_products_set:\n",
    "            logging.info(\"Página repetida. Encerrando.\")\n",
    "            break\n",
    "        \n",
    "        previous_products_set = current_products_set\n",
    "        new_products_count = 0\n",
    "        \n",
    "        for product_url in page_products:\n",
    "            if product_url not in processed_urls:\n",
    "                processed_urls.add(product_url)\n",
    "                collected_urls.append(product_url)\n",
    "                new_products_count += 1\n",
    "        \n",
    "        if new_products_count == 0:\n",
    "            logging.info(\"Nenhum novo produto. Encerrando.\")\n",
    "            break\n",
    "        \n",
    "        current_page += 1\n",
    "        current_url = suggested_next_url or build_listing_url(page_number=current_page)\n",
    "    \n",
    "    return collected_urls\n",
    "\n",
    "def _collect_section_text(soup, target_anchors=(\"PRINCIPAIS BENEFÍCIOS\",\"BENEFÍCIOS\",\"BENEFICIOS\",\"RESULTADOS\",\"POR QUE AMAR\")):\n",
    "\n",
    "    collected_text_chunks = []\n",
    "    fallback_text = normalize_space(soup.get_text(\" \"))\n",
    "    \n",
    "    header_elements = soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"])\n",
    "    for header_element in header_elements:\n",
    "        header_title = normalize_space(header_element.get_text()).upper()\n",
    "        \n",
    "        if any(anchor in header_title for anchor in target_anchors):\n",
    "            current_element = header_element.parent if header_element.parent else header_element\n",
    "            traversal_hops = 0\n",
    "            section_content = []\n",
    "            \n",
    "            while current_element and traversal_hops < 12:\n",
    "                current_element = current_element.find_next_sibling()\n",
    "                if not current_element:\n",
    "                    break\n",
    "                \n",
    "                if current_element.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    element_text = normalize_space(current_element.get_text(\" \"))\n",
    "                    section_content.append(element_text)\n",
    "                elif current_element.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                \n",
    "                traversal_hops += 1\n",
    "            \n",
    "            if section_content:\n",
    "                combined_section = \" \".join(section_content)\n",
    "                collected_text_chunks.append(combined_section)\n",
    "    \n",
    "    return \" \".join(collected_text_chunks) if collected_text_chunks else fallback_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4f415",
   "metadata": {},
   "source": [
    "## Benefícios, Ingredientes e Tipos de pele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7727cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_benefits(soup):\n",
    "   \n",
    "    extracted_text = _collect_section_text(soup)\n",
    "    normalized_text = normalize_text(extracted_text)\n",
    "    \n",
    "    identified_benefits = set()\n",
    "    for canonical_benefit, synonym_list in BENEFIT_SYNONYMS_PT.items():\n",
    "        for synonym in synonym_list:\n",
    "            if synonym and normalize_text(synonym) in normalized_text:\n",
    "                identified_benefits.add(canonical_benefit)\n",
    "                break\n",
    "    \n",
    "    if not identified_benefits:\n",
    "        return normalize_space(extracted_text)[:220]\n",
    "    \n",
    "    ordered_benefits = [benefit for benefit in BENEFIT_CANONICAL_ORDER if benefit in identified_benefits]\n",
    "    return \", \".join(ordered_benefits)\n",
    "\n",
    "def extract_skin_types(soup):\n",
    "\n",
    "    skin_type_anchors = (\"PARA QUAIS TIPOS DE PELE\",\"TIPO DE PELE\",\"TIPOS DE PELE\",\"PELE\")\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    header_elements = soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"])\n",
    "    for header_element in header_elements:\n",
    "        header_text = normalize_space(header_element.get_text()).upper()\n",
    "        if any(anchor in header_text for anchor in skin_type_anchors):\n",
    "            current_element = header_element.parent if header_element.parent else header_element\n",
    "            content_accumulator = []\n",
    "            traversal_count = 0\n",
    "            \n",
    "            while current_element and traversal_count < 10:\n",
    "                current_element = current_element.find_next_sibling()\n",
    "                if not current_element:\n",
    "                    break\n",
    "                    \n",
    "                if current_element.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    element_text = normalize_space(current_element.get_text(\" \"))\n",
    "                    content_accumulator.append(element_text)\n",
    "                elif current_element.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                    \n",
    "                traversal_count += 1\n",
    "            \n",
    "            if content_accumulator:\n",
    "                extracted_text = \" \".join(content_accumulator)\n",
    "                break\n",
    "            \n",
    "    if not extracted_text:\n",
    "        extracted_text = normalize_space(soup.get_text(\" \"))\n",
    "    \n",
    "    normalized_text = normalize_text(extracted_text)\n",
    "    identified_skin_types = set()\n",
    "    \n",
    "    for canonical_type, synonym_list in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for synonym in synonym_list:\n",
    "            if synonym and normalize_text(synonym) in normalized_text:\n",
    "                identified_skin_types.add(canonical_type)\n",
    "                break\n",
    "    \n",
    "    if identified_skin_types:\n",
    "        ordered_types = [skin_type for skin_type in SKIN_TYPE_CANONICAL_ORDER if skin_type in identified_skin_types]\n",
    "        return \", \".join(ordered_types)\n",
    "    \n",
    "    truncated_text = extracted_text[:200]\n",
    "    return truncated_text + (\"...\" if len(extracted_text) > 200 else \"\")\n",
    "\n",
    "def extract_active_ingredients(soup):\n",
    "  \n",
    "    extracted_text = \"\"\n",
    "\n",
    "    header_elements = soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"])\n",
    "    for header_element in header_elements:\n",
    "        header_title = normalize_space(header_element.get_text()).upper()\n",
    "        if \"PRINCIPAIS ATIVOS\" in header_title or \"ATIVOS\" in header_title:\n",
    "            current_element = header_element.parent if header_element.parent else header_element\n",
    "            traversal_count = 0\n",
    "            content_accumulator = []\n",
    "            \n",
    "            while current_element and traversal_count < 12:\n",
    "                current_element = current_element.find_next_sibling()\n",
    "                if not current_element:\n",
    "                    break\n",
    "                    \n",
    "                if current_element.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    element_text = normalize_space(current_element.get_text(\" \"))\n",
    "                    content_accumulator.append(element_text)\n",
    "                elif current_element.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                    \n",
    "                traversal_count += 1\n",
    "            \n",
    "            if content_accumulator:\n",
    "                extracted_text = \" \".join(content_accumulator)\n",
    "                break\n",
    "            \n",
    "    if not extracted_text:\n",
    "        for header_element in header_elements:\n",
    "            header_title = normalize_space(header_element.get_text()).upper()\n",
    "            composition_keywords = [\"COMPOSIÇÃO\", \"COMPOSICAO\", \"INGREDIENTES\"]\n",
    "            if any(keyword in header_title for keyword in composition_keywords):\n",
    "                paragraph_element = header_element.find_next(\"p\")\n",
    "                if paragraph_element:\n",
    "                    extracted_text = normalize_space(paragraph_element.get_text(\" \"))\n",
    "                    break\n",
    "                \n",
    "    if not extracted_text:\n",
    "        extracted_text = normalize_space(soup.get_text(\" \"))\n",
    "    \n",
    "    normalized_text = normalize_text(extracted_text)\n",
    "    identified_ingredients = set()\n",
    "    \n",
    "    for valid_ingredient in INGREDIENTES_VALIDOS:\n",
    "        if normalize_text(valid_ingredient) in normalized_text:\n",
    "            identified_ingredients.add(valid_ingredient)\n",
    "    \n",
    "    if identified_ingredients:\n",
    "        sorted_ingredients = sorted(identified_ingredients, key=lambda x: strip_accents(x).lower())\n",
    "        return \", \".join(sorted_ingredients)\n",
    "    \n",
    "    truncated_text = extracted_text[:300]\n",
    "    return truncated_text + (\"...\" if len(extracted_text) > 300 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dff806",
   "metadata": {},
   "source": [
    "## Imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2dc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_best(soup):\n",
    "\n",
    "    open_graph_meta = soup.select_one('meta[property=\"og:image\"]')\n",
    "    if open_graph_meta and open_graph_meta.get(\"content\"):\n",
    "        og_image_url = open_graph_meta[\"content\"]\n",
    "        if og_image_url.startswith(\"//\"):\n",
    "            og_image_url = \"https:\" + og_image_url\n",
    "        return og_image_url\n",
    "    \n",
    "    best_image_url = \"\"\n",
    "    best_image_width = -1\n",
    "    \n",
    "    def make_absolute_url(url_input):\n",
    "        if not url_input:\n",
    "            return \"\"\n",
    "        if url_input.startswith(\"//\"):\n",
    "            return \"https:\" + url_input\n",
    "        return urljoin(BASE_URL, url_input)\n",
    "    \n",
    "    image_elements = soup.select(\"img\")\n",
    "    for image_element in image_elements:\n",
    "        srcset_attribute = image_element.get(\"srcset\") or \"\"\n",
    "        src_attribute = image_element.get(\"src\") or \"\"\n",
    "        \n",
    "        if srcset_attribute:\n",
    "            srcset_parts = srcset_attribute.split(\",\")\n",
    "            for srcset_part in srcset_parts:\n",
    "                part_components = srcset_part.strip().split()\n",
    "                if not part_components:\n",
    "                    continue\n",
    "                \n",
    "                candidate_url = part_components[0]\n",
    "                candidate_width = -1\n",
    "                \n",
    "                if len(part_components) > 1 and part_components[1].endswith(\"w\"):\n",
    "                    try:\n",
    "                        width_string = part_components[1][:-1]\n",
    "                        candidate_width = int(width_string)\n",
    "                    except ValueError:\n",
    "                        candidate_width = -1\n",
    "                \n",
    "                if candidate_width > best_image_width:\n",
    "                    best_image_width = candidate_width\n",
    "                    best_image_url = make_absolute_url(candidate_url)\n",
    "                    \n",
    "        elif src_attribute:\n",
    "            if best_image_width < 0:\n",
    "                best_image_url = make_absolute_url(src_attribute)\n",
    "                best_image_width = 0\n",
    "    \n",
    "    if best_image_url:\n",
    "        if best_image_url.startswith(\"//\"):\n",
    "            best_image_url = \"https:\" + best_image_url\n",
    "        return best_image_url\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def download_image(session, image_url, destination_directory, filename_slug):\n",
    "\n",
    "    if not image_url:\n",
    "        return \"\"\n",
    "    \n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "    \n",
    "    width_match = re.search(r\"[?&]width=(\\d+)\", image_url)\n",
    "    image_width = width_match.group(1) if width_match else \"\"\n",
    "    \n",
    "    if image_width:\n",
    "        output_filename = f\"{filename_slug}__{image_width}.jpg\"\n",
    "    else:\n",
    "        output_filename = f\"{filename_slug}.jpg\"\n",
    "    \n",
    "    output_path = os.path.join(destination_directory, output_filename)\n",
    "    \n",
    "    try:\n",
    "        response = session.get(image_url, timeout=session.timeout, headers=HEADERS)\n",
    "        if response.status_code == 200:\n",
    "            with open(output_path, \"wb\") as image_file:\n",
    "                image_file.write(response.content)\n",
    "            return output_path\n",
    "        \n",
    "        logging.warning(\"Falha ao baixar imagem %s (HTTP %s)\", image_url, response.status_code)\n",
    "    except Exception as download_error:\n",
    "        logging.warning(\"Erro ao baixar imagem %s: %s\", image_url, download_error)\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68c8f4",
   "metadata": {},
   "source": [
    "## Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product(html_content, product_url):\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    product_name = \"\"\n",
    "    title_element = soup.select_one(\"h1.h2.product-single__title\") or soup.select_one(\"h1.product-single__title\")\n",
    "    if title_element:\n",
    "        product_name = normalize_space(title_element.get_text())\n",
    "\n",
    "    if any(excluded_keyword in (product_name or \"\").lower() for excluded_keyword in EXCLUDE_KEYWORDS):\n",
    "        return None\n",
    "\n",
    "    product_price = \"\"\n",
    "    price_element = soup.select_one(\"span.product__price\") or soup.select_one(\"span[data-product-price]\")\n",
    "    if price_element:\n",
    "        product_price = normalize_price(price_element.get_text())\n",
    "\n",
    "    product_benefits = extract_benefits(soup) or \"\"\n",
    "    compatible_skin_types = extract_skin_types(soup) or \"\"\n",
    "    active_ingredients = extract_active_ingredients(soup) or \"\"\n",
    "\n",
    "    product_category = classify_category_from_name(product_name)\n",
    "\n",
    "    product_image_url = extract_image_best(soup)\n",
    "    image_filename = get_image_filename(product_image_url)\n",
    "\n",
    "    def format_field_list(field_content):\n",
    "\n",
    "        if not field_content:\n",
    "            return \"\"\n",
    "        cleaned_items = [item.strip() for item in field_content.replace(\",\", \";\").split(\";\") if item.strip()]\n",
    "        return \"; \".join(cleaned_items)\n",
    "\n",
    "    structured_product_data = {\n",
    "        \"marca\": \"ollie\",\n",
    "        \"nome\": product_name,\n",
    "        \"subtitulo\": None,                 \n",
    "        \"categoria\": product_category,           \n",
    "        \"quantidade\": \"\",           \n",
    "        \"preco\": product_price,\n",
    "        \"beneficios\": format_field_list(product_benefits),\n",
    "        \"ingredientes\": format_field_list(active_ingredients),\n",
    "        \"tipo_pele\": format_field_list(compatible_skin_types),\n",
    "        \"imagem\": image_filename,\n",
    "        \"_imagem_url\": product_image_url\n",
    "    }\n",
    "    \n",
    "    return structured_product_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fd9cc",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41455c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_output(products_data, output_json_path):\n",
    "  \n",
    "    output_columns = [\n",
    "        \"marca\",\"nome\",\"subtitulo\",\"categoria\", \"quantidade\",\"preco\",\n",
    "        \"ingredientes\",\"beneficios\",\"tipo_pele\",\"imagem\",\n",
    "    ]\n",
    "\n",
    "    cleaned_json_data = [\n",
    "        {column: product.get(column, \"\") for column in output_columns} \n",
    "        for product in products_data\n",
    "    ]\n",
    "    \n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(cleaned_json_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "def run_scraper(output_json=\"ollie_products.json\", images_directory=\"images/\", \n",
    "                max_retries=3, timeout=20, max_products=80):\n",
    " \n",
    "    session = make_session(max_retries=max_retries, timeout=timeout)\n",
    "    \n",
    "    output_directory = os.path.dirname(output_json) or \".\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    os.makedirs(images_directory, exist_ok=True)\n",
    "\n",
    "    discovered_product_urls = scrape_listing(session, max_pages=20)\n",
    "    logging.info(\"Total de URLs: %d\", len(discovered_product_urls))\n",
    "\n",
    "    collected_products = []\n",
    "    processed_product_names = set()\n",
    "\n",
    "    for current_index, product_url in enumerate(discovered_product_urls, 1):\n",
    "        if len(collected_products) >= max_products:\n",
    "            logger.info(\"Limite de %d produtos atingido. Parando.\", max_products)\n",
    "            break\n",
    "\n",
    "        page_html, _ = fetch_html(session, product_url)\n",
    "        parsed_product_data = parse_product(page_html, product_url)\n",
    "        \n",
    "        if parsed_product_data is None:\n",
    "            logging.info(\"Skip (exclusão): %s\", product_url)\n",
    "            continue\n",
    "\n",
    "        product_name_key = (parsed_product_data.get(\"nome\") or \"\").strip().lower()\n",
    "        if product_name_key in processed_product_names:\n",
    "            logging.info(\"Skip (duplicado): %s\", product_name_key)\n",
    "            continue\n",
    "        \n",
    "        processed_product_names.add(product_name_key)\n",
    "\n",
    "        filename_slug = slugify(parsed_product_data.get(\"nome\") or os.path.basename(urlparse(product_url).path))\n",
    "        image_download_result = download_image(\n",
    "            session, \n",
    "            parsed_product_data.get(\"_imagem_url\",\"\"), \n",
    "            images_directory, \n",
    "            filename_slug\n",
    "        )\n",
    "\n",
    "        parsed_product_data.pop(\"_imagem_url\", None)\n",
    "        collected_products.append(parsed_product_data)\n",
    "\n",
    "        if current_index % 3 == 0:\n",
    "            write_json_output(collected_products, output_json)\n",
    "            logging.info(\"Parcial salva (%d itens).\", len(collected_products))\n",
    "\n",
    "    write_json_output(collected_products, output_json)\n",
    "    logging.info(\"Finalizado: %d itens\", len(collected_products))\n",
    "    \n",
    "    return len(collected_products), output_json, images_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fd41673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:06:44,618 | INFO | Página 1 | 33 produtos\n",
      "2025-10-10 13:06:45,978 | INFO | Página 2 | 6 produtos\n",
      "2025-10-10 13:06:45,978 | INFO | Página 2 | 6 produtos\n",
      "2025-10-10 13:06:47,596 | INFO | Página 3 | 33 produtos\n",
      "2025-10-10 13:06:47,596 | INFO | Nenhum novo produto. Encerrando.\n",
      "2025-10-10 13:06:47,597 | INFO | Total de URLs descobertas: 39\n",
      "2025-10-10 13:06:47,596 | INFO | Página 3 | 33 produtos\n",
      "2025-10-10 13:06:47,596 | INFO | Nenhum novo produto. Encerrando.\n",
      "2025-10-10 13:06:47,597 | INFO | Total de URLs descobertas: 39\n",
      "2025-10-10 13:06:50,540 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:50,540 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:51,879 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:51,879 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:53,307 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:53,307 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:54,904 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:54,904 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:56,382 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:56,382 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:57,914 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:06:57,914 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-10-10 13:07:01,068 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:01,068 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:02,402 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:02,402 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:03,692 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:03,692 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:04,868 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:04,868 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:06,234 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:06,234 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-10-10 13:07:12,463 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-hidratante-corporal-fps-60\n",
      "2025-10-10 13:07:12,463 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-hidratante-corporal-fps-60\n",
      "2025-10-10 13:07:17,057 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/iluminador-corporal-fps-40\n",
      "2025-10-10 13:07:17,057 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/iluminador-corporal-fps-40\n",
      "2025-10-10 13:07:20,081 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-10-10 13:07:20,081 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-10-10 13:07:21,501 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-10-10 13:07:21,501 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-10-10 13:07:22,809 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/ollie-club-meia-esportiva\n",
      "2025-10-10 13:07:22,809 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/ollie-club-meia-esportiva\n",
      "2025-10-10 13:07:24,268 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/manguito-ollie\n",
      "2025-10-10 13:07:24,268 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/manguito-ollie\n",
      "2025-10-10 13:07:25,263 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-pratica\n",
      "2025-10-10 13:07:25,263 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-pratica\n",
      "2025-10-10 13:07:26,354 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-iluminada\n",
      "2025-10-10 13:07:26,354 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-iluminada\n",
      "2025-10-10 13:07:27,607 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-iluminada\n",
      "2025-10-10 13:07:27,607 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-iluminada\n",
      "2025-10-10 13:07:29,246 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-de-verao-o-ano-todo\n",
      "2025-10-10 13:07:29,246 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-de-verao-o-ano-todo\n",
      "2025-10-10 13:07:30,729 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/loucos-por-esporte\n",
      "2025-10-10 13:07:30,729 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/loucos-por-esporte\n",
      "2025-10-10 13:07:31,810 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-resistencia\n",
      "2025-10-10 13:07:31,810 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-resistencia\n",
      "2025-10-10 13:07:33,438 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-performance\n",
      "2025-10-10 13:07:33,438 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-performance\n",
      "2025-10-10 13:07:37,960 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-spray-corporal\n",
      "2025-10-10 13:07:37,960 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-spray-corporal\n",
      "2025-10-10 13:07:39,476 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/caixa-especial-para-presente\n",
      "2025-10-10 13:07:39,476 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/caixa-especial-para-presente\n",
      "2025-10-10 13:07:41,171 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-completona\n",
      "2025-10-10 13:07:41,171 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-completona\n",
      "2025-10-10 13:07:42,879 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-para-peles-oleosas\n",
      "2025-10-10 13:07:42,879 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-para-peles-oleosas\n",
      "2025-10-10 13:07:44,190 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/charms-ollie\n",
      "2025-10-10 13:07:44,190 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/charms-ollie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: ollie_products.json (10 produtos)\n",
      "____________________________________________________________________________________________________________\n",
      "\n",
      "Fim da execução! Produtos extraídos: 10\n",
      "JSON: /home/usuario/Área de trabalho/Dados/Ollie/ollie_products.json\n",
      "Imagens salvas em: /home/usuario/Área de trabalho/Dados/Ollie/images\n",
      "____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def save_data(products_data: List[Dict]):\n",
    "    \"\"\"Salva dados dos produtos em formato JSON estruturado\"\"\"\n",
    "    if not products_data:\n",
    "        print(\"\\nNenhum dado para salvar.\")\n",
    "        return\n",
    "\n",
    "    cleaned_products = []\n",
    "    for product in products_data:\n",
    "        cleaned_products.append({\n",
    "            \"marca\": product.get(\"marca\"),\n",
    "            \"nome\": product.get(\"nome\"),\n",
    "            \"subtitulo\": product.get(\"subtitulo\"),\n",
    "            \"categoria\": product.get(\"categoria\"),\n",
    "            \"quantidade\": product.get(\"quantidade\"),\n",
    "            \"preco\": product.get(\"preco\"),\n",
    "            \"beneficios\": product.get(\"beneficios\"),\n",
    "            \"ingredientes\": product.get(\"ingredientes\"),\n",
    "            \"tipo_pele\": product.get(\"tipo_pele\"),\n",
    "            \"imagem\": product.get(\"imagem\"),\n",
    "        })\n",
    "\n",
    "    with open(\"ollie_products.json\", \"w\", encoding=\"utf-8\") as json_output:\n",
    "        json.dump(cleaned_products, json_output, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"JSON: ollie_products.json ({len(cleaned_products)} produtos)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    OUTPUT_JSON = \"ollie_products.json\"\n",
    "    IMAGES_DIRECTORY = \"images/\"\n",
    "    MAX_RETRIES = 3\n",
    "    TIMEOUT = 20\n",
    "    MAX_PRODUCTS = 80\n",
    "\n",
    "    try:\n",
    "        session = make_session(max_retries=MAX_RETRIES, timeout=TIMEOUT)\n",
    "        \n",
    "        output_directory = os.path.dirname(OUTPUT_JSON) or \".\"\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        os.makedirs(IMAGES_DIRECTORY, exist_ok=True)\n",
    "\n",
    "        discovered_product_urls = scrape_listing(session, max_pages=20)\n",
    "        logging.info(\"Total de URLs descobertas: %d\", len(discovered_product_urls))\n",
    "\n",
    "        collected_products = []\n",
    "        processed_product_names = set()\n",
    "\n",
    "        for current_index, product_url in enumerate(discovered_product_urls, 1):\n",
    "            if len(collected_products) >= MAX_PRODUCTS:\n",
    "                logger.info(\"Limite de %d produtos atingido. Parando.\", MAX_PRODUCTS)\n",
    "                break\n",
    "\n",
    "            page_html, _ = fetch_html(session, product_url)\n",
    "            parsed_product_data = parse_product(page_html, product_url)\n",
    "            \n",
    "            if parsed_product_data is None:\n",
    "                logging.info(\"Skip (exclusão): %s\", product_url)\n",
    "                continue\n",
    "\n",
    "            product_name_key = (parsed_product_data.get(\"nome\") or \"\").strip().lower()\n",
    "            if product_name_key in processed_product_names:\n",
    "                logging.info(\"Skip (duplicado): %s\", product_name_key)\n",
    "                continue\n",
    "            \n",
    "            processed_product_names.add(product_name_key)\n",
    "\n",
    "            filename_slug = slugify(parsed_product_data.get(\"nome\") or os.path.basename(urlparse(product_url).path))\n",
    "            download_image(\n",
    "                session, \n",
    "                parsed_product_data.get(\"_imagem_url\",\"\"), \n",
    "                IMAGES_DIRECTORY, \n",
    "                filename_slug\n",
    "            )\n",
    "\n",
    "            parsed_product_data.pop(\"_imagem_url\", None)\n",
    "            collected_products.append(parsed_product_data)\n",
    "\n",
    "        save_data(collected_products)\n",
    "        print(\"____________________________________________________________________________________________________________\")\n",
    "        print(f\"\\nFim da execução! Produtos extraídos: {len(collected_products)}\")\n",
    "        print(f\"JSON: {os.path.abspath(OUTPUT_JSON)}\")\n",
    "        print(f\"Imagens salvas em: {os.path.abspath(IMAGES_DIRECTORY)}\")\n",
    "        print(\"____________________________________________________________________________________________________________\")\n",
    "\n",
    "    except Exception as execution_error:\n",
    "        print(f\"\\nERRO: {execution_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d46fde",
   "metadata": {},
   "source": [
    "## Conversão JSON para CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a624f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV gerado: ollie_products.csv (8 linhas)\n",
      "A partir do JSON: ollie_products.json\n"
     ]
    }
   ],
   "source": [
    "def json_to_csv(json_file=\"ollie_products.json\", csv_file=\"ollie_products.csv\"):\n",
    "\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Nenhum dado encontrado no arquivo {json_file}\")\n",
    "            return\n",
    "        \n",
    "        cols = [\"marca\", \"nome\", \"subtitulo\", \"categoria\", \"quantidade\", \"preco\", \"beneficios\", \"ingredientes\", \"tipo_pele\", \"imagem\"]\n",
    "        \n",
    "        with open(csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=cols)\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "\n",
    "                csv_row = {k: (row.get(k) or \"\") for k in cols}\n",
    "                writer.writerow(csv_row)\n",
    "        \n",
    "        print(f\"CSV gerado: {csv_file} ({len(data)} linhas)\")\n",
    "        print(f\"A partir do JSON: {json_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Arquivo {json_file} não encontrado!\")\n",
    "        \n",
    "        import glob\n",
    "        json_files = glob.glob(\"*.json\")\n",
    "        if json_files:\n",
    "            for f in json_files:\n",
    "                print(f\"   - {f}\")\n",
    "        else:\n",
    "            print(\"   Nenhum arquivo .json encontrado\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter JSON para CSV: {e}\")\n",
    "\n",
    "\n",
    "json_to_csv(\"ollie_products.json\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
