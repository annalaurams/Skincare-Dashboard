{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54eb250f",
   "metadata": {},
   "source": [
    "# Creamy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6308cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, os, re, csv, json, time\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "sys.path.append(os.path.abspath(\"/home/usuario/√Årea de trabalho/Dados/models\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "\n",
    "from exclude import (\n",
    "    EXCLUDE_KEYWORDS,\n",
    ")\n",
    "\n",
    "from ingredient import (\n",
    "    INGREDIENTES_VALIDOS,\n",
    ")\n",
    "\n",
    "from benefits import (\n",
    "    BENEFIT_SYNONYMS_PT,\n",
    "    BENEFIT_CANONICAL_ORDER,\n",
    ")\n",
    "\n",
    "from category import (CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01abbf",
   "metadata": {},
   "source": [
    "## Informa√ß√µes Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5598dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.creamy.com.br/\"\n",
    "LISTING_URL_TEMPLATE = \"https://www.creamy.com.br/produtos?page={page}\"\n",
    "MAX_PAGES = 9\n",
    "\n",
    "OUT_JSON = \"creamy_products.json\"\n",
    "OUT_CSV  = \"creamy_products.csv\"\n",
    "IMG_DIR  = \"images\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8a3d3",
   "metadata": {},
   "source": [
    "## Utilit√°rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "339222d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def _strip_accents_lower(s: str) -> str:\n",
    "    return strip_accents(s or \"\").lower().strip()\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = strip_accents(text.lower())\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
    "    text = re.sub(r\"-+\", \"-\", text).strip(\"-\")\n",
    "    return text or \"produto\"\n",
    "\n",
    "def get_soup(url, max_retries=3, timeout=25):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                return BeautifulSoup(r.text, \"lxml\")\n",
    "            print(f\"[WARN] {url} -> status {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {url} -> {e}\")\n",
    "        time.sleep(1.1 * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def looks_excluded(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica se o texto cont√©m alguma palavra da lista EXCLUDE_KEYWORDS.\n",
    "    Retorna True se encontrar qualquer palavra de exclus√£o.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Lista completa e atualizada de palavras de exclus√£o\n",
    "    exclude_list = [\n",
    "        'kit', 'kits', 'combo', 'duo', 'dupla', 'trio', 'rotina', 'corporal', 'corpo', 'hair',\n",
    "        'cabelo', 'shampoo', 'condicionador', 'body', 'capilar',\n",
    "        'manguito', 'meia', 'mistery', 'caixa', 'refil', 'caneta', 'geladeira', 'massageador', \n",
    "        'pincel', 'pincel', 'adesivo', 'adesivos', 'faixa', 'mini-geladeira', \n",
    "        'maquiagem', 'fragrancia', 'fragancia', 'perfume', 'deodorante', 'desodorante'\n",
    "    ]\n",
    "    \n",
    "    # Normaliza o texto para compara√ß√£o (remove acentos e converte para min√∫sculas)\n",
    "    text_norm = strip_accents(text.lower())\n",
    "    \n",
    "    # Verifica cada palavra de exclus√£o\n",
    "    for keyword in exclude_list:\n",
    "        if keyword and strip_accents(keyword.lower()) in text_norm:\n",
    "            return True\n",
    "    \n",
    "    # Tamb√©m verifica as palavras do EXCLUDE_KEYWORDS importado (caso esteja atualizado)\n",
    "    for keyword in EXCLUDE_KEYWORDS:\n",
    "        if keyword and strip_accents(keyword.lower()) in text_norm:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def parse_price_to_str(price_text: str) -> str:\n",
    "    if not price_text:\n",
    "        return \"\"\n",
    "    t = price_text.replace(\"R$\", \"\").replace(\"r$\", \"\").strip()\n",
    "    t = t.replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \".\")\n",
    "    m = re.findall(r\"[0-9]+(?:\\.[0-9]{1,2})?\", t)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return f\"{float(m[0]):.2f}\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def split_list_candidates(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    t = text.replace(\"<br>\", \";\").replace(\"<br/>\", \";\").replace(\"<br />\", \";\")\n",
    "    parts = re.split(r\"[;‚Ä¢|/\\n,]\", t)\n",
    "    return [normalize_space(p) for p in parts if normalize_space(p)]\n",
    "\n",
    "def filter_ingredients_creamy(ings_raw):\n",
    "    allowed_norm = [a.lower() for a in INGREDIENTES_VALIDOS]\n",
    "    allowed_noacc = [strip_accents(a.lower()) for a in INGREDIENTES_VALIDOS]\n",
    "    out = []\n",
    "    for ing in ings_raw:\n",
    "        ing_l = ing.lower()\n",
    "        ing_noacc = strip_accents(ing_l)\n",
    "        match = None\n",
    "        for a_norm, a_noacc in zip(allowed_norm, allowed_noacc):\n",
    "            if a_norm in ing_l or a_noacc in ing_noacc:\n",
    "                match = a_norm\n",
    "                break\n",
    "        if match and match not in out:\n",
    "            out.append(match)\n",
    "    return \"; \".join(out)\n",
    "\n",
    "def find_all_text(soup, selectors):\n",
    "    for sel in selectors:\n",
    "        node = soup.select_one(sel)\n",
    "        if node and node.get_text(strip=True):\n",
    "            return node.get_text(\" \", strip=True)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee1cb8",
   "metadata": {},
   "source": [
    "## Benef√≠cios, Ingredientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6de8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padroniza_beneficios(textos_beneficios):\n",
    "    if not textos_beneficios:\n",
    "        return []\n",
    "    encontrados = set()\n",
    "    norm_syn = {\n",
    "        canonico: [_strip_accents_lower(s) for s in patt_list if s]\n",
    "        for canonico, patt_list in BENEFIT_SYNONYMS_PT.items()\n",
    "    }\n",
    "    for txt in textos_beneficios:\n",
    "        n = _strip_accents_lower(txt)\n",
    "        for canonico, padds in norm_syn.items():\n",
    "            if any(patt in n for patt in padds):\n",
    "                encontrados.add(canonico)\n",
    "    if BENEFIT_CANONICAL_ORDER:\n",
    "        order_map = {name: i for i, name in enumerate(BENEFIT_CANONICAL_ORDER)}\n",
    "        return sorted(list(encontrados), key=lambda x: order_map.get(x, 999))\n",
    "    return sorted(list(encontrados))\n",
    "\n",
    "def extract_benefits(soup):\n",
    "    items = []\n",
    "    for ul in soup.select(\"ul, ol\"):\n",
    "        lis = [normalize_space(li.get_text(\" \", strip=True)) for li in ul.select(\"li\")]\n",
    "        for li in lis:\n",
    "            if 0 < len(li) <= 120:\n",
    "                items.append(li)\n",
    "    for th in soup.select(\"th, td\"):\n",
    "        txt = normalize_space(th.get_text(\" \", strip=True))\n",
    "        if 0 < len(txt) <= 120:\n",
    "            items.append(txt)\n",
    "    uniq, seen = [], set()\n",
    "    for it in items:\n",
    "        if it and it not in seen:\n",
    "            uniq.append(it); seen.add(it)\n",
    "    pad = padroniza_beneficios(uniq)\n",
    "    return \"; \".join(pad)\n",
    "\n",
    "def extract_ingredients(soup):\n",
    "    possible_labels = [\n",
    "        \"ingredientes\", \"composi√ß√£o\", \"composicao\", \"f√≥rmula\", \"formula\", \"ingredients\", \"active ingredients\"\n",
    "    ]\n",
    "    text_blocks = []\n",
    "    for el in soup.select(\"div, section, table, article, ul, ol, p\"):\n",
    "        txt = el.get_text(\" \", strip=True)\n",
    "        low = txt.lower()\n",
    "        if any(lbl in low for lbl in possible_labels):\n",
    "            text_blocks.append(txt)\n",
    "    text_blocks = sorted(set(text_blocks), key=len)\n",
    "    raw = []\n",
    "    for block in text_blocks:\n",
    "        raw.extend(split_list_candidates(block))\n",
    "    raw = [r for r in raw if len(r) <= 100]\n",
    "    return filter_ingredients_creamy(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22c2d5",
   "metadata": {},
   "source": [
    "## Pre√ßo e Tipo de pele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9d52aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_size_from_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = re.search(r\"(\\d+[\\.,]?\\d*)\\s*(ml|g|l)\\b\", text.lower())\n",
    "    if m:\n",
    "        val = m.group(1).replace(\",\", \".\")\n",
    "        unit = m.group(2).upper()\n",
    "        if unit == 'L' and not val.endswith('L'):\n",
    "            return f\"{val}L\"\n",
    "        return f\"{val}{unit}\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_tipos_pele(soup):\n",
    "    def _strip_accents_lower(s: str) -> str:\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "        s = s.lower()\n",
    "        s = s.replace(\"-\", \" \")\n",
    "        s = re.sub(r\"[^\\w\\s]\", \" \", s)   \n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    page_text = soup.get_text(\" \", strip=True)\n",
    "    txt_norm = _strip_accents_lower(page_text)\n",
    "\n",
    "    norm_syn = {\n",
    "        canonical: [_strip_accents_lower(s) for s in syns if s]\n",
    "        for canonical, syns in SKIN_TYPE_SYNONYMS_PT.items()\n",
    "    }\n",
    "\n",
    "    encontrados = set()\n",
    "    \n",
    "    # Primeiro, verifica se tem \"todos os tipos\" - se tiver, retorna APENAS isso\n",
    "    todos_synonyms = norm_syn.get(\"todos os tipos\", [])\n",
    "    for pattern in todos_synonyms:\n",
    "        if pattern and pattern in txt_norm:\n",
    "            return \"todos os tipos\"\n",
    "    \n",
    "    # Se n√£o tem \"todos os tipos\", processa os outros tipos normalmente\n",
    "    for canonical, patterns in norm_syn.items():\n",
    "        if canonical == \"todos os tipos\":\n",
    "            continue\n",
    "        if any(p and p in txt_norm for p in patterns):\n",
    "            encontrados.add(canonical)\n",
    "\n",
    "    if not encontrados:\n",
    "        return \"\"  \n",
    "\n",
    "    order_map = {name: i for i, name in enumerate(SKIN_TYPE_CANONICAL_ORDER or [])}\n",
    "    ordered = sorted(encontrados, key=lambda x: order_map.get(x, 10_000))\n",
    "\n",
    "    return \"; \".join(ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988ead5",
   "metadata": {},
   "source": [
    "## Imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2440f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(soup, product_name):\n",
    "    selectors = [\n",
    "        \"img.vtex-store-components-3-x-productImageTag\",\n",
    "        \"img.product-image\",\n",
    "        \"img[src*='/arquivos/']\",\n",
    "        \"img[src*='cdn']\",\n",
    "    ]\n",
    "    src = None\n",
    "    for sel in selectors:\n",
    "        node = soup.select_one(sel)\n",
    "        if node and node.get(\"src\"):\n",
    "            src = node.get(\"src\"); break\n",
    "        if node and node.get(\"data-src\"):\n",
    "            src = node.get(\"data-src\"); break\n",
    "    if not src:\n",
    "        return \"\"\n",
    "    img_url = src if src.startswith(\"http\") else urljoin(BASE_URL, src)\n",
    "    from urllib.parse import urlparse\n",
    "    ext = os.path.splitext(urlparse(img_url).path)[1] or \".jpg\"\n",
    "    fname = f\"{slugify(product_name)}{ext}\"\n",
    "    fpath = os.path.join(IMG_DIR, fname)\n",
    "    try:\n",
    "        r = SESSION.get(img_url, timeout=25)\n",
    "        if r.status_code == 200 and r.content:\n",
    "            with open(fpath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            return fname\n",
    "    except Exception as e:\n",
    "        print(f\"[IMG] Falha ao baixar {img_url}: {e}\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2301ed9",
   "metadata": {},
   "source": [
    "## Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bdac5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CAT_ORDER_MAP = {c: i for i, c in enumerate(CATEGORY_CANONICAL_ORDER)}\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return _strip_accents_lower(s or \"\")\n",
    "\n",
    "def classify_category_from_name(name: str, subtitle: str | None = None, desc: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Usa CATEGORY_HINTS para mapear nome/subt√≠tulo/descri√ß√£o em uma categoria can√¥nica.\n",
    "    \"\"\"\n",
    "    txt = _norm(f\"{name or ''} {subtitle or ''} {desc or ''}\")\n",
    "    hits = []\n",
    "    for cat, needles in CATEGORY_HINTS.items():\n",
    "        for needle in needles:\n",
    "            if _norm(needle) in txt:\n",
    "                hits.append(cat)\n",
    "                break\n",
    "    if not hits:\n",
    "        return \"\"   # ou \"outros\"\n",
    "    hits.sort(key=lambda c: _CAT_ORDER_MAP.get(c, 10_000))\n",
    "    return hits[0]\n",
    "\n",
    "def guess_category(url: str, name: str) -> str:\n",
    "    \"\"\"\n",
    "    Usa apenas CATEGORY_HINTS para classificar categoria, removendo mapeamento manual.\n",
    "    \"\"\"\n",
    "    # Usa a mesma l√≥gica de classify_category_from_name\n",
    "    return classify_category_from_name(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c2f3d",
   "metadata": {},
   "source": [
    "## Produtos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d881e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e5090aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product_page(url, fallback_category=\"\"):\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        return None\n",
    "    \n",
    "    name = find_all_text(soup, [\n",
    "        \"h1.vtex-store-components-3-x-productName\",\n",
    "        \"h1.productName\",\n",
    "        \"h1\",\n",
    "        \"div.product-name h1\",\n",
    "    ])\n",
    "    if not name:\n",
    "        if soup.title and soup.title.string:\n",
    "            name = soup.title.string.split(\"|\")[0].strip()\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    if looks_excluded(name) or looks_excluded(url):\n",
    "        print(f\"[SKIP] Produto exclu√≠do: {name}\")\n",
    "        return None\n",
    "\n",
    "    subtitle = find_all_text(soup, [\n",
    "        \"span.vtex-product-summary-2-x-description-short div\",\n",
    "        \"span.vtex-product-summary-2-x-description-short\",\n",
    "        \"div.vtex-rich-text-0-x-container p\",\n",
    "        \"div.productDescription\",\n",
    "        \"div.product-brief\",\n",
    "    ])\n",
    "    subtitle = subtitle if (subtitle and len(subtitle) <= 220) else \"\"\n",
    "    \n",
    "    # Corrigida a extra√ß√£o de pre√ßo para capturar os novos seletores\n",
    "    price_text = find_all_text(soup, [\n",
    "        \"p.priceCustom__sellingPrice span\",  # Novo seletor espec√≠fico\n",
    "        \"span.vtex-product-price-1-x-sellingPriceValue\",\n",
    "        \"span.selling-price\",\n",
    "        \"span.price\",\n",
    "    ])\n",
    "    price = parse_price_to_str(price_text)\n",
    "    \n",
    "    beneficios = extract_benefits(soup)\n",
    "    ingredientes = extract_ingredients(soup)\n",
    "    tipos_pele = extract_tipos_pele(soup)\n",
    "    \n",
    "    size = extract_size_from_text(name)\n",
    "    if not size:\n",
    "        size = extract_size_from_text(subtitle)\n",
    "    if not size:\n",
    "        details_txt = find_all_text(soup, [\n",
    "            \"div.vtex-store-components-3-x-productDescriptionText\",\n",
    "            \"div.productDescription\",\n",
    "            \"section#descricao\",\n",
    "        ])\n",
    "        size = extract_size_from_text(details_txt)\n",
    "    \n",
    "    cat_by_hints = classify_category_from_name(name, subtitle)\n",
    "    categoria = cat_by_hints or fallback_category or guess_category(url, name)\n",
    "    \n",
    "    img_name = download_image(soup, name)\n",
    "    \n",
    "    return {\n",
    "        \"marca\": \"creamy\",\n",
    "        \"nome\": name.strip(),\n",
    "        \"subtitulo\": subtitle if subtitle else \"\",\n",
    "        \"categoria\": categoria,\n",
    "        \"preco\": price if price else \"\",\n",
    "        \"quantidade\": size if size else \"\",\n",
    "        \"beneficios\": beneficios.lower() if beneficios else \"\",\n",
    "        \"ingredientes\": ingredientes.lower() if ingredientes else \"\",\n",
    "        \"tipos_pele\": tipos_pele,\n",
    "        \"imagem\": img_name,\n",
    "    }\n",
    "\n",
    "def listing_get_product_links(page_url: str):\n",
    "    soup = get_soup(page_url)\n",
    "    if soup is None:\n",
    "        return []\n",
    "    links = set()\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = href if href.startswith(\"http\") else urljoin(BASE_URL, href)\n",
    "        if re.search(r\"/p($|\\?)\", full):\n",
    "            links.add(full)\n",
    "    return sorted(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e41381",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6e1d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LIST] https://www.creamy.com.br/produtos?page=1\n",
      "  - 10 links\n",
      "  - 10 links\n",
      "  [+] Creme Retexturizador - √Åcido Glic√≥lico :: 84.20\n",
      "  [+] Creme Retexturizador - √Åcido Glic√≥lico :: 84.20\n",
      "  [+] S√©rum Renovador Suave - √Åcido L√°tico :: 73.67\n",
      "  [+] S√©rum Renovador Suave - √Åcido L√°tico :: 73.67\n",
      "  [+] Gel Clareador Antiacne - √Åcido Mand√©lico :: 84.20\n",
      "  [+] Gel Clareador Antiacne - √Åcido Mand√©lico :: 84.20\n",
      "  [+] Gel-creme Hidratante Calmante - Calming Cream :: 52.62\n",
      "  [+] Gel-creme Hidratante Calmante - Calming Cream :: 52.62\n",
      "  [+] Creme Clareador para Olhos - Eye Cream :: 136.83\n",
      "  [+] Creme Clareador para Olhos - Eye Cream :: 136.83\n",
      "  [+] Gel de Limpeza :: 63.15\n",
      "  [+] Gel de Limpeza :: 63.15\n",
      "  [+] Protetor  Solar FPS 60 :: 63.15\n",
      "  [+] Protetor  Solar FPS 60 :: 63.15\n",
      "  [+] Retinol - Creme Corretivo Anti sinais :: 105.25\n",
      "  [+] Retinol - Creme Corretivo Anti sinais :: 105.25\n",
      "  [+] S√©rum Antioxidante Clareador - Vitamina C Gold :: 126.31\n",
      "  [+] S√©rum Antioxidante Clareador - Vitamina C Gold :: 126.31\n",
      "  [+] S√©rum Antioxidante - Vitamina C :: 105.25\n",
      "  [+] S√©rum Antioxidante - Vitamina C :: 105.25\n",
      "[LIST] https://www.creamy.com.br/produtos?page=2\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=2\n",
      "  - 11 links\n",
      "  [+] S√©rum Hidratante com √Åcido Hialur√¥nico :: 63.15\n",
      "  [+] S√©rum Hidratante com √Åcido Hialur√¥nico :: 63.15\n",
      "  [+] Creme de Tratamento Labial - Lip Balm Incolor :: 31.49\n",
      "  [+] Creme de Tratamento Labial - Lip Balm Incolor :: 31.49\n",
      "  [+] S√©rum Fortalecedor Antioleosidade - Niacinamide B :: 94.73\n",
      "  [+] S√©rum Fortalecedor Antioleosidade - Niacinamide B :: 94.73\n",
      "[LIST] https://www.creamy.com.br/produtos?page=3\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=3\n",
      "  - 11 links\n",
      "  [+] Emuls√£o de Limpeza :: 63.15\n",
      "  [+] Emuls√£o de Limpeza :: 63.15\n",
      "[LIST] https://www.creamy.com.br/produtos?page=4\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=4\n",
      "  - 11 links\n",
      "  [+] S√©rum de Preven√ß√£o Anti idade :: 73.67\n",
      "  [+] S√©rum de Preven√ß√£o Anti idade :: 73.67\n",
      "[LIST] https://www.creamy.com.br/produtos?page=5\n",
      "[LIST] https://www.creamy.com.br/produtos?page=5\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=6\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=6\n",
      "  - 10 links\n",
      "  - 10 links\n",
      "  [+] S√©rum Antiacne Adapalenato :: 126.31\n",
      "  [+] S√©rum Antiacne Adapalenato :: 126.31\n",
      "[LIST] https://www.creamy.com.br/produtos?page=7\n",
      "[LIST] https://www.creamy.com.br/produtos?page=7\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=8\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=8\n",
      "  - 11 links\n",
      "  - 11 links\n",
      "  [+] Gel-creme Retexturizador Antioleosidade - Glicointense Peel :: 105.25\n",
      "  [+] Gel-creme Retexturizador Antioleosidade - Glicointense Peel :: 105.25\n",
      "  [+] Creme Hidratante Antivermelhid√£o - Intensive Cream :: 105.25\n",
      "  [+] Creme Hidratante Antivermelhid√£o - Intensive Cream :: 105.25\n",
      "  [+] Limpador Facial Glicerinado :: 63.15\n",
      "  [+] Limpador Facial Glicerinado :: 63.15\n",
      "  [+] Creme Hidratante Labial Latte :: 36.83\n",
      "  [+] Creme Hidratante Labial Latte :: 36.83\n",
      "[LIST] https://www.creamy.com.br/produtos?page=9\n",
      "[LIST] https://www.creamy.com.br/produtos?page=9\n",
      "  - 11 links\n",
      "Total coletado: 20\n",
      "[OK] JSON salvo em creamy_products.json (20 itens)\n",
      "Pr√©via do primeiro item:\n",
      "{\n",
      "  \"marca\": \"creamy\",\n",
      "  \"nome\": \"Creme Retexturizador - √Åcido Glic√≥lico\",\n",
      "  \"subtitulo\": \"Reduz poros e melhora a textura da pele\",\n",
      "  \"categoria\": \"s√©rum\",\n",
      "  \"preco\": \"84.20\",\n",
      "  \"quantidade\": \"\",\n",
      "  \"beneficios\": \"antissinais; col√°geno; controle da oleosidade; hidrata√ß√£o; minimiza poros; suaviza textura\",\n",
      "  \"ingredientes\": \"√°cido glic√≥lico; lha; niacinamida; benzoato de s√≥dio; √°lcool cet√≠lico; hidr√≥xido de am√¥nio; bisabolol; goma xantana; fenoxietanol; glicerina; stearic acid; polysorbate; cyclopentasiloxane; phenoxyethanol; ethylhexylglycerin; disodium edta\",\n",
      "  \"tipos_pele\": \"todos os tipos\",\n",
      "  \"imagem\": \"creme-retexturizador-acido-glicolico.jpg\"\n",
      "}\n",
      "  - 11 links\n",
      "Total coletado: 20\n",
      "[OK] JSON salvo em creamy_products.json (20 itens)\n",
      "Pr√©via do primeiro item:\n",
      "{\n",
      "  \"marca\": \"creamy\",\n",
      "  \"nome\": \"Creme Retexturizador - √Åcido Glic√≥lico\",\n",
      "  \"subtitulo\": \"Reduz poros e melhora a textura da pele\",\n",
      "  \"categoria\": \"s√©rum\",\n",
      "  \"preco\": \"84.20\",\n",
      "  \"quantidade\": \"\",\n",
      "  \"beneficios\": \"antissinais; col√°geno; controle da oleosidade; hidrata√ß√£o; minimiza poros; suaviza textura\",\n",
      "  \"ingredientes\": \"√°cido glic√≥lico; lha; niacinamida; benzoato de s√≥dio; √°lcool cet√≠lico; hidr√≥xido de am√¥nio; bisabolol; goma xantana; fenoxietanol; glicerina; stearic acid; polysorbate; cyclopentasiloxane; phenoxyethanol; ethylhexylglycerin; disodium edta\",\n",
      "  \"tipos_pele\": \"todos os tipos\",\n",
      "  \"imagem\": \"creme-retexturizador-acido-glicolico.jpg\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_scraper():\n",
    "    visited = set()\n",
    "    items = []\n",
    "    for page in range(1, MAX_PAGES+1):\n",
    "        url = LISTING_URL_TEMPLATE.format(page=page)\n",
    "        print(f\"[LIST] {url}\")\n",
    "        prod_links = listing_get_product_links(url)\n",
    "        print(f\"  - {len(prod_links)} links\")\n",
    "        for purl in prod_links:\n",
    "            if purl in visited or looks_excluded(purl):\n",
    "                continue\n",
    "            item = parse_product_page(purl)\n",
    "            if item:\n",
    "                visited.add(purl)\n",
    "                items.append(item)\n",
    "                print(f\"  [+] {item['nome']} :: {item['preco']}\")\n",
    "            time.sleep(0.6)\n",
    "    return items\n",
    "\n",
    "def save_data(items):\n",
    "    \"\"\"Salva apenas dados em JSON - CSV removido conforme solicitado\"\"\"\n",
    "    cols = [\"marca\", \"nome\", \"subtitulo\", \"categoria\", \"quantidade\", \"preco\", \"beneficios\", \"ingredientes\", \"tipo_pele\", \"imagem\"]\n",
    "    clean = [{k: it.get(k, \"\") for k in cols} for it in items]\n",
    "    \n",
    "    with open(\"creamy_products.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clean, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[OK] JSON salvo em creamy_products.json ({len(clean)} itens)\")\n",
    "\n",
    "# Executar o scraper\n",
    "if __name__ == \"__main__\":\n",
    "    items = run_scraper()\n",
    "    print(f\"Total coletado: {len(items)}\")\n",
    "    save_data(items)\n",
    "    if items:\n",
    "        print(\"Pr√©via do primeiro item:\")\n",
    "        print(json.dumps(items[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efc47c",
   "metadata": {},
   "source": [
    "## Fun√ß√£o Opcional: Converter JSON para CSV\n",
    "\n",
    "Use a fun√ß√£o abaixo apenas quando precisar gerar um arquivo CSV a partir do JSON j√° salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf4a78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erro ao converter JSON para CSV: name 'json' is not defined\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def json_to_csv(json_file=\"alterado.json\", csv_file=\"creamy_products.csv\"):\n",
    "    \"\"\"\n",
    "    Converte o arquivo JSON salvo para CSV.\n",
    "    Uso: \n",
    "    - json_to_csv()  # usa o arquivo padr√£o\n",
    "    - json_to_csv(\"meu_arquivo.json\", \"meu_arquivo.csv\")  # usa arquivo personalizado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Nenhum dado encontrado no arquivo {json_file}\")\n",
    "            return\n",
    "        \n",
    "        cols = [\"marca\", \"nome\", \"subtitulo\", \"categoria\", \"quantidade\", \"preco\", \"beneficios\", \"ingredientes\", \"tipo_pele\", \"imagem\"]\n",
    "        \n",
    "        with open(csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=cols)\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                # Converte None para string vazia no CSV\n",
    "                csv_row = {k: (row.get(k) or \"\") for k in cols}\n",
    "                writer.writerow(csv_row)\n",
    "        \n",
    "        print(f\"‚úÖ CSV gerado: {csv_file} ({len(data)} linhas)\")\n",
    "        print(f\"üìÅ A partir do JSON: {json_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Arquivo {json_file} n√£o encontrado!\")\n",
    "        print(\"üìÇ Arquivos JSON dispon√≠veis na pasta:\")\n",
    "        import glob\n",
    "        json_files = glob.glob(\"*.json\")\n",
    "        if json_files:\n",
    "            for f in json_files:\n",
    "                print(f\"   - {f}\")\n",
    "        else:\n",
    "            print(\"   Nenhum arquivo .json encontrado\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao converter JSON para CSV: {e}\")\n",
    "\n",
    "\n",
    "json_to_csv(\"alterado.json\")  # seu arquivo personalizado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
