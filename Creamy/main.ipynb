{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54eb250f",
   "metadata": {},
   "source": [
    "# Creamy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6308cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, os, re, csv, json, time\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "sys.path.append(os.path.abspath(\"./../models\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "\n",
    "from exclude import (\n",
    "    EXCLUDE_KEYWORDS,\n",
    ")\n",
    "\n",
    "from ingredient import (\n",
    "    INGREDIENTES_VALIDOS,\n",
    ")\n",
    "\n",
    "from benefits import (\n",
    "    BENEFIT_SYNONYMS_PT,\n",
    "    BENEFIT_CANONICAL_ORDER,\n",
    ")\n",
    "\n",
    "from category import (CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01abbf",
   "metadata": {},
   "source": [
    "### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5598dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.creamy.com.br/\"\n",
    "LISTING_URL_TEMPLATE = \"https://www.creamy.com.br/produtos?page={page}\"\n",
    "MAX_PAGES = 9\n",
    "\n",
    "OUT_JSON = \"creamy_products.json\"\n",
    "OUT_CSV  = \"creamy_products.csv\"\n",
    "IMG_DIR  = \"images\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8a3d3",
   "metadata": {},
   "source": [
    "## Utilitários\n",
    "\n",
    "### Funções auxiliares para normalização de texto, remoção de acentos, tokenização de ingredientes, padronização de dados, sanitização de nomes de arquivos e formatação de preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339222d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = strip_accents(text.lower())\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
    "    return re.sub(r\"-+\", \"-\", text).strip(\"-\") or \"produto\"\n",
    "\n",
    "def get_soup(url, max_retries=3, timeout=25):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                return BeautifulSoup(r.text, \"lxml\")\n",
    "            print(f\"[WARN] {url} -> status {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {url} -> {e}\")\n",
    "        time.sleep(1.1 * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def should_exclude_product(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    text_clean = strip_accents(text.lower())\n",
    "    \n",
    "    for keyword in EXCLUDE_KEYWORDS:\n",
    "        keyword_clean = strip_accents(keyword.lower())\n",
    "        if keyword_clean in text_clean:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def parse_price(price_text: str) -> str:\n",
    "    if not price_text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove símbolos e normaliza\n",
    "    clean_text = price_text.replace(\"R$\", \"\").replace(\"r$\", \"\").strip()\n",
    "    clean_text = clean_text.replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \".\")\n",
    "    \n",
    "    # Encontra números\n",
    "    numbers = re.findall(r\"[0-9]+(?:\\.[0-9]{1,2})?\", clean_text)\n",
    "    if not numbers:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        return f\"{float(numbers[0]):.2f}\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def split_text_list(text: str):\n",
    "    \"\"\"Divide texto em lista usando separadores comuns\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Remove tags HTML\n",
    "    text = text.replace(\"<br>\", \";\").replace(\"<br/>\", \";\").replace(\"<br />\", \";\")\n",
    "    \n",
    "    # Divide por separadores\n",
    "    parts = re.split(r\"[;•|/\\n,]\", text)\n",
    "    \n",
    "    return [normalize_space(part) for part in parts if normalize_space(part)]\n",
    "\n",
    "def filter_ingredients(raw_ingredients):\n",
    "    if not raw_ingredients:\n",
    "        return \"\"\n",
    "    \n",
    "    valid_ingredients = []\n",
    "    \n",
    "    for ingredient in raw_ingredients:\n",
    "        ingredient_clean = strip_accents(ingredient.lower())\n",
    "        \n",
    "        for valid in INGREDIENTES_VALIDOS:\n",
    "            valid_clean = strip_accents(valid.lower())\n",
    "            if valid_clean in ingredient_clean:\n",
    "                if valid_clean not in valid_ingredients:\n",
    "                    valid_ingredients.append(valid_clean)\n",
    "                break\n",
    "    \n",
    "    return \"; \".join(valid_ingredients)\n",
    "\n",
    "def find_text_by_selectors(soup, selectors):\n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element and element.get_text(strip=True):\n",
    "            return element.get_text(\" \", strip=True)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee1cb8",
   "metadata": {},
   "source": [
    "## Benefícios, Ingredientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6de8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_product_benefits(benefit_text_list):\n",
    "    if not benefit_text_list:\n",
    "        return []\n",
    "    \n",
    "    identified_benefits = set()\n",
    "    normalized_synonyms = {\n",
    "        canonical_benefit: [strip_accents(synonym).lower() for synonym in synonym_list if synonym]\n",
    "        for canonical_benefit, synonym_list in BENEFIT_SYNONYMS_PT.items()\n",
    "    }\n",
    "    \n",
    "    for benefit_text in benefit_text_list:\n",
    "        normalized_text = strip_accents(benefit_text).lower()\n",
    "        for canonical_benefit, pattern_list in normalized_synonyms.items():\n",
    "            if any(pattern in normalized_text for pattern in pattern_list):\n",
    "                identified_benefits.add(canonical_benefit)\n",
    "    \n",
    "    if BENEFIT_CANONICAL_ORDER:\n",
    "        benefit_order_mapping = {benefit_name: index for index, benefit_name in enumerate(BENEFIT_CANONICAL_ORDER)}\n",
    "        sorted_benefits = sorted(list(identified_benefits), key=lambda benefit: benefit_order_mapping.get(benefit, 999))\n",
    "        return sorted_benefits\n",
    "    \n",
    "    return sorted(list(identified_benefits))\n",
    "\n",
    "def extract_product_benefits(html_soup):\n",
    "    extracted_benefit_items = []\n",
    "    \n",
    "    list_elements = html_soup.select(\"ul, ol\")\n",
    "    for list_element in list_elements:\n",
    "        list_item_texts = [normalize_space(list_item.get_text(\" \", strip=True)) for list_item in list_element.select(\"li\")]\n",
    "        for item_text in list_item_texts:\n",
    "            if 0 < len(item_text) <= 120:\n",
    "                extracted_benefit_items.append(item_text)\n",
    "    \n",
    "    table_elements = html_soup.select(\"th, td\")\n",
    "    for table_element in table_elements:\n",
    "        element_text = normalize_space(table_element.get_text(\" \", strip=True))\n",
    "        if 0 < len(element_text) <= 120:\n",
    "            extracted_benefit_items.append(element_text)\n",
    "    \n",
    "    unique_benefits = []\n",
    "    processed_benefits = set()\n",
    "    for benefit_item in extracted_benefit_items:\n",
    "        if benefit_item and benefit_item not in processed_benefits:\n",
    "            unique_benefits.append(benefit_item)\n",
    "            processed_benefits.add(benefit_item)\n",
    "    \n",
    "    standardized_benefits = standardize_product_benefits(unique_benefits)\n",
    "    return \"; \".join(standardized_benefits)\n",
    "\n",
    "def extract_product_ingredients(html_soup):\n",
    "    ingredient_section_labels = [\n",
    "        \"ingredientes\", \"composição\", \"composicao\", \"fórmula\", \"formula\", \"ingredients\", \"active ingredients\"\n",
    "    ]\n",
    "    \n",
    "    relevant_text_blocks = []\n",
    "    content_elements = html_soup.select(\"div, section, table, article, ul, ol, p\")\n",
    "    \n",
    "    for content_element in content_elements:\n",
    "        element_text = content_element.get_text(\" \", strip=True)\n",
    "        lowercase_text = element_text.lower()\n",
    "        \n",
    "        if any(label in lowercase_text for label in ingredient_section_labels):\n",
    "            relevant_text_blocks.append(element_text)\n",
    "    \n",
    "    sorted_text_blocks = sorted(set(relevant_text_blocks), key=len)\n",
    "    raw_ingredient_list = []\n",
    "    \n",
    "    for text_block in sorted_text_blocks:\n",
    "        raw_ingredient_list.extend(split_text_list(text_block))\n",
    "    \n",
    "    filtered_ingredients = [ingredient for ingredient in raw_ingredient_list if len(ingredient) <= 100]\n",
    "    return filter_ingredients(filtered_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22c2d5",
   "metadata": {},
   "source": [
    "## Preço e Tipo de pele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9d52aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_quantity_from_text(product_text: str) -> str:\n",
    "    if not product_text:\n",
    "        return \"\"\n",
    "    \n",
    "    quantity_pattern = re.search(r\"(\\d+[\\.,]?\\d*)\\s*(ml|g|l)\\b\", product_text.lower())\n",
    "    if quantity_pattern:\n",
    "        numeric_value = quantity_pattern.group(1).replace(\",\", \".\")\n",
    "        measurement_unit = quantity_pattern.group(2).upper()\n",
    "        \n",
    "        if measurement_unit == 'L' and not numeric_value.endswith('L'):\n",
    "            return f\"{numeric_value}L\"\n",
    "        return f\"{numeric_value}{measurement_unit}\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_compatible_skin_types(html_soup):\n",
    "    def normalize_skin_type_text(input_text: str) -> str:\n",
    "        if not input_text:\n",
    "            return \"\"\n",
    "        \n",
    "        accent_removed_text = \"\".join(\n",
    "            character for character in unicodedata.normalize(\"NFD\", input_text) \n",
    "            if unicodedata.category(character) != \"Mn\"\n",
    "        )\n",
    "        lowercase_text = accent_removed_text.lower()\n",
    "        hyphen_normalized_text = lowercase_text.replace(\"-\", \" \")\n",
    "        special_chars_removed = re.sub(r\"[^\\w\\s]\", \" \", hyphen_normalized_text)   \n",
    "        whitespace_normalized = re.sub(r\"\\s+\", \" \", special_chars_removed).strip()\n",
    "        \n",
    "        return whitespace_normalized\n",
    "\n",
    "    complete_page_text = html_soup.get_text(\" \", strip=True)\n",
    "    normalized_page_text = normalize_skin_type_text(complete_page_text)\n",
    "\n",
    "    normalized_skin_synonyms = {\n",
    "        canonical_type: [normalize_skin_type_text(synonym) for synonym in synonym_list if synonym]\n",
    "        for canonical_type, synonym_list in SKIN_TYPE_SYNONYMS_PT.items()\n",
    "    }\n",
    "\n",
    "    identified_skin_types = set()\n",
    "    \n",
    "    universal_skin_synonyms = normalized_skin_synonyms.get(\"todos os tipos\", [])\n",
    "    for universal_pattern in universal_skin_synonyms:\n",
    "        if universal_pattern and universal_pattern in normalized_page_text:\n",
    "            return \"todos os tipos\"\n",
    "    \n",
    "    for canonical_skin_type, pattern_list in normalized_skin_synonyms.items():\n",
    "        if canonical_skin_type == \"todos os tipos\":\n",
    "            continue\n",
    "        if any(pattern and pattern in normalized_page_text for pattern in pattern_list):\n",
    "            identified_skin_types.add(canonical_skin_type)\n",
    "\n",
    "    if not identified_skin_types:\n",
    "        return \"\"  \n",
    "\n",
    "    skin_type_order_mapping = {skin_type: index for index, skin_type in enumerate(SKIN_TYPE_CANONICAL_ORDER or [])}\n",
    "    ordered_skin_types = sorted(identified_skin_types, key=lambda skin_type: skin_type_order_mapping.get(skin_type, 10_000))\n",
    "\n",
    "    return \"; \".join(ordered_skin_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988ead5",
   "metadata": {},
   "source": [
    "## Imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2440f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(soup, product_name):\n",
    "    selectors = [\n",
    "        \"img.vtex-store-components-3-x-productImageTag\",\n",
    "        \"img.product-image\",\n",
    "        \"img[src*='/arquivos/']\",\n",
    "        \"img[src*='cdn']\",\n",
    "    ]\n",
    "    src = None\n",
    "    for sel in selectors:\n",
    "        node = soup.select_one(sel)\n",
    "        if node and node.get(\"src\"):\n",
    "            src = node.get(\"src\"); break\n",
    "        if node and node.get(\"data-src\"):\n",
    "            src = node.get(\"data-src\"); break\n",
    "    if not src:\n",
    "        return \"\"\n",
    "    img_url = src if src.startswith(\"http\") else urljoin(BASE_URL, src)\n",
    "    from urllib.parse import urlparse\n",
    "    ext = os.path.splitext(urlparse(img_url).path)[1] or \".jpg\"\n",
    "    fname = f\"{slugify(product_name)}{ext}\"\n",
    "    fpath = os.path.join(IMG_DIR, fname)\n",
    "    try:\n",
    "        r = SESSION.get(img_url, timeout=25)\n",
    "        if r.status_code == 200 and r.content:\n",
    "            with open(fpath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            return fname\n",
    "    except Exception as e:\n",
    "        print(f\"[IMG] Falha ao baixar {img_url}: {e}\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2301ed9",
   "metadata": {},
   "source": [
    "## Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdac5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CAT_ORDER_MAP = {c: i for i, c in enumerate(CATEGORY_CANONICAL_ORDER)}\n",
    "\n",
    "def normalize_category_text(s: str) -> str:\n",
    "    return strip_accents(s or \"\").lower()\n",
    "\n",
    "def classify_category_from_name(name: str, subtitle: str | None = None, desc: str | None = None) -> str:\n",
    " \n",
    "    txt = normalize_category_text(f\"{name or ''} {subtitle or ''} {desc or ''}\")\n",
    "    hits = []\n",
    "    for cat, needles in CATEGORY_HINTS.items():\n",
    "        for needle in needles:\n",
    "            if normalize_category_text(needle) in txt:\n",
    "                hits.append(cat)\n",
    "                break\n",
    "    if not hits:\n",
    "        return \"\"  \n",
    "    hits.sort(key=lambda c: _CAT_ORDER_MAP.get(c, 10_000))\n",
    "    return hits[0]\n",
    "\n",
    "def guess_category(url: str, name: str) -> str:\n",
    "    return classify_category_from_name(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c2f3d",
   "metadata": {},
   "source": [
    "## Produtos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d881e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5090aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product_page(product_url, fallback_category=\"\"):\n",
    "    html_soup = get_soup(product_url)\n",
    "    if html_soup is None:\n",
    "        return None\n",
    "    \n",
    "    product_name_selectors = [\n",
    "        \"h1.vtex-store-components-3-x-productName\",\n",
    "        \"h1.productName\",\n",
    "        \"h1\",\n",
    "        \"div.product-name h1\",\n",
    "    ]\n",
    "    product_name = find_text_by_selectors(html_soup, product_name_selectors)\n",
    "    \n",
    "    if not product_name:\n",
    "        if html_soup.title and html_soup.title.string:\n",
    "            product_name = html_soup.title.string.split(\"|\")[0].strip()\n",
    "    \n",
    "    if not product_name:\n",
    "        return None\n",
    "    \n",
    "    if should_exclude_product(product_name) or should_exclude_product(product_url):\n",
    "        print(f\"[SKIP] Produto excluído: {product_name}\")\n",
    "        return None\n",
    "\n",
    "    subtitle_selectors = [\n",
    "        \"span.vtex-product-summary-2-x-description-short div\",\n",
    "        \"span.vtex-product-summary-2-x-description-short\",\n",
    "        \"div.vtex-rich-text-0-x-container p\",\n",
    "        \"div.productDescription\",\n",
    "        \"div.product-brief\",\n",
    "    ]\n",
    "    product_subtitle = find_text_by_selectors(html_soup, subtitle_selectors)\n",
    "    validated_subtitle = product_subtitle if (product_subtitle and len(product_subtitle) <= 220) else \"\"\n",
    "    \n",
    "    price_selectors = [\n",
    "        \"p.priceCustom__sellingPrice span\",\n",
    "        \"span.vtex-product-price-1-x-sellingPriceValue\",\n",
    "        \"span.selling-price\",\n",
    "        \"span.price\",\n",
    "    ]\n",
    "    raw_price_text = find_text_by_selectors(html_soup, price_selectors)\n",
    "    formatted_price = parse_price(raw_price_text)\n",
    "    \n",
    "    extracted_benefits = extract_product_benefits(html_soup)\n",
    "    extracted_ingredients = extract_product_ingredients(html_soup)\n",
    "    compatible_skin_types = extract_compatible_skin_types(html_soup)\n",
    "    \n",
    "    product_quantity = extract_product_quantity_from_text(product_name)\n",
    "    if not product_quantity:\n",
    "        product_quantity = extract_product_quantity_from_text(validated_subtitle)\n",
    "    if not product_quantity:\n",
    "        details_selectors = [\n",
    "            \"div.vtex-store-components-3-x-productDescriptionText\",\n",
    "            \"div.productDescription\",\n",
    "            \"section#descricao\",\n",
    "        ]\n",
    "        details_text = find_text_by_selectors(html_soup, details_selectors)\n",
    "        product_quantity = extract_product_quantity_from_text(details_text)\n",
    "    \n",
    "    category_from_hints = classify_category_from_name(product_name, validated_subtitle)\n",
    "    final_category = category_from_hints or fallback_category or guess_category(product_url, product_name)\n",
    "    \n",
    "    downloaded_image_name = download_image(html_soup, product_name)\n",
    "    \n",
    "    structured_product_data = {\n",
    "        \"marca\": \"creamy\",\n",
    "        \"nome\": product_name.strip(),\n",
    "        \"subtitulo\": validated_subtitle if validated_subtitle else \"\",\n",
    "        \"categoria\": final_category,\n",
    "        \"preco\": formatted_price if formatted_price else \"\",\n",
    "        \"quantidade\": product_quantity if product_quantity else \"\",\n",
    "        \"beneficios\": extracted_benefits.lower() if extracted_benefits else \"\",\n",
    "        \"ingredientes\": extracted_ingredients.lower() if extracted_ingredients else \"\",\n",
    "        \"tipos_pele\": compatible_skin_types,\n",
    "        \"imagem\": downloaded_image_name,\n",
    "    }\n",
    "    \n",
    "    return structured_product_data\n",
    "\n",
    "def extract_product_links_from_listing(listing_page_url: str):\n",
    "    listing_soup = get_soup(listing_page_url)\n",
    "    if listing_soup is None:\n",
    "        return []\n",
    "    \n",
    "    discovered_product_links = set()\n",
    "    anchor_elements = listing_soup.select(\"a[href]\")\n",
    "    \n",
    "    for anchor_element in anchor_elements:\n",
    "        href_attribute = anchor_element.get(\"href\")\n",
    "        if not href_attribute:\n",
    "            continue\n",
    "        \n",
    "        absolute_url = href_attribute if href_attribute.startswith(\"http\") else urljoin(BASE_URL, href_attribute)\n",
    "        \n",
    "        if re.search(r\"/p($|\\?)\", absolute_url):\n",
    "            discovered_product_links.add(absolute_url)\n",
    "    \n",
    "    return sorted(discovered_product_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e41381",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6fbe59",
   "metadata": {},
   "source": [
    "### Saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e1d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "WEB SCRAPING Creamy\n",
      "__________________________________________________\n",
      "Processando página 1: https://www.creamy.com.br/produtos?page=1\n",
      "Encontrados 10 produtos na página\n",
      "Encontrados 10 produtos na página\n",
      "Produto extraído: Creme Retexturizador - Ácido Glicólico - R$ 84.20\n",
      "Produto extraído: Creme Retexturizador - Ácido Glicólico - R$ 84.20\n",
      "Produto extraído: Sérum Renovador Suave - Ácido Lático - R$ 73.67\n",
      "Produto extraído: Sérum Renovador Suave - Ácido Lático - R$ 73.67\n",
      "Produto extraído: Gel Clareador Antiacne - Ácido Mandélico - R$ 84.20\n",
      "Produto extraído: Gel Clareador Antiacne - Ácido Mandélico - R$ 84.20\n",
      "Produto extraído: Tônico Antioleosidade - Ácido Salicílico - R$ 85.25\n",
      "Produto extraído: Tônico Antioleosidade - Ácido Salicílico - R$ 85.25\n",
      "Produto extraído: Gel-creme Hidratante Calmante - Calming Cream - R$ 52.62\n",
      "Produto extraído: Gel-creme Hidratante Calmante - Calming Cream - R$ 52.62\n",
      "Produto extraído: Creme Clareador para Olhos - Eye Cream - R$ 123.15\n",
      "Produto extraído: Creme Clareador para Olhos - Eye Cream - R$ 123.15\n",
      "Produto extraído: Gel de Limpeza - R$ 63.15\n",
      "Produto extraído: Gel de Limpeza - R$ 63.15\n",
      "Produto extraído: Protetor  Solar FPS 60 - R$ 63.15\n",
      "Produto extraído: Protetor  Solar FPS 60 - R$ 63.15\n",
      "Produto extraído: Retinol - Creme Corretivo Anti sinais - R$ 94.73\n",
      "Produto extraído: Retinol - Creme Corretivo Anti sinais - R$ 94.73\n",
      "Produto extraído: Sérum Antioxidante - Vitamina C - R$ 105.25\n",
      "Produto extraído: Sérum Antioxidante - Vitamina C - R$ 105.25\n",
      "Processando página 2: https://www.creamy.com.br/produtos?page=2\n",
      "Processando página 2: https://www.creamy.com.br/produtos?page=2\n",
      "Encontrados 11 produtos na página\n",
      "Encontrados 11 produtos na página\n",
      "Produto extraído: Tônico Clareador - Ácido Tranexâmico - R$ 94.73\n",
      "Produto extraído: Tônico Clareador - Ácido Tranexâmico - R$ 94.73\n",
      "Produto extraído: Sérum Hidratante com Ácido Hialurônico - R$ 63.15\n",
      "Produto extraído: Sérum Hidratante com Ácido Hialurônico - R$ 63.15\n",
      "Produto extraído: Creme de Tratamento Labial - Lip Balm Incolor - R$ 31.57\n",
      "Produto extraído: Creme de Tratamento Labial - Lip Balm Incolor - R$ 31.57\n",
      "Processando página 3: https://www.creamy.com.br/produtos?page=3\n",
      "Processando página 3: https://www.creamy.com.br/produtos?page=3\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 4: https://www.creamy.com.br/produtos?page=4\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 4: https://www.creamy.com.br/produtos?page=4\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 5: https://www.creamy.com.br/produtos?page=5\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 5: https://www.creamy.com.br/produtos?page=5\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 6: https://www.creamy.com.br/produtos?page=6\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 6: https://www.creamy.com.br/produtos?page=6\n",
      "Encontrados 11 produtos na página\n",
      "Encontrados 11 produtos na página\n",
      "Produto extraído: Sérum Antiacne Adapalenato - R$ 126.31\n",
      "Produto extraído: Sérum Antiacne Adapalenato - R$ 126.31\n",
      "Processando página 7: https://www.creamy.com.br/produtos?page=7\n",
      "Processando página 7: https://www.creamy.com.br/produtos?page=7\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 8: https://www.creamy.com.br/produtos?page=8\n",
      "Encontrados 11 produtos na página\n",
      "Processando página 8: https://www.creamy.com.br/produtos?page=8\n",
      "Encontrados 11 produtos na página\n",
      "Encontrados 11 produtos na página\n",
      "Produto extraído: Sérum de Prevenção Anti idade - R$ 73.67\n",
      "Produto extraído: Sérum de Prevenção Anti idade - R$ 73.67\n",
      "Produto extraído: Gel-creme Retexturizador Antioleosidade - Glicointense Peel - R$ 105.25\n",
      "Produto extraído: Gel-creme Retexturizador Antioleosidade - Glicointense Peel - R$ 105.25\n",
      "Produto extraído: Creme Hidratante Antivermelhidão - Intensive Cream - R$ 105.25\n",
      "Produto extraído: Creme Hidratante Antivermelhidão - Intensive Cream - R$ 105.25\n",
      "Processando página 9: https://www.creamy.com.br/produtos?page=9\n",
      "Processando página 9: https://www.creamy.com.br/produtos?page=9\n",
      "Encontrados 11 produtos na página\n",
      "Encontrados 11 produtos na página\n",
      "Produto extraído: Creme Hidratante Labial Mocha - R$ 36.83\n",
      "Produto extraído: Creme Hidratante Labial Mocha - R$ 36.83\n",
      "__________________________________________________\n",
      "Fim da execução! Total coletado: 18 produtos\n",
      "JSON gerado: creamy_products.json (18 produtos)\n",
      "__________________________________________________\n",
      "Fim da execução! Total coletado: 18 produtos\n",
      "JSON gerado: creamy_products.json (18 produtos)\n"
     ]
    }
   ],
   "source": [
    "def execute_creamy_scraper():\n",
    "    visited_product_urls = set()\n",
    "    scraped_products_list = []\n",
    "    \n",
    "    for current_page in range(1, MAX_PAGES + 1):\n",
    "        page_listing_url = LISTING_URL_TEMPLATE.format(page=current_page)\n",
    "        print(f\"Processando página {current_page}: {page_listing_url}\")\n",
    "        \n",
    "        discovered_product_links = extract_product_links_from_listing(page_listing_url)\n",
    "        print(f\"Encontrados {len(discovered_product_links)} produtos na página\")\n",
    "        \n",
    "        for product_url in discovered_product_links:\n",
    "            if product_url in visited_product_urls or should_exclude_product(product_url):\n",
    "                continue\n",
    "                \n",
    "            extracted_product_data = parse_product_page(product_url)\n",
    "            if extracted_product_data:\n",
    "                visited_product_urls.add(product_url)\n",
    "                scraped_products_list.append(extracted_product_data)\n",
    "                print(f\"Produto extraído: {extracted_product_data['nome']} - R$ {extracted_product_data['preco']}\")\n",
    "            \n",
    "            time.sleep(0.6)\n",
    "    \n",
    "    return scraped_products_list\n",
    "\n",
    "def save_data(products_data):\n",
    "    output_columns = [\"marca\", \"nome\", \"subtitulo\", \"categoria\", \"quantidade\", \"preco\", \"beneficios\", \"ingredientes\", \"tipo_pele\", \"imagem\"]\n",
    "    cleaned_products_data = [{column: product.get(column, \"\") for column in output_columns} for product in products_data]\n",
    "    \n",
    "    with open(\"creamy_products.json\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(cleaned_products_data, output_file, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"JSON gerado: creamy_products.json ({len(cleaned_products_data)} produtos)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"_\" * 50)\n",
    "    print(\"WEB SCRAPING Creamy\")\n",
    "    print(\"_\" * 50)\n",
    "    \n",
    "    extracted_products = execute_creamy_scraper()\n",
    "\n",
    "    print(\"_\" * 50)\n",
    "    print(f\"Fim da execução! Total coletado: {len(extracted_products)} produtos\")\n",
    "    \n",
    "    save_data(extracted_products)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8c5a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6efc47c",
   "metadata": {},
   "source": [
    "###  CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78517b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV gerado: creamy_products.csv (20 linhas)\n",
      "A partir do JSON: creamy_products.json\n"
     ]
    }
   ],
   "source": [
    "def json_to_csv(json_file=\"creamy_products.json\", csv_file=\"creamy_products.csv\"):\n",
    "\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Nenhum dado encontrado no arquivo {json_file}\")\n",
    "            return\n",
    "        \n",
    "        cols = [\"marca\", \"nome\", \"subtitulo\", \"categoria\", \"quantidade\", \"preco\", \"beneficios\", \"ingredientes\", \"tipo_pele\", \"imagem\"]\n",
    "        \n",
    "        with open(csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=cols)\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "\n",
    "                csv_row = {k: (row.get(k) or \"\") for k in cols}\n",
    "                writer.writerow(csv_row)\n",
    "        \n",
    "        print(f\"CSV gerado: {csv_file} ({len(data)} linhas)\")\n",
    "        print(f\"A partir do JSON: {json_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Arquivo {json_file} não encontrado!\")\n",
    "        \n",
    "        import glob\n",
    "        json_files = glob.glob(\"*.json\")\n",
    "        if json_files:\n",
    "            for f in json_files:\n",
    "                print(f\"   - {f}\")\n",
    "        else:\n",
    "            print(\"   Nenhum arquivo .json encontrado\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter JSON para CSV: {e}\")\n",
    "\n",
    "\n",
    "json_to_csv(\"creamy_products.json\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
