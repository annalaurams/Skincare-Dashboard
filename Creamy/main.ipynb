{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66024b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LIST] https://www.creamy.com.br/produtos?page=1\n",
      "  - 10 links\n",
      "  [+] Ácido Glicólico :: 84.20\n",
      "  [+] Ácido Lático :: 73.67\n",
      "  [+] Ácido Mandélico :: 84.20\n",
      "  [+] Ácido Salicílico :: 94.73\n",
      "  [+] Calming Cream :: 52.62\n",
      "  [+] Gel de Limpeza :: 63.15\n",
      "  [+] Protetor  Solar FPS 60 Watery Lotion :: 63.15\n",
      "  [+] Sérum Facial Retinal :: 136.83\n",
      "  [+] Vitamina C Gold :: 126.31\n",
      "  [+] Vitamina C :: 105.25\n",
      "[LIST] https://www.creamy.com.br/produtos?page=2\n",
      "  - 11 links\n",
      "  [+] Sérum Hidratante Facial :: 63.15\n",
      "  [+] Lip Balm Incolor :: 31.49\n",
      "  [+] Niacinamide B Complex 20% - 30ml :: 94.73\n",
      "[LIST] https://www.creamy.com.br/produtos?page=3\n",
      "  - 11 links\n",
      "  [+] Retinol :: 105.25\n",
      "[LIST] https://www.creamy.com.br/produtos?page=4\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=5\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=6\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=7\n",
      "  - 11 links\n",
      "[LIST] https://www.creamy.com.br/produtos?page=8\n",
      "  - 11 links\n",
      "  [+] Glicointense Peel :: 105.25\n",
      "  [+] Intensive Cream :: 105.25\n",
      "  [+] Limpador Facial Glicerinado :: 63.15\n",
      "  [+] PEPTIDE LIP BALM TREATMENT LATTE 10G :: 36.83\n",
      "  [+] Refil Ácido Glicólico 30g :: 71.57\n",
      "  [+] Refil Ácido Mandélico 30g :: 71.57\n",
      "[LIST] https://www.creamy.com.br/produtos?page=9\n",
      "  - 11 links\n",
      "  [+] FRAGRÂNCIA COSMIC LOVE 75ML :: 157.88\n",
      "  [+] Refil Gel Creme Retinol 30g :: 89.46\n",
      "  [+] Refil Sérum Facil Ácido Lático 30ml :: 63.15\n",
      "Total coletado: 23\n",
      "[OK] JSON salvo em creamy_products.json (23 itens)\n",
      "[OK] CSV salvo em creamy_products.csv (23 linhas)\n",
      "Prévia do primeiro item:\n",
      "{\n",
      "  \"site\": \"creamy\",\n",
      "  \"categoria\": \"p\",\n",
      "  \"nome\": \"Ácido Glicólico\",\n",
      "  \"subtitulo\": \"Reduz poros e melhora a textura da pele\",\n",
      "  \"preco\": \"84.20\",\n",
      "  \"beneficios\": \"hidratação; controle da oleosidade; antissinais; minimiza poros\",\n",
      "  \"ingredientes\": \"ácido glicólico; lha; niacinamida; benzoato de sódio; álcool cetílico; hidróxido de amônio; bisabolol; goma xantana; fenoxietanol; glycolic acid; stearic acid; glycerin; polysorbate; cyclopentasiloxane; phenoxyethanol; ethylhexylglycerin; disodium edta; vitamina c\",\n",
      "  \"tamanho\": \"\",\n",
      "  \"tipos_pele\": \"oleosa; seca; mista; normal; acneica; madura; sensivel\",\n",
      "  \"imagem\": \"acido-glicolico.jpg\",\n",
      "  \"_source_url\": \"https://www.creamy.com.br/acido-glicolico/p\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Creamy — Web Scraping (com padronização de benefícios via models)\n",
    "\n",
    "import sys, subprocess, os, re, csv, json, time\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "\n",
    "from exclude import (\n",
    "    EXCLUDE_KEYWORDS,\n",
    ")\n",
    "\n",
    "from ingredient import (\n",
    "    INGREDIENTES_VALIDOS,\n",
    ")\n",
    "\n",
    "from benefits import (\n",
    "    BENEFIT_SYNONYMS_PT,\n",
    "    BENEFIT_CANONICAL_ORDER,\n",
    ")\n",
    "\n",
    "BASE_URL = \"https://www.creamy.com.br/\"\n",
    "LISTING_URL_TEMPLATE = \"https://www.creamy.com.br/produtos?page={page}\"\n",
    "MAX_PAGES = 9\n",
    "\n",
    "\n",
    "OUT_JSON = \"creamy_products.json\"\n",
    "OUT_CSV  = \"creamy_products.csv\"\n",
    "IMG_DIR  = \"imagens\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "})\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def _strip_accents_lower(s: str) -> str:\n",
    "    return strip_accents(s or \"\").lower().strip()\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = strip_accents(text.lower())\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
    "    text = re.sub(r\"-+\", \"-\", text).strip(\"-\")\n",
    "    return text or \"produto\"\n",
    "\n",
    "def get_soup(url, max_retries=3, timeout=25):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                return BeautifulSoup(r.text, \"lxml\")\n",
    "            print(f\"[WARN] {url} -> status {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {url} -> {e}\")\n",
    "        time.sleep(1.1 * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def looks_excluded(text: str) -> bool:\n",
    "    t = strip_accents((text or \"\").lower())\n",
    "    return any(kw in t for kw in EXCLUDE_KEYWORDS)\n",
    "\n",
    "def parse_price_to_str(price_text: str) -> str:\n",
    "    if not price_text:\n",
    "        return \"\"\n",
    "    t = price_text.replace(\"R$\", \"\").replace(\"r$\", \"\").strip()\n",
    "    t = t.replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \".\")\n",
    "    m = re.findall(r\"[0-9]+(?:\\.[0-9]{1,2})?\", t)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return f\"{float(m[0]):.2f}\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def split_list_candidates(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    t = text.replace(\"<br>\", \";\").replace(\"<br/>\", \";\").replace(\"<br />\", \";\")\n",
    "    parts = re.split(r\"[;•|/\\n,]\", t)\n",
    "    return [normalize_space(p) for p in parts if normalize_space(p)]\n",
    "\n",
    "def filter_ingredients_creamy(ings_raw):\n",
    "    allowed_norm = [a.lower() for a in INGREDIENTES_VALIDOS]\n",
    "    allowed_noacc = [strip_accents(a.lower()) for a in INGREDIENTES_VALIDOS]\n",
    "    out = []\n",
    "    \n",
    "    for ing in ings_raw:\n",
    "        ing_l = ing.lower()\n",
    "        ing_noacc = strip_accents(ing_l)\n",
    "        match = None\n",
    "        for a_norm, a_noacc in zip(allowed_norm, allowed_noacc):\n",
    "            if a_norm in ing_l or a_noacc in ing_noacc:\n",
    "                match = a_norm\n",
    "                break\n",
    "        if match and match not in out:\n",
    "            out.append(match)\n",
    "    return \"; \".join(out)\n",
    "\n",
    "def find_all_text(soup, selectors):\n",
    "    for sel in selectors:\n",
    "        node = soup.select_one(sel)\n",
    "        if node and node.get_text(strip=True):\n",
    "            return node.get_text(\" \", strip=True)\n",
    "    return \"\"\n",
    "\n",
    "# ==== NOVO: padronização de benefícios ====\n",
    "def padroniza_beneficios(textos_beneficios):\n",
    "    if not textos_beneficios:\n",
    "        return []\n",
    "    encontrados = set()\n",
    "    norm_syn = {\n",
    "        canonico: [_strip_accents_lower(s) for s in patt_list if s]\n",
    "        for canonico, patt_list in BENEFIT_SYNONYMS_PT.items()\n",
    "    }\n",
    "    for txt in textos_beneficios:\n",
    "        n = _strip_accents_lower(txt)\n",
    "        for canonico, padds in norm_syn.items():\n",
    "            if any(patt in n for patt in padds):\n",
    "                encontrados.add(canonico)\n",
    "\n",
    "    if BENEFIT_CANONICAL_ORDER:\n",
    "        order_map = {name: i for i, name in enumerate(BENEFIT_CANONICAL_ORDER)}\n",
    "        return sorted(list(encontrados), key=lambda x: order_map.get(x, 999))\n",
    "    return sorted(list(encontrados))\n",
    "\n",
    "def extract_benefits(soup):\n",
    "    # Coleta bruta\n",
    "    items = []\n",
    "    for ul in soup.select(\"ul, ol\"):\n",
    "        lis = [normalize_space(li.get_text(\" \", strip=True)) for li in ul.select(\"li\")]\n",
    "        for li in lis:\n",
    "            if 0 < len(li) <= 120:\n",
    "                items.append(li)\n",
    "    for th in soup.select(\"th, td\"):\n",
    "        txt = normalize_space(th.get_text(\" \", strip=True))\n",
    "        if 0 < len(txt) <= 120:\n",
    "            items.append(txt)\n",
    "    # Únicos mantendo ordem de primeira aparição\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for it in items:\n",
    "        if it and it not in seen:\n",
    "            uniq.append(it)\n",
    "            seen.add(it)\n",
    "    # === NOVO: padroniza com base no models ===\n",
    "    pad = padroniza_beneficios(uniq)\n",
    "    return \"; \".join(pad)\n",
    "\n",
    "def extract_ingredients(soup):\n",
    "    possible_labels = [\n",
    "        \"ingredientes\", \"composição\", \"composicao\", \"fórmula\", \"formula\", \"ingredients\", \"active ingredients\"\n",
    "    ]\n",
    "    text_blocks = []\n",
    "\n",
    "    for el in soup.select(\"div, section, table, article, ul, ol, p\"):\n",
    "        txt = el.get_text(\" \", strip=True)\n",
    "        low = txt.lower()\n",
    "        if any(lbl in low for lbl in possible_labels):\n",
    "            text_blocks.append(txt)\n",
    "    text_blocks = sorted(set(text_blocks), key=len)\n",
    "    raw = []\n",
    "    for block in text_blocks:\n",
    "        raw.extend(split_list_candidates(block))\n",
    "    raw = [r for r in raw if len(r) <= 100]\n",
    "    return filter_ingredients_creamy(raw)\n",
    "\n",
    "def extract_size_from_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = re.search(r\"(\\d+[\\.,]?\\d*)\\s*(ml|g|l)\\b\", text.lower())\n",
    "    if m:\n",
    "        val = m.group(1).replace(\",\", \".\")\n",
    "        unit = m.group(2).upper()\n",
    "        if unit == 'L' and not val.endswith('L'):\n",
    "            return f\"{val}L\"\n",
    "        return f\"{val}{unit}\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_tipos_pele(soup):\n",
    "    \"\"\"Extrai os tipos de pele mencionados no produto e ordena canonicamente, se disponível.\"\"\"\n",
    "    tipos_encontrados = set()\n",
    "    tipos_mapeamento = {\n",
    "        \"oleosa\": \"oleosa\",\n",
    "        \"seca\": \"seca\", \n",
    "        \"mista\": \"mista\",\n",
    "        \"sensivel\": \"sensivel\",\n",
    "        \"sensível\": \"sensivel\",\n",
    "        \"normal\": \"normal\",\n",
    "        \"acneica\": \"acneica\",\n",
    "        \"madura\": \"madura\"\n",
    "    }\n",
    "    page_text = soup.get_text(\" \", strip=True).lower()\n",
    "    for palavra, tipo in tipos_mapeamento.items():\n",
    "        if palavra in page_text:\n",
    "            tipos_encontrados.add(tipo)\n",
    "\n",
    "    if not tipos_encontrados:\n",
    "        tipos_encontrados = {\"mista\", \"oleosa\", \"seca\", \"sensivel\"}\n",
    "\n",
    "    tipos_list = list(tipos_encontrados)\n",
    "    if SKIN_TYPE_CANONICAL_ORDER:\n",
    "        order_map = {name: i for i, name in enumerate(SKIN_TYPE_CANONICAL_ORDER)}\n",
    "        tipos_list = sorted(tipos_list, key=lambda x: order_map.get(x, 999))\n",
    "    else:\n",
    "        tipos_list = sorted(tipos_list)\n",
    "\n",
    "    return \"; \".join(tipos_list)\n",
    "\n",
    "def download_image(soup, product_name):\n",
    "    selectors = [\n",
    "        \"img.vtex-store-components-3-x-productImageTag\",\n",
    "        \"img.product-image\",\n",
    "        \"img[src*='/arquivos/']\",\n",
    "        \"img[src*='cdn']\",\n",
    "    ]\n",
    "    src = None\n",
    "\n",
    "    for sel in selectors:\n",
    "        node = soup.select_one(sel)\n",
    "        if node and node.get(\"src\"):\n",
    "            src = node.get(\"src\")\n",
    "            break\n",
    "        if node and node.get(\"data-src\"):\n",
    "            src = node.get(\"data-src\")\n",
    "            break\n",
    "    if not src:\n",
    "        return \"\"\n",
    "    \n",
    "    img_url = src if src.startswith(\"http\") else urljoin(BASE_URL, src)\n",
    "    from urllib.parse import urlparse\n",
    "    ext = os.path.splitext(urlparse(img_url).path)[1] or \".jpg\"\n",
    "    fname = f\"{slugify(product_name)}{ext}\"\n",
    "    fpath = os.path.join(IMG_DIR, fname)\n",
    "\n",
    "    try:\n",
    "        r = SESSION.get(img_url, timeout=25)\n",
    "        if r.status_code == 200 and r.content:\n",
    "            with open(fpath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            return fname\n",
    "    except Exception as e:\n",
    "        print(f\"[IMG] Falha ao baixar {img_url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "def guess_category(url: str, name: str) -> str:\n",
    "    nlow = name.lower()\n",
    "    mapping = [\n",
    "        (\"protetor\", \"protetor solar\"), (\"fps\", \"protetor solar\"),\n",
    "        (\"bastão\", \"bastão\"), (\"bastao\", \"bastão\"),\n",
    "        (\"sérum\", \"sérum\"), (\"serum\", \"sérum\"),\n",
    "        (\"creme\", \"creme\"), (\"hidratante\", \"hidratante\"),\n",
    "        (\"gel de limpeza\", \"gel de limpeza\"), (\"sabonete\", \"sabonete\"),\n",
    "        (\"tônico\", \"tônico\"), (\"tonico\", \"tônico\"),\n",
    "        (\"óleo\", \"óleo\"), (\"oleo\", \"óleo\"),\n",
    "        (\"emulsão\", \"emulsão\"), (\"emulsao\", \"emulsão\"),\n",
    "        (\"lip balm\", \"lip balm\"), (\"bálsamo\", \"bálsamo\"),\n",
    "        (\"peel\", \"peel\"), (\"esfoliante\", \"esfoliante\"),\n",
    "        (\"limpador\", \"limpador\"), (\"fragrância\", \"fragrância\"),\n",
    "        (\"body\", \"creme corporal\"), (\"corporal\", \"creme corporal\"),\n",
    "        (\"capilar\", \"capilar\")\n",
    "    ]\n",
    "\n",
    "    for key, val in mapping:\n",
    "        if key in nlow:\n",
    "            return val\n",
    "    \n",
    "    path = urlparse(url).path.strip(\"/\")\n",
    "    base = path.split(\"/\")[-1]\n",
    "    tokens = base.replace(\".p\", \"\").replace(\".html\", \"\").split(\"-\")\n",
    "    cat = \" \".join(tokens[:2]).strip()\n",
    "    return normalize_space(cat) if cat else \"\"\n",
    "\n",
    "def parse_product_page(url, fallback_category=\"\"):\n",
    "    soup = get_soup(url)\n",
    "    if soup is None:\n",
    "        return None\n",
    "    \n",
    "    name = find_all_text(soup, [\n",
    "        \"h1.vtex-store-components-3-x-productName\",\n",
    "        \"h1.productName\",\n",
    "        \"h1\",\n",
    "        \"div.product-name h1\",\n",
    "    ])\n",
    "    \n",
    "    if not name:\n",
    "        if soup.title and soup.title.string:\n",
    "            name = soup.title.string.split(\"|\")[0].strip()\n",
    "    \n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    if looks_excluded(name) or looks_excluded(url):\n",
    "        print(f\"[SKIP] Produto excluído: {name}\")\n",
    "        return None\n",
    "\n",
    "    subtitle = find_all_text(soup, [\n",
    "        \"span.vtex-product-summary-2-x-description-short div\",\n",
    "        \"span.vtex-product-summary-2-x-description-short\",\n",
    "        \"div.vtex-rich-text-0-x-container p\",\n",
    "        \"div.productDescription\",\n",
    "        \"div.product-brief\",\n",
    "    ])\n",
    "    subtitle = subtitle if (subtitle and len(subtitle) <= 220) else \"\"\n",
    "    \n",
    "    price_text = find_all_text(soup, [\n",
    "        \"span.vtex-product-price-1-x-sellingPriceValue\",\n",
    "        \"span.selling-price\",\n",
    "        \"span.price\",\n",
    "    ])\n",
    "    price = parse_price_to_str(price_text)\n",
    "    \n",
    "    beneficios = extract_benefits(soup)\n",
    "    ingredientes = extract_ingredients(soup)\n",
    "    tipos_pele = extract_tipos_pele(soup)\n",
    "    \n",
    "    size = extract_size_from_text(name)\n",
    "    if not size:\n",
    "        size = extract_size_from_text(subtitle)\n",
    "    if not size:\n",
    "        details_txt = find_all_text(soup, [\n",
    "            \"div.vtex-store-components-3-x-productDescriptionText\",\n",
    "            \"div.productDescription\",\n",
    "            \"section#descricao\",\n",
    "        ])\n",
    "        size = extract_size_from_text(details_txt)\n",
    "    \n",
    "    categoria = fallback_category or guess_category(url, name)\n",
    "    img_name = download_image(soup, name)\n",
    "    \n",
    "    return {\n",
    "        \"site\": \"creamy\",\n",
    "        \"categoria\": categoria,\n",
    "        \"nome\": name.strip(),\n",
    "        \"subtitulo\": subtitle if subtitle else \"\",\n",
    "        \"preco\": price if price else \"\",\n",
    "        \"beneficios\": beneficios.lower() if beneficios else \"\",\n",
    "        \"ingredientes\": ingredientes.lower() if ingredientes else \"\",\n",
    "        \"tamanho\": size if size else \"\",\n",
    "        \"tipos_pele\": tipos_pele,\n",
    "        \"imagem\": img_name,\n",
    "        \"_source_url\": url,\n",
    "    }\n",
    "\n",
    "def listing_get_product_links(page_url: str):\n",
    "    soup = get_soup(page_url)\n",
    "    if soup is None:\n",
    "        return []\n",
    "    links = set()\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        full = href if href.startswith(\"http\") else urljoin(BASE_URL, href)\n",
    "        if re.search(r\"/p($|\\?)\", full):\n",
    "            links.add(full)\n",
    "    return sorted(links)\n",
    "\n",
    "def run_scraper():\n",
    "    visited = set()\n",
    "    items = []\n",
    "\n",
    "    for page in range(1, MAX_PAGES+1):\n",
    "        url = LISTING_URL_TEMPLATE.format(page=page)\n",
    "        print(f\"[LIST] {url}\")\n",
    "        prod_links = listing_get_product_links(url)\n",
    "        print(f\"  - {len(prod_links)} links\")\n",
    "        \n",
    "        for purl in prod_links:\n",
    "            if purl in visited or looks_excluded(purl):\n",
    "                continue\n",
    "            item = parse_product_page(purl)\n",
    "            if item:\n",
    "                visited.add(purl)\n",
    "                items.append(item)\n",
    "                print(f\"  [+] {item['nome']} :: {item['preco']}\")\n",
    "            time.sleep(0.6)\n",
    "    return items\n",
    "\n",
    "def save_outputs(items):\n",
    "    cols = [\"site\", \"categoria\", \"nome\", \"subtitulo\", \"preco\", \"beneficios\", \"ingredientes\", \"tamanho\", \"tipos_pele\", \"imagem\"]\n",
    "    clean = [{k: it.get(k, \"\") for k in cols} for it in items]\n",
    "    \n",
    "    with open(\"creamy_products.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clean, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[OK] JSON salvo em creamy_products.json ({len(clean)} itens)\")\n",
    "\n",
    "    with open(\"creamy_products.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writeheader()\n",
    "        for row in clean:\n",
    "            w.writerow(row)\n",
    "    print(f\"[OK] CSV salvo em creamy_products.csv ({len(clean)} linhas)\")\n",
    "\n",
    "# Executar o scraper\n",
    "if __name__ == \"__main__\":\n",
    "    items = run_scraper()\n",
    "    print(f\"Total coletado: {len(items)}\")\n",
    "    save_outputs(items)\n",
    "\n",
    "    if items:\n",
    "        print(\"Prévia do primeiro item:\")\n",
    "        print(json.dumps(items[0], ensure_ascii=False, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
