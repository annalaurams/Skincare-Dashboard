{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bcebce",
   "metadata": {},
   "source": [
    "# Ollie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df346d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.12/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, json, time, random, logging, unicodedata, sys\n",
    "from urllib.parse import urljoin, urlencode, urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/usuario/Área de trabalho/Dados/models\"))\n",
    "\n",
    "from skin import (\n",
    "    SKIN_TYPE_CANONICAL_ORDER,\n",
    "    SKIN_TYPE_SYNONYMS_PT,\n",
    ")\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "from category import CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9650f5",
   "metadata": {},
   "source": [
    "## Informações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cad34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"ollie\")\n",
    "\n",
    "BASE_URL = \"https://meuollie.com.br\"\n",
    "COLLECTION_PATH = \"/collections/loja-produtos-ollie\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065f909",
   "metadata": {},
   "source": [
    "## Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc8290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_accents_lower(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def norm_space(t):\n",
    "    if not t: return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def norm_text_for_match(t: str) -> str:\n",
    "    t = t or \"\"\n",
    "    t = strip_accents(t.lower())\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def format_brl_price(num_str: str) -> str:\n",
    "    if not num_str:\n",
    "        return \"\"\n",
    "    try:\n",
    "        v = float(num_str.replace(\",\", \".\"))\n",
    "        return f\"{v:.2f}\"\n",
    "    except Exception:\n",
    "        return num_str\n",
    "\n",
    "def norm_price(t):\n",
    "    if not t: return \"\"\n",
    "    t = t.replace(\"R$\", \"\").strip().replace(\".\",\"\").replace(\",\",\".\")\n",
    "    m = re.findall(r\"[0-9.]+\", t)\n",
    "    return m[0] if m else \"\"\n",
    "\n",
    "def slugify(text):\n",
    "    t = text.lower().strip()\n",
    "    t = re.sub(r\"[^a-z0-9\\-\\_\\sáàâãäéèêëíìîïóòôõöúùûüç]\", \"\", t)\n",
    "    t = t.replace(\" \", \"-\")\n",
    "    for a,b in ((\"á\",\"a\"),(\"à\",\"a\"),(\"â\",\"a\"),(\"ã\",\"a\"),(\"ä\",\"a\"),\n",
    "                (\"é\",\"e\"),(\"è\",\"e\"),(\"ê\",\"e\"),(\"ë\",\"e\"),\n",
    "                (\"í\",\"i\"),(\"ì\",\"i\"),(\"î\",\"i\"),(\"ï\",\"i\"),\n",
    "                (\"ó\",\"o\"),(\"ò\",\"o\"),(\"ô\",\"o\"),(\"õ\",\"o\"),(\"ö\",\"o\"),\n",
    "                (\"ú\",\"u\"),(\"ù\",\"u\"),(\"û\",\"u\"),(\"ü\",\"u\"),\n",
    "                (\"ç\",\"c\")):\n",
    "        t = t.replace(a,b)\n",
    "    return re.sub(r\"-+\", \"-\", t).strip(\"-\")\n",
    "\n",
    "def get_image_filename(url):\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    parsed = urlparse(url)\n",
    "    filename = os.path.basename(parsed.path)\n",
    "    return filename.lower() if filename else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38e140",
   "metadata": {},
   "source": [
    "## Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78963379",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CATEGORY_ORDER_MAP = {name: i for i, name in enumerate(CATEGORY_CANONICAL_ORDER)}\n",
    "\n",
    "def classify_category_from_name(name: str, description: str | None = None) -> str | None:\n",
    "    txt = _strip_accents_lower(f\"{name or ''} {description or ''}\")\n",
    "    hits = []\n",
    "    for cat, needles in CATEGORY_HINTS.items():\n",
    "        for n in needles:\n",
    "            if _strip_accents_lower(n) in txt:\n",
    "                hits.append(cat)\n",
    "                break\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits.sort(key=lambda c: _CATEGORY_ORDER_MAP.get(c, 10_000))\n",
    "    return hits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71292cd0",
   "metadata": {},
   "source": [
    "## Sessões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1d3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_session(max_retries=3, backoff=0.5, timeout=20):\n",
    "    \n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=max_retries, read=max_retries, connect=max_retries,\n",
    "        backoff_factor=backoff, status_forcelist=[429,500,502,503,504],\n",
    "        allowed_methods=frozenset([\"GET\"]), raise_on_status=False\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.headers.update(HEADERS)\n",
    "    s.timeout = timeout\n",
    "    return s\n",
    "\n",
    "def fetch_html(session, url, delay_range=(0.6,1.1)):\n",
    "\n",
    "    time.sleep(random.uniform(*delay_range))\n",
    "    r = session.get(url, timeout=session.timeout)\n",
    "    if r.status_code >= 400:\n",
    "        logger.warning(\"HTTP %s em %s\", r.status_code, url)\n",
    "    return r.text, r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8d65f",
   "metadata": {},
   "source": [
    "## Paginação e Listagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ce307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_listing_url(page=1, phcursor=None):\n",
    "    base = urljoin(BASE_URL, COLLECTION_PATH)\n",
    "    params = []\n",
    "    if page and page > 1:\n",
    "        params.append((\"page\", str(page)))\n",
    "    if phcursor:\n",
    "        params.append((\"phcursor\", phcursor))\n",
    "    return base + (\"?\" + urlencode(params) if params else \"\")\n",
    "\n",
    "def parse_listing(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    product_urls = []\n",
    "    for a in soup.select(\"a[href*='/products/']\"):\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        if \"/products/\" in href:\n",
    "            product_urls.append(urljoin(BASE_URL, href))\n",
    "\n",
    "    next_url = None\n",
    "    a_next = soup.find(\"a\", attrs={\"title\": lambda x: x and \"Avançar\" in x})\n",
    "    if a_next and a_next.get(\"href\"):\n",
    "        next_url = urljoin(BASE_URL, a_next[\"href\"])\n",
    "    if not next_url:\n",
    "        for a in soup.select(\"a[href*='?page=']\"):\n",
    "            next_url = urljoin(BASE_URL, a.get(\"href\")); break\n",
    "\n",
    "    seen, uniq = set(), []\n",
    "    for u in product_urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u); uniq.append(u)\n",
    "    return uniq, next_url\n",
    "\n",
    "def scrape_listing(session, page_cap=20):\n",
    "    all_urls, seen, page = [], set(), 1\n",
    "    next_url = build_listing_url(page=1)\n",
    "    prev_set = set()\n",
    "    while next_url and page <= page_cap:\n",
    "        html, _ = fetch_html(session, next_url)\n",
    "        product_urls, hinted_next = parse_listing(html)\n",
    "        logging.info(\"Página %d | %d produtos\", page, len(product_urls))\n",
    "        if not product_urls:\n",
    "            logging.info(\"Sem produtos. Encerrando.\")\n",
    "            break\n",
    "        cur_set = set(product_urls)\n",
    "        if cur_set == prev_set:\n",
    "            logging.info(\"Página repetida. Encerrando.\")\n",
    "            break\n",
    "        prev_set = cur_set\n",
    "        added = 0\n",
    "        for u in product_urls:\n",
    "            if u not in seen:\n",
    "                seen.add(u); all_urls.append(u); added += 1\n",
    "        if added == 0:\n",
    "            logging.info(\"Nenhum novo produto. Encerrando.\")\n",
    "            break\n",
    "        page += 1\n",
    "        next_url = hinted_next or build_listing_url(page=page)\n",
    "    return all_urls\n",
    "\n",
    "def _collect_section_text(soup, anchors=(\"PRINCIPAIS BENEFÍCIOS\",\"BENEFÍCIOS\",\"BENEFICIOS\",\"RESULTADOS\",\"POR QUE AMAR\")):\n",
    "    chunks = []\n",
    "    big_text = norm_space(soup.get_text(\" \"))\n",
    "    for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "        title = norm_space(b.get_text()).upper()\n",
    "        if any(a in title for a in anchors):\n",
    "            cur = b.parent if b.parent else b\n",
    "            hops, acc = 0, []\n",
    "            while cur and hops < 12:\n",
    "                cur = cur.find_next_sibling()\n",
    "                if not cur: break\n",
    "                if cur.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    acc.append(norm_space(cur.get_text(\" \")))\n",
    "                elif cur.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                hops += 1\n",
    "            if acc:\n",
    "                chunks.append(\" \".join(acc))\n",
    "    return \" \".join(chunks) if chunks else big_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4f415",
   "metadata": {},
   "source": [
    "## Benefícios, Ingredientes e Tipos de pele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7727cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_benefits(soup):\n",
    "    text = _collect_section_text(soup)\n",
    "    text_norm = norm_text_for_match(text)\n",
    "    found = set()\n",
    "    for canonical, synonyms in BENEFIT_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            if syn and norm_text_for_match(syn) in text_norm:\n",
    "                found.add(canonical); break\n",
    "    if not found:\n",
    "      \n",
    "        return norm_space(text)[:220]\n",
    "    ordered = [b for b in BENEFIT_CANONICAL_ORDER if b in found]\n",
    "    return \", \".join(ordered)\n",
    "\n",
    "def extract_skin_types(soup):\n",
    "    anchors = (\"PARA QUAIS TIPOS DE PELE\",\"TIPO DE PELE\",\"TIPOS DE PELE\",\"PELE\")\n",
    "    full_text = \"\"\n",
    "    for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "        t = norm_space(b.get_text()).upper()\n",
    "        if any(a in t for a in anchors):\n",
    "            cur = b.parent if b.parent else b\n",
    "            acc, hops = [], 0\n",
    "            while cur and hops < 10:\n",
    "                cur = cur.find_next_sibling()\n",
    "                if not cur: break\n",
    "                if cur.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    acc.append(norm_space(cur.get_text(\" \")))\n",
    "                elif cur.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                hops += 1\n",
    "            if acc:\n",
    "                full_text = \" \".join(acc); break\n",
    "            \n",
    "    if not full_text:\n",
    "        full_text = norm_space(soup.get_text(\" \"))\n",
    "    txt = norm_text_for_match(full_text)\n",
    "    found = set()\n",
    "    for canonical, synonyms in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for syn in synonyms:\n",
    "            if syn and norm_text_for_match(syn) in txt:\n",
    "                found.add(canonical); break\n",
    "    if found:\n",
    "        ordered = [c for c in SKIN_TYPE_CANONICAL_ORDER if c in found]\n",
    "        return \", \".join(ordered)\n",
    "    return full_text[:200] + (\"...\" if len(full_text) > 200 else \"\")\n",
    "\n",
    "def extract_active_ingredients(soup):\n",
    "    text = \"\"\n",
    "\n",
    "    for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "        title = norm_space(b.get_text()).upper()\n",
    "        if \"PRINCIPAIS ATIVOS\" in title or \"ATIVOS\" in title:\n",
    "            cur = b.parent if b.parent else b\n",
    "            hops, acc = 0, []\n",
    "            while cur and hops < 12:\n",
    "                cur = cur.find_next_sibling()\n",
    "                if not cur: break\n",
    "                if cur.name in (\"p\",\"div\",\"span\",\"ul\",\"ol\",\"li\"):\n",
    "                    acc.append(norm_space(cur.get_text(\" \")))\n",
    "                elif cur.name in (\"h1\",\"h2\",\"h3\",\"strong\",\"b\"):\n",
    "                    break\n",
    "                hops += 1\n",
    "            if acc:\n",
    "                text = \" \".join(acc); break\n",
    "            \n",
    "    if not text:\n",
    "        for b in soup.find_all([\"b\",\"strong\",\"h1\",\"h2\",\"h3\"]):\n",
    "            title = norm_space(b.get_text()).upper()\n",
    "            if \"COMPOSIÇÃO\" in title or \"COMPOSICAO\" in title or \"INGREDIENTES\" in title:\n",
    "                p = b.find_next(\"p\")\n",
    "                if p:\n",
    "                    text = norm_space(p.get_text(\" \")); break\n",
    "                \n",
    "    if not text:\n",
    "        text = norm_space(soup.get_text(\" \"))\n",
    "    text_norm = norm_text_for_match(text)\n",
    "    found = set()\n",
    "    for ing in INGREDIENTES_VALIDOS:\n",
    "        if norm_text_for_match(ing) in text_norm:\n",
    "            found.add(ing)\n",
    "    if found:\n",
    "        return \", \".join(sorted(found, key=lambda x: strip_accents(x).lower()))\n",
    "    return text[:300] + (\"...\" if len(text) > 300 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dff806",
   "metadata": {},
   "source": [
    "## Imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2dc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_best(soup):\n",
    "    og = soup.select_one('meta[property=\"og:image\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        url = og[\"content\"]\n",
    "        if url.startswith(\"//\"): url = \"https:\" + url\n",
    "        return url\n",
    "    best_url, best_w = \"\", -1\n",
    "    for img in soup.select(\"img\"):\n",
    "        srcset = img.get(\"srcset\") or \"\"\n",
    "        src = img.get(\"src\") or \"\"\n",
    "        def absurl(u):\n",
    "            if not u: return \"\"\n",
    "            if u.startswith(\"//\"): return \"https:\" + u\n",
    "            return urljoin(BASE_URL, u)\n",
    "        if srcset:\n",
    "            for part in srcset.split(\",\"):\n",
    "                bits = part.strip().split()\n",
    "                if not bits: continue\n",
    "                cand, w = bits[0], -1\n",
    "                if len(bits) > 1 and bits[1].endswith(\"w\"):\n",
    "                    try: w = int(bits[1][:-1])\n",
    "                    except: w = -1\n",
    "                if w > best_w:\n",
    "                    best_w = w; best_url = absurl(cand)\n",
    "        elif src:\n",
    "            if best_w < 0:\n",
    "                best_url = absurl(src); best_w = 0\n",
    "    if best_url:\n",
    "        if best_url.startswith(\"//\"):\n",
    "            best_url = \"https:\" + best_url\n",
    "        return best_url\n",
    "    return \"\"\n",
    "\n",
    "def download_image(session, url, dest_dir, slug):\n",
    "    if not url: return \"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    m = re.search(r\"[?&]width=(\\d+)\", url)\n",
    "    width = m.group(1) if m else \"\"\n",
    "    fname = f\"{slug}__{width}.jpg\" if width else f\"{slug}.jpg\"\n",
    "    path = os.path.join(dest_dir, fname)\n",
    "    try:\n",
    "        r = session.get(url, timeout=session.timeout, headers=HEADERS)\n",
    "        if r.status_code == 200:\n",
    "            with open(path, \"wb\") as f: f.write(r.content)\n",
    "            return path\n",
    "        logging.warning(\"Falha ao baixar imagem %s (HTTP %s)\", url, r.status_code)\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Erro ao baixar imagem %s: %s\", url, e)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68c8f4",
   "metadata": {},
   "source": [
    "## Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc1e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_product(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    nome = \"\"\n",
    "    name = soup.select_one(\"h1.h2.product-single__title\") or soup.select_one(\"h1.product-single__title\")\n",
    "    if name: nome = norm_space(name.get_text())\n",
    "\n",
    "    if any(k in (nome or \"\").lower() for k in EXCLUDE_KEYWORDS):\n",
    "        return None\n",
    "\n",
    "    preco_num = \"\"\n",
    "    price_el = soup.select_one(\"span.product__price\") or soup.select_one(\"span[data-product-price]\")\n",
    "    if price_el:\n",
    "        preco_num = norm_price(price_el.get_text())\n",
    "    preco_fmt = format_brl_price(preco_num) if preco_num else \"\"\n",
    "\n",
    "    beneficios = extract_benefits(soup) or \"\"\n",
    "    tipos_de_pele = extract_skin_types(soup) or \"\"\n",
    "    ingredientes = extract_active_ingredients(soup) or \"\"\n",
    "\n",
    "    categoria = classify_category_from_name(nome)\n",
    "\n",
    "    imagem_url = extract_image_best(soup)\n",
    "    imagem_filename = get_image_filename(imagem_url)\n",
    "\n",
    "    data = {\n",
    "        \"marca\": \"ollie\",\n",
    "        \"nome\": nome,\n",
    "        \"subtitulo\": None,                 \n",
    "        \"categoria\": categoria,           \n",
    "        \"quantidade\": \"\",           \n",
    "        \"preco\": preco_fmt,\n",
    "        \"beneficios\": \"; \".join([x.strip() for x in beneficios.replace(\",\", \";\").split(\";\") if x.strip()]),\n",
    "        \"ingredientes\": \"; \".join([x.strip() for x in ingredientes.replace(\",\", \";\").split(\";\") if x.strip()]),\n",
    "        \"tipo_pele\": \"; \".join([x.strip() for x in tipos_de_pele.replace(\",\", \";\").split(\";\") if x.strip()]),\n",
    "        \"imagem\": imagem_filename\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fd9cc",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41455c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(products_data, out_csv, out_json):\n",
    "\n",
    "    csv_columns = [\n",
    "        \"marca\",\"nome\",\"subtitulo\",\"categoria\", \"quantidade\",\"preco\",\n",
    "        \"ingredientes\",\"beneficios\",\"tipo_pele\",\"imagem\",\n",
    "    ]\n",
    "\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "        w.writeheader()\n",
    "        for product in products_data:\n",
    "            w.writerow({k: product.get(k, \"\") for k in csv_columns})\n",
    "\n",
    "    json_data = [{k: product.get(k, \"\") for k in csv_columns} for product in products_data]\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def run_scraper(out_csv=\"ollie_products.csv\", out_json=\"ollie_products.json\",\n",
    "                images_dir=\"images/\", max_retries=3, timeout=20, max_products=80):\n",
    "    s = make_session(max_retries=max_retries, timeout=timeout)\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(out_json) or \".\", exist_ok=True)\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    product_urls = scrape_listing(s, page_cap=20)\n",
    "    logging.info(\"Total de URLs: %d\", len(product_urls))\n",
    "\n",
    "    products_data = []\n",
    "    seen_names = set()\n",
    "\n",
    "    for i, url in enumerate(product_urls, 1):\n",
    "        if len(products_data) >= max_products:\n",
    "            logger.info(\"Limite de %d produtos atingido. Parando.\", max_products)\n",
    "            break\n",
    "\n",
    "        html, _ = fetch_html(s, url)\n",
    "        data = parse_product(html, url)\n",
    "        if data is None:\n",
    "            logging.info(\"Skip (exclusão): %s\", url)\n",
    "            continue\n",
    "\n",
    "        nome_key = (data.get(\"nome\") or \"\").strip().lower()\n",
    "        if nome_key in seen_names:\n",
    "            logging.info(\"Skip (duplicado): %s\", nome_key)\n",
    "            continue\n",
    "        seen_names.add(nome_key)\n",
    "\n",
    "        slug = slugify(data.get(\"nome\") or os.path.basename(urlparse(url).path))\n",
    "        _ = download_image(s, data.get(\"_imagem_url\",\"\"), images_dir, slug)\n",
    "\n",
    "        data.pop(\"_imagem_url\", None)\n",
    "        products_data.append(data)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            write_outputs(products_data, out_csv, out_json)\n",
    "            logging.info(\"Parcial salva (%d itens).\", len(products_data))\n",
    "\n",
    "    write_outputs(products_data, out_csv, out_json)\n",
    "    logging.info(\"Finalizado: %d itens\", len(products_data))\n",
    "    return len(products_data), out_csv, out_json, images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd41673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 11:30:39,339 | INFO | Página 1 | 33 produtos\n",
      "2025-09-24 11:30:40,738 | INFO | Página 2 | 6 produtos\n",
      "2025-09-24 11:30:41,854 | INFO | Página 3 | 33 produtos\n",
      "2025-09-24 11:30:41,854 | INFO | Nenhum novo produto. Encerrando.\n",
      "2025-09-24 11:30:41,855 | INFO | Total de URLs: 39\n",
      "2025-09-24 11:30:44,423 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-09-24 11:30:45,871 | INFO | Skip (duplicado): glow hidratante facial fps50\n",
      "2025-09-24 11:30:47,281 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-hidratante-corporal-fps-60\n",
      "2025-09-24 11:30:49,831 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-24 11:30:51,104 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-24 11:30:52,534 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-24 11:30:53,992 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-24 11:30:55,413 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-24 11:30:56,818 | INFO | Skip (duplicado): protetor solar em bastão com cor fps 95\n",
      "2025-09-24 11:30:57,932 | INFO | Parcial salva (3 itens).\n",
      "2025-09-24 11:30:59,305 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-24 11:31:00,933 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-24 11:31:02,050 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-24 11:31:03,731 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-24 11:31:05,418 | INFO | Skip (duplicado): bastão 3 em 1 - blush, batom e sombra fps 95\n",
      "2025-09-24 11:31:06,699 | INFO | Parcial salva (4 itens).\n",
      "2025-09-24 11:31:08,776 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/iluminador-corporal-fps-40\n",
      "2025-09-24 11:31:10,336 | INFO | Parcial salva (6 itens).\n",
      "2025-09-24 11:31:14,029 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/ollie-club-meia-esportiva\n",
      "2025-09-24 11:31:15,094 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/manguito-ollie\n",
      "2025-09-24 11:31:16,229 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-pratica\n",
      "2025-09-24 11:31:17,585 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-iluminada\n",
      "2025-09-24 11:31:18,808 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-iluminada\n",
      "2025-09-24 11:31:19,977 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-pele-de-verao-o-ano-todo\n",
      "2025-09-24 11:31:21,349 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/loucos-por-esporte\n",
      "2025-09-24 11:31:22,930 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-resistencia\n",
      "2025-09-24 11:31:23,980 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-alta-performance\n",
      "2025-09-24 11:31:25,274 | INFO | Parcial salva (9 itens).\n",
      "2025-09-24 11:31:28,402 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/protetor-solar-spray-corporal\n",
      "2025-09-24 11:31:29,595 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/caixa-especial-para-presente\n",
      "2025-09-24 11:31:31,012 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-make-completona\n",
      "2025-09-24 11:31:32,111 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/kit-para-peles-oleosas\n",
      "2025-09-24 11:31:33,301 | INFO | Skip (exclusão): https://meuollie.com.br/collections/loja-produtos-ollie/products/charms-ollie\n",
      "2025-09-24 11:31:33,302 | INFO | Finalizado: 10 itens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCRAPING CONCLUÍDO ===\n",
      "Itens coletados: 10\n",
      "CSV: /home/usuario/Área de trabalho/Dados/Ollie/ollie_products.csv\n",
      "JSON: /home/usuario/Área de trabalho/Dados/Ollie/ollie_products.json\n",
      "Imagens: /home/usuario/Área de trabalho/Dados/Ollie/images\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OUT_CSV = \"ollie_products.csv\"\n",
    "    OUT_JSON = \"ollie_products.json\"\n",
    "    IMAGES_DIR = \"images/\"\n",
    "    MAX_RETRIES = 3\n",
    "    TIMEOUT = 20\n",
    "    MAX_PRODUCTS = 80\n",
    "\n",
    "    try:\n",
    "        n_final, csv_path, json_path, img_dir = run_scraper(\n",
    "            out_csv=OUT_CSV,\n",
    "            out_json=OUT_JSON,\n",
    "            images_dir=IMAGES_DIR,\n",
    "            max_retries=MAX_RETRIES,\n",
    "            timeout=TIMEOUT,\n",
    "            max_products=MAX_PRODUCTS,\n",
    "        )\n",
    "        print(\"\\n=== SCRAPING CONCLUÍDO ===\")\n",
    "        print(\"Itens coletados:\", n_final)\n",
    "        print(\"CSV:\", os.path.abspath(csv_path))\n",
    "        print(\"JSON:\", os.path.abspath(json_path))\n",
    "        print(\"Imagens:\", os.path.abspath(img_dir))\n",
    "    except Exception as e:\n",
    "        print(\"Erro durante o scraping:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
