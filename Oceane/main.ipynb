{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e68cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coletando via VTEX Search API…\n",
      "[API] +50 (total 50)\n",
      "[API] +50 (total 100)\n",
      "[API] +50 (total 150)\n",
      "[API] +50 (total 200)\n",
      "[API] +50 (total 250)\n",
      "[API] +24 (total 274)\n",
      "[INFO] Produtos recebidos: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16200/1755377021.py:303: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for tag in soup.find_all(text=re.compile(r\"(?i)composi[cç][aã]o|ingredientes\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processados 20/274\n",
      "[INFO] Processados 40/274\n",
      "[INFO] Processados 60/274\n",
      "[INFO] Processados 80/274\n",
      "[INFO] Processados 100/274\n",
      "[INFO] Processados 120/274\n",
      "[INFO] Processados 140/274\n",
      "[INFO] Processados 160/274\n",
      "[INFO] Processados 180/274\n",
      "[INFO] Processados 200/274\n",
      "[INFO] Processados 220/274\n",
      "[INFO] Processados 240/274\n",
      "[INFO] Processados 260/274\n",
      "[OK] JSON: /home/usuario/Área de trabalho/Dados/Oceane/oceane_products.json\n",
      "[OK] CSV : /home/usuario/Área de trabalho/Dados/Oceane/oceane_products.csv\n",
      "[OK] Itens salvos: 107\n",
      "[OK] Imagens em: /home/usuario/Área de trabalho/Dados/Oceane/images\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Océane /skincare — API (URLs, preço, imagem fallback) + PDP HTML (ingredientes, benefícios, imagem) — 1 célula\n",
    "\n",
    "import os, re, json, time, random, unicodedata\n",
    "from urllib.parse import urlsplit\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ========================= CONFIG =========================\n",
    "BASE_URL = \"https://www.oceane.com.br\"\n",
    "CAT_PATH = \"skincare\"\n",
    "BRAND = \"oceane\"\n",
    "\n",
    "BATCH = 50\n",
    "REQ_TIMEOUT = 25\n",
    "RETRY = 3\n",
    "SLEEP = (0.5, 1.0)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                   \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,application/json;q=0.8,*/*;q=0.7\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.6,en;q=0.5\",\n",
    "    \"Origin\": BASE_URL,\n",
    "    \"Referer\": f\"{BASE_URL}/{CAT_PATH}\",\n",
    "}\n",
    "\n",
    "IMAGES_DIR = Path(\"images\")\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DOWNLOAD_IMAGES = True\n",
    "\n",
    "# ========================= SEUS MÓDULOS =========================\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/home/usuario/Área de trabalho/Dados/models\"))\n",
    "\n",
    "try:\n",
    "    from skin import SKIN_TYPE_CANONICAL_ORDER, SKIN_TYPE_SYNONYMS_PT\n",
    "except Exception:\n",
    "    SKIN_TYPE_CANONICAL_ORDER = [\"seca\",\"oleosa\",\"mista\",\"normal\",\"sensível\",\"acneica\",\"madura\",\"todas\"]\n",
    "    SKIN_TYPE_SYNONYMS_PT = {\n",
    "        \"seca\":[\"pele seca\",\"secas\"],\n",
    "        \"oleosa\":[\"oleosa\",\"oleosas\",\"oleosidade\",\"sebo\",\"seborreguladora\"],\n",
    "        \"mista\":[\"mista\",\"mistas\"],\n",
    "        \"normal\":[\"normal\",\"normais\"],\n",
    "        \"sensível\":[\"sensível\",\"sensíveis\",\"sensibilidade\"],\n",
    "        \"acneica\":[\"acne\",\"espinhas\",\"acneica\",\"acneicas\"],\n",
    "        \"madura\":[\"madura\",\"rugas\",\"linhas finas\",\"anti-idade\"],\n",
    "        \"todas\":[\"todos os tipos\",\"todas as peles\",\"todas\"]\n",
    "    }\n",
    "\n",
    "try:\n",
    "    from exclude import EXCLUDE_KEYWORDS\n",
    "except Exception:\n",
    "    EXCLUDE_KEYWORDS = []\n",
    "\n",
    "try:\n",
    "    from ingredient import INGREDIENTES_VALIDOS as _IV\n",
    "    INGREDIENTES_VALIDOS = set(s.strip().casefold() for s in _IV)\n",
    "except Exception:\n",
    "    INGREDIENTES_VALIDOS = {\n",
    "        \"retinol\",\"niacinamida\",\"vitamina c\",\"ácido hialurônico\",\"hialuronato de sódio\",\n",
    "        \"cafeína\",\"taurina\",\"pantenol\",\"glicerina\",\"tocoferol\",\"ceramidas\",\n",
    "        \"ácido salicílico\",\"centella asiatica\",\"aloe vera\",\"madecassoside\",\"ceramide np\"\n",
    "    }\n",
    "\n",
    "try:\n",
    "    from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "except Exception:\n",
    "    BENEFIT_SYNONYMS_PT = {\n",
    "        \"hidratação\": [\"hidrata\",\"hidratada\",\"hidratante\",\"hidratação\"],\n",
    "        \"acalma\": [\"acalma\",\"calmante\",\"suaviza\",\"ameniza vermelhidão\",\"reduz vermelhidão\"],\n",
    "        \"antioxidante\": [\"antioxidante\",\"protege contra radicais livres\"],\n",
    "        \"antiolheiras\": [\"olheiras\",\"antiolheiras\",\"clareia olheiras\"],\n",
    "        \"diminui inchaço/bolsas\": [\"inchaço\",\"bolsas\",\"desincha\",\"drenante\"],\n",
    "        \"controle de oleosidade\": [\"controla a oleosidade\",\"seborreguladora\",\"reduz sebo\"],\n",
    "        \"anti-idade\": [\"rugas\",\"linhas finas\",\"anti-idade\",\"firma\",\"firmeza\",\"colágeno\"],\n",
    "        \"uniformiza tom\": [\"uniformiza\",\"clareador\",\"hiperpigmentação\",\"manchas\"],\n",
    "        \"regeneração\": [\"regeneração\",\"repara\",\"barreira da pele\",\"cicatrizante\"],\n",
    "    }\n",
    "    BENEFIT_CANONICAL_ORDER = [\n",
    "        \"hidratação\",\"controle de oleosidade\",\"antioxidante\",\"acalma\",\n",
    "        \"anti-idade\",\"uniformiza tom\",\"antiolheiras\",\"diminui inchaço/bolsas\",\"regeneração\"\n",
    "    ]\n",
    "\n",
    "try:\n",
    "    from category import CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS\n",
    "except Exception:\n",
    "    CATEGORY_CANONICAL_ORDER = [\"limpeza\",\"tônico\",\"esfoliante\",\"máscara\",\"hidratante\",\"sérum\",\"protetor solar\",\"tratamento para área dos olhos\",\"lábios\",\"acessórios\",\"kits\",\"outros\"]\n",
    "    CATEGORY_HINTS = {\n",
    "        \"tratamento para área dos olhos\":[\"olhos\",\"eye\",\"olheira\",\"olheiras\",\"bolsas\",\"eye cream\",\"cica eye\"],\n",
    "        \"sérum\":[\"serum\",\"sérum\"],\n",
    "        \"hidratante\":[\"creme\",\"gel-creme\",\"loção\",\"hidratante\",\"gel hidratante\",\"cream\"],\n",
    "        \"limpeza\":[\"sabonete\",\"gel de limpeza\",\"cleanser\",\"demaquilante\",\"cleansing\",\"balm demaquilante\"],\n",
    "        \"máscara\":[\"máscara\",\"mask\"],\n",
    "        \"protetor solar\":[\"fps\",\"protetor\",\"sunscreen\",\"solar\"],\n",
    "        \"lábios\":[\"lip\",\"lábios\",\"balm\",\"lip balm\"],\n",
    "        \"kits\":[\"kit\",\"combo\",\"k\"]\n",
    "    }\n",
    "\n",
    "# ========================= HELPERS =========================\n",
    "def _sleep(): time.sleep(random.uniform(*SLEEP))\n",
    "def clean_space(s): return \" \".join((s or \"\").split())\n",
    "def normcase(s): return unicodedata.normalize(\"NFKC\", (s or \"\")).strip().casefold()\n",
    "\n",
    "def strip_accents_lower(s):\n",
    "    if not s: return \"\"\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^\\w\\s-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def fetch(url: str, params: dict | None = None):\n",
    "    for attempt in range(1, RETRY+1):\n",
    "        try:\n",
    "            _sleep()\n",
    "            r = SESSION.get(url, headers=HEADERS, params=params or {}, timeout=REQ_TIMEOUT)\n",
    "            if 200 <= r.status_code < 300:\n",
    "                return r\n",
    "            print(f\"[HTTP] {r.status_code} -> {r.url} (tentativa {attempt})\")\n",
    "            if r.status_code in (403, 429): time.sleep(2.0 * attempt)\n",
    "            else: time.sleep(0.7 * attempt)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[NET] {e.__class__.__name__} -> {url} (tentativa {attempt})\")\n",
    "            time.sleep(1.0 * attempt)\n",
    "    return None\n",
    "\n",
    "def get_soup(url: str) -> Optional[BeautifulSoup]:\n",
    "    r = fetch(url)\n",
    "    if not r: return None\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "def only_filename_from_url(u: str) -> str:\n",
    "    if not u: return \"\"\n",
    "    return os.path.basename(urlsplit(u)._replace(query=\"\").path) or \"\"\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    s = strip_accents_lower(name)\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s).strip(\"-\")\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s)\n",
    "    return s or \"produto\"\n",
    "\n",
    "def download_image(image_url: str, product_name: str) -> str:\n",
    "    if not image_url: return \"\"\n",
    "    try:\n",
    "        r = fetch(image_url)\n",
    "        if not r: return \"\"\n",
    "        # extensão\n",
    "        ext = \".jpg\"\n",
    "        lu = image_url.lower()\n",
    "        if \".png\" in lu: ext = \".png\"\n",
    "        elif \".webp\" in lu: ext = \".webp\"\n",
    "        base = sanitize_filename(product_name)\n",
    "        dest = IMAGES_DIR / f\"{base}{ext}\"\n",
    "        c = 1\n",
    "        while dest.exists():\n",
    "            dest = IMAGES_DIR / f\"{base}-{c}{ext}\"\n",
    "            c += 1\n",
    "        with open(dest, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return dest.name\n",
    "    except Exception as e:\n",
    "        print(f\"[IMG] falha {e} -> {image_url}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========================= NORMALIZAÇÕES =========================\n",
    "QTY_RX = re.compile(r\"(?P<val>\\d+[.,]?\\d*)\\s*(?P<u>ml|mL|l|g|mg|kg|un|und|unid|gr|grs)\\b\", re.I)\n",
    "\n",
    "def extract_quantity(*texts) -> str:\n",
    "    blob = \" \".join([t for t in texts if t])\n",
    "    m = QTY_RX.search(blob or \"\")\n",
    "    if not m: return \"\"\n",
    "    val = m.group(\"val\").replace(\",\", \".\")\n",
    "    u = m.group(\"u\").lower()\n",
    "    u = {\"ml\":\"ml\",\"mL\":\"ml\",\"l\":\"l\",\"g\":\"g\",\"mg\":\"mg\",\"kg\":\"kg\",\"un\":\"un\",\"und\":\"un\",\"unid\":\"un\",\"gr\":\"g\",\"grs\":\"g\"}.get(u,u)\n",
    "    try:\n",
    "        if \".\" in val and float(val).is_integer(): val = str(int(float(val)))\n",
    "    except: pass\n",
    "    return f\"{val}{u}\"\n",
    "\n",
    "def normalize_category_from_texts(*texts) -> str:\n",
    "    blob_ci = normcase(\" \".join([t for t in texts if t]))\n",
    "    for canon, hints in CATEGORY_HINTS.items():\n",
    "        if any(normcase(h) in blob_ci for h in hints): return canon\n",
    "    for canon in CATEGORY_CANONICAL_ORDER:\n",
    "        if normcase(canon) in blob_ci: return canon\n",
    "    return \"outros\"\n",
    "\n",
    "def normalize_skin_types(*texts) -> str:\n",
    "    blob_ci = normcase(\" \".join([t for t in texts if t]))\n",
    "    found = set()\n",
    "    for canon, syns in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        pool = syns if isinstance(syns, (list,tuple,set)) else [syns]\n",
    "        if any(normcase(s) in blob_ci for s in pool) or normcase(canon) in blob_ci:\n",
    "            found.add(canon)\n",
    "    order = SKIN_TYPE_CANONICAL_ORDER or []\n",
    "    return \"; \".join([c for c in order if c in found]) if order else \"; \".join(sorted(found))\n",
    "\n",
    "def looks_like_ingredients(txt: str) -> bool:\n",
    "    \"\"\"Heurística: muito separador, nomes químicos/INCI, vírgulas restantes.\"\"\"\n",
    "    if not txt: return False\n",
    "    t = txt.strip()\n",
    "    commas = t.count(\",\")\n",
    "    # Se tiver muitas vírgulas e vários termos conhecidos, provavelmente é INCI/composição\n",
    "    hits = sum(1 for k in [\"gly\", \"niacin\", \"acid\", \"extract\", \"ceramide\", \"hyalur\", \"alcohol\", \"parfum\", \"lecithin\", \"glycer\", \"stear\"] if k in t.lower())\n",
    "    return commas >= 8 and hits >= 2\n",
    "\n",
    "def normalize_ingredients(text: str) -> str:\n",
    "    if not text: return \"\"\n",
    "    # cortar após \"Composição\"/\"Ingredientes\" se houver\n",
    "    cut = re.split(r\"(?i)\\b(composição|ingredientes)\\b[:\\-]?\\s*\", text, maxsplit=1)\n",
    "    raw = cut[2] if len(cut) >= 3 else text\n",
    "    parts = re.split(r\"[;,/]\\s*|\\s+\\-\\s+|\\s•\\s|•\", raw)\n",
    "    parts = [p.strip() for p in parts if p and p.strip()]\n",
    "    if INGREDIENTES_VALIDOS:\n",
    "        filt = []\n",
    "        for p in parts:\n",
    "            p_ci = strip_accents_lower(p).replace(\"acido hialuronico\",\"ácido hialurônico\")\n",
    "            if (p_ci in INGREDIENTES_VALIDOS) or any(p_ci.startswith(k) for k in INGREDIENTES_VALIDOS):\n",
    "                filt.append(p_ci)\n",
    "        parts = filt if filt else parts  # se nada bater, mantém lista original\n",
    "    # dedup mantendo ordem\n",
    "    seen, out = set(), []\n",
    "    for p in parts:\n",
    "        if p not in seen:\n",
    "            seen.add(p); out.append(p)\n",
    "    return \"; \".join(out)\n",
    "\n",
    "def extract_benefits_from_text(text: str) -> str:\n",
    "    \"\"\"Mapeia texto livre -> benefícios canônicos via BENEFIT_SYNONYMS_PT.\"\"\"\n",
    "    if not text: return \"\"\n",
    "    n = strip_accents_lower(text)\n",
    "    found = set()\n",
    "    for canonico, patt_list in BENEFIT_SYNONYMS_PT.items():\n",
    "        for patt in patt_list:\n",
    "            if patt and strip_accents_lower(patt) in n:\n",
    "                found.add(canonico); break\n",
    "    if not found: return \"\"\n",
    "    if BENEFIT_CANONICAL_ORDER:\n",
    "        order = {b:i for i,b in enumerate(BENEFIT_CANONICAL_ORDER)}\n",
    "        return \"; \".join(sorted(found, key=lambda x: order.get(x, 999)))\n",
    "    return \"; \".join(sorted(found))\n",
    "\n",
    "# ========================= VTEX SEARCH API =========================\n",
    "def api_list_batch(offset: int, size: int) -> list:\n",
    "    url = f\"{BASE_URL}/api/catalog_system/pub/products/search/{CAT_PATH}\"\n",
    "    params = {\"map\":\"c\", \"_from\": offset, \"_to\": offset + size - 1}\n",
    "    r = fetch(url, params=params)\n",
    "    if not r: return []\n",
    "    try:\n",
    "        data = r.json()\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def api_iter_all() -> list:\n",
    "    items, off = [], 0\n",
    "    while True:\n",
    "        batch = api_list_batch(off, BATCH)\n",
    "        if not batch: break\n",
    "        items.extend(batch)\n",
    "        print(f\"[API] +{len(batch)} (total {len(items)})\")\n",
    "        if len(batch) < BATCH: break\n",
    "        off += BATCH\n",
    "    return items\n",
    "\n",
    "# ========================= PDP HTML EXTRACT =========================\n",
    "def extract_pdp_fields(url: str, api_fallback: dict | None) -> dict:\n",
    "    \"\"\"Retorna dict com ingredientes, beneficios, tipo_pele extra, quantidade extra e imagem baixada.\"\"\"\n",
    "    out = {\"ingredientes\": \"\", \"beneficios\": \"\", \"tipo_pele_extra\": \"\", \"quantidade_extra\": \"\", \"imagem_fn\": \"\", \"nome_html\": \"\", \"categoria_html\": \"\"}\n",
    "    soup = get_soup(url)\n",
    "    if not soup: \n",
    "        return out\n",
    "\n",
    "    # Nome (às vezes melhor no HTML)\n",
    "    name = \"\"\n",
    "    for sel in [\"h1.productName\",\"h1.vtex-store-components-3-x-productNameContainer\",\".vtex-store-components-3-x-productBrand\",\"h1\"]:\n",
    "        n = soup.select_one(sel)\n",
    "        if n:\n",
    "            name = clean_space(n.get_text(\" \", strip=True))\n",
    "            break\n",
    "    out[\"nome_html\"] = name\n",
    "\n",
    "    # Breadcrumb → categoria extra\n",
    "    bc = soup.select_one('[data-testid=\"breadcrumb\"]') or soup.select_one(\".vtex-breadcrumb-1-x-container\")\n",
    "    breadcrumb_text = clean_space(bc.get_text(\" \", strip=True)) if bc else \"\"\n",
    "    out[\"categoria_html\"] = normalize_category_from_texts(breadcrumb_text, name)\n",
    "\n",
    "    # Blocos \"specifications\" / \"display:contents\"\n",
    "    content_divs = soup.select(\".vtex-store-components-3-x-content--specifications-tabs div[style*='display:contents']\")\n",
    "    texts = [clean_space(d.get_text(\" \", strip=True)) for d in content_divs if d]\n",
    "    texts = [t for t in texts if t]\n",
    "\n",
    "    # Ingredientes: procurar por \"Composição\" antes OU heurística química\n",
    "    ingredientes = \"\"\n",
    "    # procura tag que mencione Composição e pega o próximo display:contents\n",
    "    comp_label = None\n",
    "    for tag in soup.find_all(text=re.compile(r\"(?i)composi[cç][aã]o|ingredientes\")):\n",
    "        comp_label = tag\n",
    "        break\n",
    "    if comp_label:\n",
    "        # pega o próximo div display:contents depois do label\n",
    "        cont = None\n",
    "        node = comp_label.parent\n",
    "        for _ in range(8):\n",
    "            if not node: break\n",
    "            node = node.find_next()\n",
    "            if node and node.name == \"div\" and node.get(\"style\") and \"display:contents\" in node.get(\"style\"):\n",
    "                cont = node; break\n",
    "        if cont:\n",
    "            ingredientes = clean_space(cont.get_text(\" \", strip=True))\n",
    "\n",
    "    if not ingredientes:\n",
    "        # heurística: escolha o primeiro bloco que \"parece\" INCI\n",
    "        for t in texts:\n",
    "            if looks_like_ingredients(t):\n",
    "                ingredientes = t; break\n",
    "\n",
    "    # Benefícios: concatenar blocos descritivos (que não parecem INCI)\n",
    "    benefits_texts = [t for t in texts if not looks_like_ingredients(t)]\n",
    "    beneficios = extract_benefits_from_text(\" \".join(benefits_texts))\n",
    "\n",
    "    # Tipos de pele extra e quantidade extra vindos da PDP\n",
    "    tipo_pele_extra = normalize_skin_types(name, \" \".join(texts))\n",
    "    quantidade_extra = extract_quantity(name, \" \".join(texts))\n",
    "\n",
    "    # Imagem principal: pegar maior do srcset\n",
    "    def best_src_from_img(img):\n",
    "        if not img: return \"\"\n",
    "        if img.get(\"srcset\"):\n",
    "            best = \"\"\n",
    "            best_w = 0\n",
    "            for part in img[\"srcset\"].split(\",\"):\n",
    "                part = part.strip()\n",
    "                if \" \" in part:\n",
    "                    u, w = part.rsplit(\" \", 1)\n",
    "                    w = w.strip().lower().replace(\"w\",\"\")\n",
    "                    try:\n",
    "                        wi = int(w)\n",
    "                        if wi > best_w:\n",
    "                            best_w = wi; best = u.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "            if best: return best\n",
    "        return img.get(\"src\") or \"\"\n",
    "    img_el = soup.select_one(\"img.vtex-store-components-3-x-productImageTag--main\") or \\\n",
    "             soup.select_one(\"img.vtex-store-components-3-x-productImageTag\")\n",
    "    img_url = best_src_from_img(img_el) if img_el else \"\"\n",
    "    img_fn = \"\"\n",
    "    if not img_url and api_fallback:\n",
    "        # usa imagem da API como fallback\n",
    "        items = api_fallback.get(\"items\") or []\n",
    "        for it in items:\n",
    "            imgs = it.get(\"images\") or []\n",
    "            if imgs:\n",
    "                img_url = imgs[0].get(\"imageUrl\") or imgs[0].get(\"imageUrlHttps\") or \"\"\n",
    "                if img_url: break\n",
    "\n",
    "    if img_url and DOWNLOAD_IMAGES:\n",
    "        img_fn = download_image(img_url, name or (api_fallback.get(\"productName\") if api_fallback else \"produto\"))\n",
    "    elif img_url:\n",
    "        img_fn = only_filename_from_url(img_url)\n",
    "\n",
    "    out.update({\n",
    "        \"ingredientes\": normalize_ingredients(ingredientes),\n",
    "        \"beneficios\": beneficios,\n",
    "        \"tipo_pele_extra\": tipo_pele_extra,\n",
    "        \"quantidade_extra\": quantidade_extra,\n",
    "        \"imagem_fn\": img_fn\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# ========================= PARSE (API + PDP) =========================\n",
    "def parse_product(api_obj: dict) -> Optional[Dict]:\n",
    "    name = clean_space(api_obj.get(\"productName\") or api_obj.get(\"productTitle\") or api_obj.get(\"productReference\") or \"\")\n",
    "    if not name: return None\n",
    "    if any(k.lower() in name.lower() for k in EXCLUDE_KEYWORDS): return None\n",
    "\n",
    "    # básicos via API\n",
    "    categories = api_obj.get(\"categories\") or []\n",
    "    breadcrumb = \" / \".join(categories)\n",
    "    meta_desc = (api_obj.get(\"metaTagDescription\") or \"\").strip()\n",
    "    long_desc = (api_obj.get(\"description\") or api_obj.get(\"productDescription\") or meta_desc).strip()\n",
    "\n",
    "    categoria = normalize_category_from_texts(breadcrumb, name)\n",
    "    quantidade = extract_quantity(name, long_desc)\n",
    "    tipo_pele = normalize_skin_types(name, meta_desc, long_desc)\n",
    "\n",
    "    # preço\n",
    "    preco = \"\"\n",
    "    items = api_obj.get(\"items\") or []\n",
    "    try:\n",
    "        for it in items:\n",
    "            for seller in it.get(\"sellers\", []):\n",
    "                co = seller.get(\"commertialOffer\") or {}\n",
    "                price = co.get(\"Price\") or co.get(\"price\")\n",
    "                if price is not None:\n",
    "                    preco = f\"{float(price):.2f}\"\n",
    "                    break\n",
    "            if preco: break\n",
    "    except Exception:\n",
    "        preco = \"\"\n",
    "\n",
    "    # imagem fallback (se PDP falhar)\n",
    "    img_url_api = \"\"\n",
    "    for it in items:\n",
    "        imgs = it.get(\"images\") or []\n",
    "        if imgs:\n",
    "            img_url_api = imgs[0].get(\"imageUrl\") or imgs[0].get(\"imageUrlHttps\") or \"\"\n",
    "            if img_url_api: break\n",
    "\n",
    "    # URL da PDP para enriquecer\n",
    "    slug = api_obj.get(\"linkText\") or \"\"\n",
    "    pdp_url = f\"{BASE_URL}/{slug}/p\" if slug else \"\"\n",
    "\n",
    "    # Enriquecimento pela PDP\n",
    "    pdp = extract_pdp_fields(pdp_url, api_obj) if pdp_url else {}\n",
    "\n",
    "    # Campos finais (prioriza PDP quando existir)\n",
    "    nome_final = pdp.get(\"nome_html\") or name\n",
    "    categoria_final = pdp.get(\"categoria_html\") or categoria\n",
    "    quantidade_final = pdp.get(\"quantidade_extra\") or quantidade\n",
    "    tipo_pele_final = pdp.get(\"tipo_pele_extra\") or tipo_pele\n",
    "    ingredientes_final = pdp.get(\"ingredientes\") or normalize_ingredients(long_desc)\n",
    "    beneficios_final = pdp.get(\"beneficios\") or \"\"  # mapeado por sinônimos\n",
    "\n",
    "    # imagem: prioriza a baixada da PDP; senão usa API\n",
    "    imagem_fn = pdp.get(\"imagem_fn\") or (download_image(img_url_api, nome_final) if (DOWNLOAD_IMAGES and img_url_api) else only_filename_from_url(img_url_api))\n",
    "\n",
    "    return {\n",
    "        \"marca\": BRAND,\n",
    "        \"nome\": nome_final,\n",
    "        \"subtitulo\": \"\",\n",
    "        \"categoria\": categoria_final or \"outros\",\n",
    "        \"quantidade\": quantidade_final or \"\",\n",
    "        \"preco\": preco,\n",
    "        \"beneficios\": beneficios_final,\n",
    "        \"ingredientes\": ingredientes_final,\n",
    "        \"tipo_pele\": tipo_pele_final or \"\",\n",
    "        \"imagem\": imagem_fn or \"\",\n",
    "    }\n",
    "\n",
    "# ========================= PIPELINE =========================\n",
    "def scrape_oceane_products() -> List[Dict]:\n",
    "    print(\"Coletando via VTEX Search API…\")\n",
    "    raw = api_iter_all()\n",
    "    print(f\"[INFO] Produtos recebidos: {len(raw)}\")\n",
    "\n",
    "    rows = []\n",
    "    for i, pj in enumerate(raw, 1):\n",
    "        item = parse_product(pj)\n",
    "        if item:\n",
    "            rows.append(item)\n",
    "        if i % 20 == 0:\n",
    "            print(f\"[INFO] Processados {i}/{len(raw)}\")\n",
    "    return rows\n",
    "\n",
    "def save_data(products_data: List[Dict], json_path=\"oceane_products.json\", csv_path=\"oceane_products.csv\"):\n",
    "    if not products_data:\n",
    "        print(\"Nenhum dado para salvar.\")\n",
    "        return\n",
    "    cols = [\"marca\",\"nome\",\"subtitulo\",\"categoria\",\"quantidade\",\"preco\",\"beneficios\",\"ingredientes\",\"tipo_pele\",\"imagem\"]\n",
    "    clean = []\n",
    "    for r in products_data:\n",
    "        row = {c: (r.get(c) or \"\") for c in cols}\n",
    "        clean.append(row)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clean, f, ensure_ascii=False, indent=2)\n",
    "    pd.DataFrame(clean, columns=cols).to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] JSON: {os.path.abspath(json_path)}\")\n",
    "    print(f\"[OK] CSV : {os.path.abspath(csv_path)}\")\n",
    "    print(f\"[OK] Itens salvos: {len(clean)}\")\n",
    "    print(f\"[OK] Imagens em: {IMAGES_DIR.resolve()}\")\n",
    "\n",
    "# ========================= RUN =========================\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_oceane_products()\n",
    "    # ordena pra facilitar revisão\n",
    "    data = sorted(data, key=lambda x: (x.get(\"categoria\",\"\"), x.get(\"nome\",\"\")))\n",
    "    save_data(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
