{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1926a30",
   "metadata": {},
   "source": [
    "# Sallve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d202a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sys, time, random, unicodedata, csv\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urljoin, urlparse, quote\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sys.path.append(os.path.abspath(\"./../models\"))\n",
    "\n",
    "from skin import SKIN_TYPE_CANONICAL_ORDER, SKIN_TYPE_SYNONYMS_PT\n",
    "from exclude import EXCLUDE_KEYWORDS\n",
    "from ingredient import INGREDIENTES_VALIDOS\n",
    "from benefits import BENEFIT_SYNONYMS_PT, BENEFIT_CANONICAL_ORDER\n",
    "from category import CATEGORY_CANONICAL_ORDER, CATEGORY_HINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239e6c1",
   "metadata": {},
   "source": [
    "### Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cd709",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://www.oceane.com.br\"\n",
    "LISTING_PATH = \"/skincare\"\n",
    "OUTPUT_JSON = \"oceane_products.json\"\n",
    "IMAGES_DIR = \"images\"\n",
    "MAX_TO_FETCH = 300 \n",
    "RANGE_STEP = 48  \n",
    "TIMEOUT = 25\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,application/json,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1e388",
   "metadata": {},
   "source": [
    "## Utilitários\n",
    "\n",
    "### Funções auxiliares para normalização de texto, remoção de acentos, tokenização dos ingredientes, nomes de arquivos e formatação de preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_acc(s): return \"\".join(c for c in unicodedata.normalize(\"NFKD\", s or \"\") if not unicodedata.combining(c))\n",
    "def norm(s): return re.sub(r\"\\s+\",\" \", strip_acc((s or \"\").lower())).strip()\n",
    "\n",
    "def any_excluded(text, extra=None):\n",
    "    base = norm(text or \"\")\n",
    "    if extra: base += \" \" + norm(extra or \"\")\n",
    "    for kw in EXCLUDE_KEYWORDS:\n",
    "        if norm(kw) in base:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def should_exclude(name, url=None):\n",
    "    if name and any_excluded(name): return True\n",
    "    if url and any_excluded(urlparse(url).path): return True\n",
    "    return False\n",
    "\n",
    "def slugify(text):\n",
    "    s = re.sub(r\"[^a-z0-9]+\",\"-\", norm(text or \"\"))\n",
    "    s = re.sub(r\"-+\",\"-\", s).strip(\"-\")\n",
    "    return s or \"produto\"\n",
    "\n",
    "def extract_quantity(name):\n",
    "    if not name: return None\n",
    "    for pat in [r\"(\\d+\\s*(?:g|ml|mg|l|kg))\\b\", r\"(\\d+\\s*(?:gramas|mililitros|litros))\\b\"]:\n",
    "        m = re.search(pat, name, re.I)\n",
    "        if m: return m.group(1).replace(\" \",\"\").strip()\n",
    "    return None\n",
    "\n",
    "def identify_category(name):\n",
    "    if not name: return None\n",
    "    t = norm(name)\n",
    "    for cat in CATEGORY_CANONICAL_ORDER:\n",
    "        for hint in CATEGORY_HINTS.get(cat, []):\n",
    "            if norm(hint) in t:\n",
    "                return cat\n",
    "    return None\n",
    "\n",
    "def identify_ingredients(text):\n",
    "    if not text: return None\n",
    "    t = norm(text); out=[]\n",
    "    for ing in INGREDIENTES_VALIDOS:\n",
    "        if norm(ing) in t and ing not in out: out.append(ing)\n",
    "    return \"; \".join(out) if out else None\n",
    "\n",
    "def identify_benefits(text):\n",
    "    if not text: return None\n",
    "    t = norm(text); found=set()\n",
    "    for can, syns in BENEFIT_SYNONYMS_PT.items():\n",
    "        for syn in syns:\n",
    "            if norm(syn) in t:\n",
    "                found.add(can); break\n",
    "    ordered = [b for b in BENEFIT_CANONICAL_ORDER if b in found]\n",
    "    return \"; \".join(ordered) if ordered else None\n",
    "\n",
    "def identify_skin_types(text):\n",
    "    if not text: return None\n",
    "    t = norm(text); out=[]\n",
    "    for can, syns in SKIN_TYPE_SYNONYMS_PT.items():\n",
    "        for syn in syns:\n",
    "            if norm(syn) in t and can not in out:\n",
    "                out.append(can); break\n",
    "    ordered = [s for s in SKIN_TYPE_CANONICAL_ORDER if s in out]\n",
    "    return \"; \".join(ordered) if ordered else None\n",
    "\n",
    "def session():\n",
    "    s = requests.Session()\n",
    "    s.headers.update(HEADERS)\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    retry = Retry(total=2, backoff_factor=0.5, status_forcelist=[429,500,502,503,504], allowed_methods=[\"GET\",\"HEAD\",\"OPTIONS\"])\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retry))\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a158d",
   "metadata": {},
   "source": [
    "##  Coleta de Links de Produtos Disponíveis - VTEX APi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8658d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vtex_search_ranges():\n",
    "    ranges = []\n",
    "    start = 0\n",
    "    while start < MAX_TO_FETCH:\n",
    "        end = start + RANGE_STEP\n",
    "        ranges.append((start, end))\n",
    "        start = end + 1\n",
    "    return ranges\n",
    "\n",
    "def vtex_search_candidates():\n",
    " \n",
    "    fq1 = f\"C:/{LISTING_PATH.strip('/')}/\"\n",
    "    return [\n",
    "        f\"/api/catalog_system/pub/products/search?fq={quote(fq1)}&_from={{start}}&_to={{end}}\",\n",
    "        f\"/api/catalog_system/pub/products/search?ft=skincare&_from={{start}}&_to={{end}}\",\n",
    "        f\"/api/catalog_system/pub/products/search/{LISTING_PATH.strip('/')}?_from={{start}}&_to={{end}}\", \n",
    "    ]\n",
    "\n",
    "def fetch_vtex_products():\n",
    "    items = []\n",
    "    seen_ids = set()\n",
    "    with session() as s:\n",
    "        for start, end in vtex_search_ranges():\n",
    "            got_in_range = 0\n",
    "            for pattern in vtex_search_candidates():\n",
    "                url = BASE + pattern.format(start=start, end=end)\n",
    "                try:\n",
    "                    r = s.get(url, timeout=TIMEOUT)\n",
    "                    if r.status_code != 200:\n",
    "                        continue\n",
    "                    data = r.json()\n",
    "                    if not isinstance(data, list) or not data:\n",
    "                        continue\n",
    "                    for prod in data:\n",
    "                        pid = str(prod.get(\"productId\") or prod.get(\"productID\") or \"\")\n",
    "                        if not pid or pid in seen_ids:\n",
    "                            continue\n",
    "                        seen_ids.add(pid)\n",
    "                        items.append(prod)\n",
    "                        got_in_range += 1\n",
    "                except Exception:\n",
    "                    continue\n",
    "            # heurística: se o intervalo não trouxe nada em nenhuma variante, segue para próximo range\n",
    "            if got_in_range == 0 and start > 0:\n",
    "              \n",
    "                break\n",
    "    return items\n",
    "\n",
    "def product_to_record_from_api(prod: dict) -> Optional[Dict]:\n",
    "    name = prod.get(\"productName\") or prod.get(\"productTitle\") or prod.get(\"product_name\")\n",
    "    if not name:\n",
    "        name = prod.get(\"productNameComplete\")\n",
    "    if should_exclude(name):\n",
    "        return None\n",
    "\n",
    "    description = prod.get(\"description\") or prod.get(\"descriptionShort\") or prod.get(\"metaTagDescription\")\n",
    "\n",
    "    img_url = None\n",
    "    final_price = None\n",
    "    list_price = None\n",
    "    selling_price = None\n",
    "\n",
    "    items = prod.get(\"items\") or []\n",
    "    if items:\n",
    "        it0 = items[0]\n",
    "        images = it0.get(\"images\") or []\n",
    "        if images:\n",
    "            im = images[0]\n",
    "            img_url = im.get(\"imageUrl\") or im.get(\"url\") or im.get(\"imageUrlText\")\n",
    "        sellers = it0.get(\"sellers\") or []\n",
    "        if sellers:\n",
    "            off = sellers[0].get(\"commertialOffer\") or {}\n",
    "         \n",
    "            list_price = off.get(\"ListPrice\") or off.get(\"listPrice\")\n",
    "            selling_price = off.get(\"Price\") or off.get(\"price\") or off.get(\"SellingPrice\") or off.get(\"sellingPrice\")\n",
    "\n",
    "    def fmt(p):\n",
    "        if p is None: return None\n",
    "        try: return f\"{float(p):.2f}\"\n",
    "        except: return None\n",
    "\n",
    "    selling_price = fmt(selling_price)\n",
    "    list_price = fmt(list_price)\n",
    "    final_price = selling_price or list_price\n",
    "\n",
    "    quantidade = extract_quantity(name or \"\")\n",
    "    categoria = identify_category(name or \"\")\n",
    "    beneficios = identify_benefits(description)\n",
    "    ingredientes = identify_ingredients(description)\n",
    "    tipo_pele = identify_skin_types(description)\n",
    "\n",
    "    slug = slugify(name or \"produto\")\n",
    "    img_name = None\n",
    "    if img_url:\n",
    "        img_name = download_image(img_url, slug)\n",
    "\n",
    "    if not any([name, final_price, img_url, description]):\n",
    "        return None\n",
    "\n",
    "    rec = {\n",
    "        \"marca\": \"oceane\",\n",
    "        \"nome\": name,\n",
    "        \"subtitulo\": None,\n",
    "        \"categoria\": categoria,\n",
    "        \"quantidade\": quantidade,\n",
    "        \"preco\": final_price,\n",
    "        \"beneficios\": beneficios,\n",
    "        \"ingredientes\": ingredientes,\n",
    "        \"tipo_pele\": tipo_pele,\n",
    "        \"imagem\": img_name,\n",
    "    }\n",
    "    return rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a38dc3",
   "metadata": {},
   "source": [
    "## Extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    try:\n",
    "        with session() as s:\n",
    "            r = s.get(url, timeout=TIMEOUT)\n",
    "            if r.status_code == 200 and r.text:\n",
    "                return r.text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def list_urls_from_html():\n",
    "    all_urls = set()\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = BASE + (LISTING_PATH if page == 1 else f\"{LISTING_PATH}?page={page}\")\n",
    "        html = get_html(url)\n",
    "        if not html:\n",
    "            break\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        candidates = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if not href or href.startswith(\"#\"): continue\n",
    "            if any(x in href for x in [\"/account\",\"/cart\",\"/login\",\"/busca\",\"whatsapp\"]): continue\n",
    "            if \"/p\" in href:\n",
    "                candidates.append(urljoin(BASE, href))\n",
    "        for card in soup.find_all([\"div\",\"article\",\"li\",\"section\"], class_=re.compile(r\"(product|item|shelf|search-result|grid|gallery|summary)\", re.I)):\n",
    "            a = card.find(\"a\", href=True)\n",
    "            if a and \"/p\" in a[\"href\"]:\n",
    "                candidates.append(urljoin(BASE, a[\"href\"]))\n",
    "\n",
    "        before = len(all_urls)\n",
    "        for u in candidates:\n",
    "            u = u.rstrip(\"/\")\n",
    "            if not u.endswith(\"/p\"): u = u + \"/p\"\n",
    "            all_urls.add(u)\n",
    "        added = len(all_urls) - before\n",
    "        if page > 3 and added < 3:\n",
    "            break\n",
    "        page += 1\n",
    "        time.sleep(0.4)\n",
    "    return list(all_urls)\n",
    "\n",
    "def parse_name_from_html(soup, fallback):\n",
    "    el = soup.select_one(\"span.vtex-store-components-3-x-productBrand\")\n",
    "    if el: return el.get_text(strip=True)\n",
    "    el = soup.find([\"h1\",\"span\",\"div\"], class_=re.compile(r\"productBrand|productName|name\", re.I))\n",
    "    if el: return el.get_text(strip=True)\n",
    "    if soup.title and soup.title.string:\n",
    "        t = soup.title.string.strip()\n",
    "        t = re.sub(r\" \\| .*$\", \"\", t)\n",
    "        if len(t) > 3: return t\n",
    "    # JSON-LD\n",
    "    for sc in soup.find_all(\"script\", type=re.compile(r\"ld\\+json\", re.I)):\n",
    "        try:\n",
    "            data = json.loads(sc.string or sc.get_text() or \"{}\")\n",
    "            arr = data if isinstance(data, list) else [data]\n",
    "            for it in arr:\n",
    "                if isinstance(it, dict) and it.get(\"@type\") == \"Product\" and it.get(\"name\"):\n",
    "                    return it[\"name\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fallback\n",
    "\n",
    "def parse_prices_from_html(soup):\n",
    "    def get_p(cont):\n",
    "        if not cont: return None\n",
    "        inteiro = cont.find(\"span\", class_=re.compile(r\"currencyInteger\"))\n",
    "        fracao  = cont.find(\"span\", class_=re.compile(r\"currencyFraction\"))\n",
    "        if inteiro and fracao:\n",
    "            return f\"{inteiro.get_text(strip=True)}.{fracao.get_text(strip=True)}\".replace(\",\", \".\")\n",
    "        txt = cont.get_text(\" \", strip=True)\n",
    "        m = re.search(r\"(\\d{1,3}(?:\\.\\d{3})*,\\d{2}|\\d+[.,]\\d{2})\", txt)\n",
    "        if m: return m.group(1).replace(\".\", \"\").replace(\",\", \".\")\n",
    "        return None\n",
    "    list_c = soup.find(\"span\", class_=re.compile(r\"vtex-store-components-3-x-listPriceValue\"))\n",
    "    sell_c = soup.find(\"div\", class_=re.compile(r\"vtex-store-components-3-x-price_sellingPriceContainer\"))\n",
    "    sell_s = soup.find(\"span\", class_=re.compile(r\"vtex-store-components-3-x-sellingPriceValue\"))\n",
    "    list_price = get_p(list_c)\n",
    "    selling   = get_p(sell_c) or get_p(sell_s)\n",
    "    final     = selling or list_price\n",
    "    return final, list_price, selling\n",
    "\n",
    "def parse_specs_from_html(soup):\n",
    "    c1 = soup.find(\"div\", class_=re.compile(r\"vtex-store-components-3-x-content--specifications-tabs\"))\n",
    "    c2 = soup.find(\"div\", class_=re.compile(r\"vtex-store-components-3-x-specificationsTab--specifications-mini\"))\n",
    "    texts = []\n",
    "    for c in [c1,c2]:\n",
    "        if c:\n",
    "            t = c.get_text(\" \", strip=True)\n",
    "            if t: texts.append(t)\n",
    "    text = \" \".join(texts)\n",
    "    return re.sub(r\"\\s+\",\" \", text).strip()\n",
    "\n",
    "def parse_image_from_html(soup):\n",
    "    img = soup.find(\"img\", class_=re.compile(r\"vtex-store-components-3-x-productImageTag\"))\n",
    "    if img and img.get(\"src\"): return img[\"src\"]\n",
    "    if img and img.get(\"srcset\"):\n",
    "        urls = re.findall(r\"(https?://[^\\s,]+)\\s+\\d+w\", img[\"srcset\"])\n",
    "        if urls: return urls[-1]\n",
    "    meta = soup.find(\"meta\", property=\"og:image\")\n",
    "    if meta and meta.get(\"content\"): return meta[\"content\"]\n",
    "    for sc in soup.find_all(\"script\", type=re.compile(r\"ld\\+json\", re.I)):\n",
    "        try:\n",
    "            data = json.loads(sc.string or sc.get_text() or \"{}\")\n",
    "            arr = data if isinstance(data, list) else [data]\n",
    "            for it in arr:\n",
    "                if isinstance(it, dict) and it.get(\"@type\") == \"Product\":\n",
    "                    im = it.get(\"image\")\n",
    "                    if isinstance(im, list) and im: return im[-1]\n",
    "                    if isinstance(im, str): return im\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def download_image(url_img, slug):\n",
    "    if not url_img: return None\n",
    "    try:\n",
    "        with session() as s:\n",
    "            r = s.get(url_img, timeout=TIMEOUT, headers={\"Referer\": BASE, **HEADERS})\n",
    "            if r.status_code != 200 or not r.content:\n",
    "                return None\n",
    "            os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "            ct = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            ext = \"png\"\n",
    "            if \"jpeg\" in ct or \"jpg\" in ct: ext = \"jpg\"\n",
    "            path = os.path.join(IMAGES_DIR, f\"{slug}.{ext}\")\n",
    "            with open(path, \"wb\") as f: f.write(r.content)\n",
    "            return os.path.basename(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def record_from_product_page(url):\n",
    "    html = get_html(url)\n",
    "    if not html: return None\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    name = parse_name_from_html(soup, None)\n",
    "    if should_exclude(name, url): return None\n",
    "    final, orig, promo = parse_prices_from_html(soup)\n",
    "    specs = parse_specs_from_html(soup)\n",
    "    img   = parse_image_from_html(soup)\n",
    "\n",
    "    quantidade = extract_quantity(name or \"\")\n",
    "    categoria  = identify_category(name or \"\")\n",
    "    beneficios = identify_benefits(specs)\n",
    "    ingredientes = identify_ingredients(specs)\n",
    "    tipo_pele = identify_skin_types(specs)\n",
    "    if \"olheiras\" in (specs or \"\").lower():\n",
    "        if tipo_pele and \"com olheiras\" not in tipo_pele:\n",
    "            tipo_pele = (tipo_pele + \"; com olheiras\").strip(\"; \")\n",
    "        elif not tipo_pele:\n",
    "            tipo_pele = \"com olheiras\"\n",
    "\n",
    "    if not any([name, final, img, specs]):\n",
    "        return None\n",
    "\n",
    "    img_name = download_image(img, slugify(name or \"produto\")) if img else None\n",
    "\n",
    "    return {\n",
    "        \"marca\": \"oceane\",\n",
    "        \"nome\": name,\n",
    "        \"subtitulo\": None,\n",
    "        \"categoria\": categoria,\n",
    "        \"quantidade\": quantidade,\n",
    "        \"preco\": final,\n",
    "        \"beneficios\": beneficios,\n",
    "        \"ingredientes\": ingredientes,\n",
    "        \"tipo_pele\": tipo_pele,\n",
    "        \"imagem\": img_name\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876d62d",
   "metadata": {},
   "source": [
    "## Arquivo e Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "    # 1) Tenta VTEX API \n",
    "    print(\"\\nBuscando pela API VTEX\")\n",
    "    api_products = fetch_vtex_products()\n",
    "    print(f\"\\n API retornou {len(api_products)} itens brutos\")\n",
    "\n",
    "    results: List[Dict] = []\n",
    "    if api_products:\n",
    "        for prod in api_products:\n",
    "            rec = product_to_record_from_api(prod)\n",
    "            if rec:\n",
    "              \n",
    "                if should_exclude(rec.get(\"nome\")):\n",
    "                    continue\n",
    "                results.append(rec)\n",
    "\n",
    "    # 2) Se ficou pouco, tenta HTML fallback\n",
    "    if len(results) < 50:\n",
    "        print(\" HTML (fallback)…\")\n",
    "        urls = list_urls_from_html()\n",
    "        print(f\" Quantidade: {len(urls)} URLs\")\n",
    "        for i, u in enumerate(urls, 1):\n",
    "            rec = record_from_product_page(u)\n",
    "            if rec:\n",
    "                results.append(rec)\n",
    "            time.sleep(random.uniform(0.3, 0.7))\n",
    "\n",
    "    # 3) Dedup por nome (ou imagem)\n",
    "    dedup = {}\n",
    "    for r in results:\n",
    "        key = norm(r.get(\"nome\")) or r.get(\"imagem\") or str(r)\n",
    "        if key not in dedup:\n",
    "            dedup[key] = r\n",
    "    results = list(dedup.values())\n",
    "\n",
    "    # 4) Salva e preview\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\n____________________________________________________________________________________________________________\")\n",
    "    print(\"\\nFim da Execução!\")\n",
    "    print(f\"Total de produtos salvos: {len(results)}\")\n",
    "    print(f\"JSON: {OUTPUT_JSON}\")\n",
    "    print(f\"Imagens em: {IMAGES_DIR}\")\n",
    "    print(\"____________________________________________________________________________________________________________\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    print(\"____________________________________________________________________________________________________________\")\n",
    "    print(\"WEB SCRAPING Oceane\")\n",
    "    print(\"____________________________________________________________________________________________________________\")\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74de0d",
   "metadata": {},
   "source": [
    "## Conversão JSON para CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16788693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV gerado: oceane_products.csv (32 linhas)\n",
      "A partir do JSON: oceane_products.json\n"
     ]
    }
   ],
   "source": [
    "def json_to_csv(json_file=\"oceane_products.json\", csv_file=\"oceane_products.csv\"):\n",
    "\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Nenhum dado encontrado no arquivo {json_file}\")\n",
    "            return\n",
    "        \n",
    "        cols = [\"marca\", \"nome\", \"subtitulo\", \"categoria\", \"quantidade\", \"preco\", \"beneficios\", \"ingredientes\", \"tipo_pele\", \"imagem\"]\n",
    "        \n",
    "        with open(csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=cols)\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "\n",
    "                csv_row = {k: (row.get(k) or \"\") for k in cols}\n",
    "                writer.writerow(csv_row)\n",
    "        \n",
    "        print(f\"CSV gerado: {csv_file} ({len(data)} linhas)\")\n",
    "        print(f\"A partir do JSON: {json_file}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Arquivo {json_file} não encontrado!\")\n",
    "        \n",
    "        import glob\n",
    "        json_files = glob.glob(\"*.json\")\n",
    "        if json_files:\n",
    "            for f in json_files:\n",
    "                print(f\"   - {f}\")\n",
    "        else:\n",
    "            print(\"   Nenhum arquivo .json encontrado\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter JSON para CSV: {e}\")\n",
    "\n",
    "\n",
    "json_to_csv(\"oceane_products.json\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
